<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>MS10</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:32:11 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>On the Value of Commitment and Availability Guarantees When Selling to Strategic Consumers</title><link>http://www.example.com/articles/1</link><description>Su, Xuanming; Zhang, Fuqiang
This paper studies the role of product availability in attracting consumer demand. We start with a newsvendor model, but additionally assume that stockouts are costly to consumers. The seller sets an observable price and an unobservable stocking quantity. Consumers anticipate the likelihood of stockouts and determine whether to visit the seller. We characterize the rational expectations equilibrium in this game. We propose two strategies that the seller can use to improve profits: (i) commitment (i.e., the seller, ex ante, commits to a particular quantity) and (ii) availability guarantees (i.e., the seller promises to compensate consumers, ex post, if the product is out of stock). Interestingly, the seller has an incentive to overcompensate consumers during stockouts, relative to the first-best benchmark under which social welfare is maximized. We find that first-best outcomes do not arise in equilibrium, but can be supported when the seller uses a combination of commitment and availability guarantees. Finally, we examine the robustness of these conclusions by extending our analysis to incorporate dynamic learning, multiple products, and consumer heterogeneity.</description><author>Su, Xuanming; Zhang, Fuqiang</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic Resource Dynamics of Manufacturing Firms</title><link>http://www.example.com/articles/1</link><description>Jayanthi, Shekhar; Roth, Aleda V.; Kristal, Mehmet M.; Venu, Lauren Carter-Roth
We conceptualize strategic decision-making processes within a manufacturing firm as streams of resources allocated to short-and long-term changes. The analogous ecological model, referred to as the Lotka-Volterra model, captures this dynamic tension between decisions made by the firm and its manufacturing operations. This representation leads to evolutionarily stable manufacturing strategies (ESMSs), which contribute to a firm's competitive advantage in different ways. Using a random sample of 30 firms from the U. S. semiconductor industry, we estimate parameters of the model and arrive at four ESMSs or strategic manufacturing groups that reflect theoretically and empirically distinctive adaptation patterns through their dynamic resource allocations. We observe that a majority of the firms were classified in one of the four groups, with relatively fewer firms in the other three. Notably, our classification based on ecology models agrees well with taxonomies in manufacturing and business strategy theory. Furthermore, our analysis shows significant differences among manufacturing practices and competitive capabilities of the four strategic groups. Managerially, these insights could provide the foundation to implement strategic changes that enable firms to leapfrog from one ESMS to another. This study also paves the way for development of a meso theory of the dynamics of manufacturing strategy.</description><author>Jayanthi, Shekhar; Roth, Aleda V.; Kristal, Mehmet M.; Venu, Lauren Carter-Roth</author><pubDate>Mon, 01 Jun 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Highbrow Films Gather Dust: Time-Inconsistent Preferences and Online DVD Rentals</title><link>http://www.example.com/articles/1</link><description>Milkman, Katherine L.; Rogers, Todd; Bazerman, Max H.
We report on a field study demonstrating systematic differences between the preferences people anticipate they will have over a series of options in the future and their subsequent revealed preferences over those options. Using a novel panel data set, we analyze the film rental and return patterns of a sample of online DVD rental customers over a period of four months. We predict and find that should DVDs (e. g., documentaries) are held significantly longer than want DVDs (e. g., action films) within customer. Similarly, we also predict and find that people are more likely to rent DVDs in one order and return them in the reverse order when should DVDs are rented before want DVDs. Specifically, a 1.3% increase in the probability of a reversal in preferences (from a baseline rate of 12%) ensues if the first of two sequentially rented movies has more should and fewer want characteristics than the second film. Finally, we find that as the same customers gain more experience with online DVD rentals, the extent to which they hold should films longer than want films decreases. Our results suggest that present bias has a meaningful impact on choice in the field, and that people may learn about their present bias with experience and, as a result, gain the capacity to curb its influence.</description><author>Milkman, Katherine L.; Rogers, Todd; Bazerman, Max H.</author><pubDate>Mon, 01 Jun 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An Optimal Contact Model for Maximizing Online Panel Response Rates</title><link>http://www.example.com/articles/1</link><description>Neslin, Scott A.; Novak, Thomas P.; Baker, Kenneth R.; Hoffman, Donna L.
W e develop and test an optimization model for maximizing response rates for online marketing research survey panels. The model consists of (1) a decision tree predictive model that classifies panelists into "states" and forecasts the response rate for panelists in each state and (2) a linear program that specifies how many panelists should be solicited from each state to maximize response rate. The model is forward looking in that it optimizes over a finite horizon during which S studies are to be fielded. It takes into account the desired number of responses for each study, the likely migration pattern of panelists between states as they are invited and respond or do not respond, as well as demographic requirements. The model is implemented using a rolling horizon whereby the optimal solution for S successive studies is derived and implemented for the first study. Then, as results are observed, an optimal solution is derived for the next S studies, and the solution is implemented for the first of these studies, etc. The procedure is field tested and shown to increase response rates significantly compared to the heuristic currently being used by panel management. Further analysis suggests that the improvement was due to the predictive model and that a "greedy algorithm" would have done equally well in the field test. However, further Monte Carlo simulations suggest circumstances under which the model would outperform the greedy algorithm.</description><author>Neslin, Scott A.; Novak, Thomas P.; Baker, Kenneth R.; Hoffman, Donna L.</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contagion of Wishful Thinking in Markets</title><link>http://www.example.com/articles/1</link><description>Seybert, Nicholas; Bloomfield, Robert
Prior research provides only weak and controversial evidence that people overestimate the likelihood of desirable events (wishful thinking), but strong evidence that people bet more heavily on those events (wishful betting). Two experiments show that wishful betting contaminates beliefs in laboratory financial markets because wishful betters appear to possess more favorable information than they actually do. As a consequence, market interaction exacerbates rather than mitigates wishful thinking. This phenomenon, "contagion of wishful thinking," could be problematic in many settings where people infer others' beliefs from their behavior.</description><author>Seybert, Nicholas; Bloomfield, Robert</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quasi-Robust Multiagent Contracts</title><link>http://www.example.com/articles/1</link><description>Arya, Anil; Demski, Joel; Glover, Jonathan; Liang, Pierre
A criticism of mechanism design theory is that the optimal mechanism designed for one environment can produce drastically different actions, outcomes, and payoffs in a second, even slightly different, environment. In this sense, the theoretically optimal mechanisms usually studied are not "robust." To study robust mechanisms while maintaining an expected utility maximization approach, we study a multiagent model in which the mechanism must be designed before the environment is as well understood as is usually assumed. The particular model is of an auction setting with binary private values. Our main result is that if the prior belief about the correlation in the agents' values is diffuse enough, the optimal Bayesian-Nash auction must also satisfy dominant strategy incentive constraints. Furthermore, when the optimal auction does provide dominant strategy incentives, it takes one of two forms: (i) if perfect correlation and negative correlation are excluded as possibilities, the auction incorporates all information about the prior belief over the possible correlations, and (ii) if either perfect correlation or negative correlation is a possibility, the auction does not incorporate any correlation information and can be described as a modified Vickrey auction.</description><author>Arya, Anil; Demski, Joel; Glover, Jonathan; Liang, Pierre</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multiple Sourcing and Procurement Process Selection with Bidding Events</title><link>http://www.example.com/articles/1</link><description>Tunca, Tunay I.; Wu, Qiong
We examine the procurement process selection problem of a large industrial buyer who employs reverse auctions for awarding procurement contracts. We contrast two classes of commonly used strategies under multiple sourcing; namely, single-stage reverse auctions, and two-stage processes where price-quantity adjustments between the buyer and the suppliers follow a first-stage reverse auction. Deriving bounds of efficiency for these two classes of procurement processes under convex supplier production costs, we present insights on the conditions under which each class is preferable for the buyer. Considering the effect of contracting and processing costs, a single-stage process is likely to be preferable to a two-stage process when the number of bidding suppliers is high, especially when capacity is rigid. A two-stage process with one information transfer in the second stage may be the preferred procurement mode when production is highly scalable, i.e., when the marginal production cost increase with increased production is small. When the number of suppliers is low, the effect of a decrease in production scalability depends on the current scalability level. For high scalability levels, a decrease in production scalability may decrease the efficiency of both single-stage and simple two-stage processes, whereas for low scalability levels, it tends to increase efficiency for both of these process classes. A decrease in production costs makes employing simple processes more attractive when production is highly scalable or when supplier capacity is rigid. For intermediate production scalability, however, a cost decrease may make employing two-stage processes with multiple information transfers in the second round preferable for the buyer.</description><author>Tunca, Tunay I.; Wu, Qiong</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Additive Utility in Prospect Theory</title><link>http://www.example.com/articles/1</link><description>Bleichrodt, Han; Schmidt, Ulrich; Zank, Horst
Prospect theory is currently the main descriptive theory of decision under uncertainty. It generalizes expected utility by introducing nonlinear decision weighting and loss aversion. A difficulty in the study of multiattribute utility under prospect theory is to determine when an attribute yields a gain or a loss. One possibility, adopted in the theoretical literature on multiattribute utility under prospect theory, is to assume that a decision maker determines whether the complete outcome is a gain or a loss. In this holistic evaluation, decision weighting and loss aversion are general and attribute-independent. Another possibility, more common in the empirical literature, is to assume that a decision maker has a reference point for each attribute. We give preference foundations for this attribute-specific evaluation where decision weighting and loss aversion are depending on the attributes.</description><author>Bleichrodt, Han; Schmidt, Ulrich; Zank, Horst</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Loss Functions in Option Valuation: A Framework for Selection</title><link>http://www.example.com/articles/1</link><description>Bams, Dennis; Lehnert, Thorsten; Wolff, Christian C. P.
In this paper, we investigate the importance of different loss functions when estimating and evaluating option pricing models. Our analysis shows that it is important to take into account parameter uncertainty, because this leads to uncertainty in the predicted option price. We illustrate the effect on the out-of-sample pricing errors in an application of the ad hoc Black-Scholes model to DAX index options. We confirm the empirical results of Christoffersen and Jacobs (Christoffersen, P., K. Jacobs. 2004. The importance of the loss function in option valuation. J. Financial Econom. 72 291-318) and find strong evidence for their conjecture that the squared pricing error criterion may serve as a general-purpose loss function in option valuation applications. At the same time, we provide a first yardstick to evaluate the adequacy of the loss function. This is accomplished through a data-driven method to deliver not just a point estimate of the root mean squared pricing error, but a distribution.</description><author>Bams, Dennis; Lehnert, Thorsten; Wolff, Christian C. P.</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Private Network EDI vs. Internet Electronic Markets: A Direct Comparison of Fulfillment Performance</title><link>http://www.example.com/articles/1</link><description>Yao, Yuliang; Dresner, Martin; Palmer, Jonathan
Prior literature has documented the performance benefits from the use of electronic data interchange (EDI) and the Internet. Using purchase and fulfillment records from the U. S. government's Federal Supply Service, we provide a direct comparison of performance between a private network EDI channel and an Internet electronic market. Performance is measured using order cycle time and complete orders fulfilled. Our findings show that the Internet-based electronic market outperforms the EDI-based channel on these two important measures. Order cycle times were significantly lower when using the Internet-based electronic market, whereas the percentage of complete shipments was significantly higher after controlling for product, transaction, seller, and buyer-specific factors. The electronic market even outperforms the EDI channel when buyer and transaction characteristics favor the use of EDI. Because EDI is still prevalent in many industries, these results point to the gains that may be realized by switching to the newer technology.</description><author>Yao, Yuliang; Dresner, Martin; Palmer, Jonathan</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information Market-Based Decision Fusion</title><link>http://www.example.com/articles/1</link><description>Perols, Johan; Chari, Kaushal; Agrawal, Manish
Improved classification performance has practical real-world benefits ranging from improved effectiveness in detecting diseases to increased efficiency in identifying firms that are committing financial fraud. Multiclassifier combination (MCC) aims to improve classification performance by combining the decisions of multiple individual classifiers. In this paper, we present information market-based fusion (IMF), a novel multiclassifier combiner method for decision fusion that is based on information markets. In IMF, the individual classifiers are implemented as participants in an information market where they place bets on different object classes. The reciprocals of the market odds that minimize the difference between the total betting amount and the potential payouts for different classes represent the MCC probability estimates of each class being the true object class. By using a market-based approach, IMF can adjust to changes in base-classifier performance without requiring offline training data or a static ensemble composition. Experimental results show that when the true classes of objects are only revealed for objects classified as positive, for low positive ratios, IMF outperforms three benchmarks combiner methods, majority, average, and weighted average; for high positive ratios, IMF outperforms majority and performs on par with average and weighted average. When the true classes of all objects are revealed, IMF outperforms weighted average and majority and marginally outperforms average.</description><author>Perols, Johan; Chari, Kaushal; Agrawal, Manish</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal Policies and Approximations for a Bayesian Linear Regression Inventory Model</title><link>http://www.example.com/articles/1</link><description>Azoury, Katy S.; Miyaoka, Julia
In this paper, we consider a periodic review inventory problem where demand in each period is modeled by linear regression. We use a Bayesian formulation to update the regression parameters as new information becomes available. We find that a state-dependent base-stock policy is optimal and we give structural results. One interesting finding is that our structural results are not analogous to classical results in Bayesian inventory research. This departure from classical results is due to the role that the independent variables play in the Bayesian regression formulation. Because of the computational complexity of the optimal policy, we propose a combination of two heuristics that simplifies the Bayesian inventory problem. Through analytical and numerical evaluation, we find that the heuristics provide near-optimal results.</description><author>Azoury, Katy S.; Miyaoka, Julia</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Generalized Approach to Portfolio Optimization: Improving Performance by Constraining Portfolio Norms</title><link>http://www.example.com/articles/1</link><description>DeMiguel, Victor; Garlappi, Lorenzo; Nogales, Francisco J.; Uppal, Raman
We provide a general framework for finding portfolios that perform well out-of-sample in the presence of estimation error. This framework relies on solving the traditional minimum-variance problem but subject to the additional constraint that the norm of the portfolio-weight vector be smaller than a given threshold. We show that our framework nests as special cases the shrinkage approaches of Jagannathan and Ma (Jagannathan, R., T. Ma. 2003. Risk reduction in large portfolios: Why imposing the wrong constraints helps. J. Finance 58 1651 1684) and Ledoit and Wolf (Ledoit, O., M. Wolf. 2003. Improved estimation of the covariance matrix of stock returns with an application to portfolio selection. J. Empirical Finance 10 603-621, and Ledoit, O., M. Wolf. 2004. A well-conditioned estimator for large-dimensional covariance matrices. J. Multivariate Anal. 88 365-411) and the 1/N portfolio studied in DeMiguel et al. (DeMiguel, V., L. Garlappi, R. Uppal. 2009. Optimal versus naive diversification: How inefficient is the 1/N portfolio strategy? Rev. Financial Stud. 22 1915-1953). We also use our framework to propose several new portfolio strategies. For the proposed portfolios, we provide a moment-shrinkage interpretation and a Bayesian interpretation where the investor has a prior belief on portfolio weights rather than on moments of asset returns. Finally, we compare empirically the out-of-sample performance of the new portfolios we propose to 10 strategies in the literature across five data sets. We find that the norm-constrained portfolios often have a higher Sharpe ratio than the portfolio strategies in Jagannathan and Ma (2003), Ledoit and Wolf (2003, 2004), the 1/N portfolio, and other strategies in the literature, such as factor portfolios.</description><author>DeMiguel, Victor; Garlappi, Lorenzo; Nogales, Francisco J.; Uppal, Raman</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information Sharing and Order Variability Control Under a Generalized Demand Model</title><link>http://www.example.com/articles/1</link><description>Chen, Li; Lee, Hau L.
The value of information sharing and how it could address the bullwhip effect have been the subject of studies in the literature. Most of these studies used different forms of demand models, assuming that no order smoothing was used by the retailer and that the supplier has full knowledge of the retailer's demand model and order policy. In this paper, we contribute to the literature by starting with a most general demand model, coupled with a smoothing policy for order variability control. In addition, we do not require that the supplier has full knowledge of the retailer's demand model and order policy, but instead let the retailer share its projected future orders (and freely revise them as the retailer sees fit). Under such a setting, we first obtain a unifying formula for the magnitude of the bullwhip effect. The formula indicates that it is the forecast correlation over the exposure period as a whole that determines the magnitude of the bullwhip effect. We then quantify the value of information sharing and generalize the existing results in the literature. Finally, we explore the optimal smoothing parameters that could bene fit the total supply chain. The resulting optimal policy resembles the postponement strategy. We find that information sharing together with order postponement improves the supply chain performance, even though the order variability may amplify in some cases.</description><author>Chen, Li; Lee, Hau L.</author><pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Battle of the Retail Channels: How Product Selection and Geography Drive Cross-Channel Competition</title><link>http://www.example.com/articles/1</link><description>Brynjolfsson, Erik; Hu, Yu; Rahman, Mohammad S.
A key question for Internet commerce is the nature of competition with traditional brick-and-mortar retailers. Although traditional retailers vastly outsell Internet retailers in most product categories, research on Internet retailing has largely neglected this fundamental dimension of competition. Is cross-channel competition significant, and if so, how and where can Internet retailers win this battle? This paper attempts to answer these questions using a unique combination of data sets. We collect data on local market structures for traditional retailers, and then match these data to a data set on consumer demand via two direct channels: Internet and catalog. Our analyses show that Internet retailers face significant competition from brick-and-mortar retailers when selling mainstream products, but are virtually immune from competition when selling niche products. Furthermore, because the Internet channel sells proportionately more niche products than the catalog channel, the competition between the Internet channel and local stores is less intense than the competition between the catalog channel and local stores. The methods we introduce can be used to analyze cross-channel competition in other product categories, and suggest that managers need to take into account the types of products they sell when assessing competitive strategies.</description><author>Brynjolfsson, Erik; Hu, Yu; Rahman, Mohammad S.</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition Under Generalized Attraction Models: Applications to Quality Competition Under Yield Uncertainty</title><link>http://www.example.com/articles/1</link><description>Federgruen, Awi; Yang, Nan
We characterize the equilibrium behavior in a broad class of competition models in which the competing firms' market shares are given by an attraction model, and the aggregate sales in the industry depend on the aggregate attraction value according to a general function. Each firm's revenues and costs are proportional with its expected sales volume, with a cost rate that depends on the firm's chosen attraction value according to an arbitrary increasing function. Whereas most existing competition papers with attraction models can be viewed as special cases of this general model, we apply our general results to a new set of quality competition models. Here an industry has N suppliers of a given product, who compete for the business of one or more buyers. Each of the suppliers encounters an uncertain yield factor, with a given general yield distribution. The buyers face uncertain demands over the course of a given sales season. The suppliers compete by selecting key characteristics of their yield distributions, either their means, their standard deviations, or both. These choices have implications for their per-unit cost rates.</description><author>Federgruen, Awi; Yang, Nan</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conditional Monte Carlo Estimation of Quantile Sensitivities</title><link>http://www.example.com/articles/1</link><description>Fu, Michael C.; Hong, L. Jeff; Hu, Jian-Qiang
Estimating quantile sensitivities is important in many optimization applications, from hedging in financial engineering to service-level constraints in inventory control to more general chance constraints in stochastic programming. Recently, Hong (Hong, L. J. 2009. Estimating quantile sensitivities. Oper. Res. 57 118-130) derived a batched infinitesimal perturbation analysis estimator for quantile sensitivities, and Liu and Hong (Liu, G., L. J. Hong. 2009. Kernel estimation of quantile sensitivities. Naval Res. Logist. 56 511-525) derived a kernel estimator. Both of these estimators are consistent with convergence rates bounded by n(-1/3) and n(-2/5), respectively. In this paper, we use conditional Monte Carlo to derive a consistent quantile sensitivity estimator that improves upon these convergence rates and requires no batching or binning. We illustrate the new estimator using a simple but realistic portfolio credit risk example, for which the previous work is inapplicable.</description><author>Fu, Michael C.; Hong, L. Jeff; Hu, Jian-Qiang</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inference from Streaks in Random Outcomes: Experimental Evidence on Beliefs in Regime Shifting and the Law of Small Numbers</title><link>http://www.example.com/articles/1</link><description>Asparouhova, Elena; Hertzel, Michael; Lemmon, Michael
Using data generated from laboratory experiments, we test and compare the empirical accuracy of two models that focus on judgment errors associated with processing information from random sequences. We test for regime-shifting beliefs of the type theorized in Barberis et al. (Barberis, N., A. Shleifer, R. Vishny. 1998. A model of investor sentiment. J. Financial Econom. 49(3) 307-343) and for beliefs in the "law of small numbers" as modeled in Rabin (Rabin, M. 2002. Inference by believers in the law of small numbers. Quart. J. Econom. 117(3) 775-816). In our experiments, we show subjects randomly generated sequences of binary outcomes and ask them to provide probability assessments of the direction of the next outcome. Inconsistent with regime-shifting beliefs, we find that subjects are not more likely to predict that the current streak will continue the longer the streak. Instead, consistent with Rabin (2002), subjects are more likely to expect a reversal following short streaks and continuation after long streaks. Results of a "test-of-fit" analysis based on structural estimation of each model also favor the model in Rabin. To provide more insight on Rabin, we use an additional experimental treatment to show that as the perception of the randomness of the outcome-generating process increases, subjects are more likely to predict reversals of current streaks.</description><author>Asparouhova, Elena; Hertzel, Michael; Lemmon, Michael</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implications of Expected Changes in the Seller's Price in Name-Your-Own-Price Auctions</title><link>http://www.example.com/articles/1</link><description>Fay, Scott; Laran, Juliano
The seller's threshold price in name-your-own-price auctions varies over time. However, consumers must bid without knowing when these variations occur because the threshold price is unobservable to them. This paper uses an analytical model and laboratory auctions to explore how the frequency of changes in the threshold price impacts consumer bidding behavior in name-your-own-price auctions. In particular, we consider how the frequency of these expected changes affects the optimal pattern of bid sequences (e.g., strictly increasing over time or following a nonmonotonic pattern). We find that when the probability of a price change is moderate, consumers may have an incentive to use nonmonotonic bidding patterns. Rather than steadily increasing their bids over time, consumers will, at some point in the bid sequence, decrease their bid. However, when the expected probability of a price change is very low or very high, consumers do not have an incentive to use nonmonotonic bidding patterns. Interestingly, impatient bidders are more likely to decrease their bids at some point in the bid sequence than patient bidders. Finally, we find that more frequent price changes may increase customer satisfaction.</description><author>Fay, Scott; Laran, Juliano</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Valuing Modularity as a Real Option</title><link>http://www.example.com/articles/1</link><description>Gamba, Andrea; Fusari, Nicola
We provide a general valuation approach for capital budgeting decisions involving the modularization in the design of a system. Within the framework developed by Baldwin and Clark (Baldwin, C. Y., K. B. Clark. 2000. Design Rules: The Power of Modularity. MIT Press, Cambridge, MA), we implement a valuation approach using a numerical procedure based on the least-squares Monte Carlo method proposed by Longstaff and Schwartz (Longstaff, F. A., E. S. Schwartz. 2001. Valuing American options by simulation: A simple least-squares approach. Rev. Financial Stud. 14(1) 113-147). The approach is accurate, general, and flexible.</description><author>Gamba, Andrea; Fusari, Nicola</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Matter of Balance: Specialization, Task Variety, and Individual Learning in a Software Maintenance Environment</title><link>http://www.example.com/articles/1</link><description>Narayanan, Sriram; Balasubramanian, Sridhar; Swaminathan, Jayashankar M.
Specialization at work has been recognized as a key driver of learning and productivity since the days of Adam Smith. More recently, researchers have noted that exposure to task variety can enhance learning. We examine how exposure to specialization and variety jointly drive employee productivity in a real-life setting. We analyze a data set covering 88 individuals who worked on 5,711 maintenance tasks in an offshore software support services operation. We find that, as expected, specialization enhances productivity. However, exposure to variety has a nonlinear influence on productivity; i.e., "too much variety" can impede learning. We also find that achieving a proper balance between specialization and exposure to a variety leads to the highest productivity. We capture this balance using an adaptation of the Herfindahl-Hirschman Index from the economics literature. In addition, we examine how the productivity of individuals in a workgroup is affected by member entry and exit, with the latter specified in terms of the degree of specialized experience and the degree of variety experience lost from the workgroup when a member exits. Our analysis reveals that the degree of variety experience lost has a greater impact on productivity than the degree of specialized experience that is lost.</description><author>Narayanan, Sriram; Balasubramanian, Sridhar; Swaminathan, Jayashankar M.</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Swift and Smart: The Moderating Effects of Technological Capabilities on the Market Pioneering-Firm Survival Relationship</title><link>http://www.example.com/articles/1</link><description>Franco, April M.; Sarkar, M. B.; Agarwal, Rajshree; Echambadi, Raj
We extend the concept of first-mover advantage to the context of high-technology industries with multiple product generations, and propose that the notion of first-mover advantage needs to be viewed not only through a dynamic lens, but also in conjunction with technological capability. Our main finding is that first-mover advantages are best understood in tandem with the firm's technological capabilities; early entry is beneficial only for pioneers that are technically strong. However, pioneers that are low on technological capabilities suffer from poor survival rates vis-a-vis market responders or nonentrants into new product generations.</description><author>Franco, April M.; Sarkar, M. B.; Agarwal, Rajshree; Echambadi, Raj</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Silver Lining Effect: Formal Analysis and Experiments</title><link>http://www.example.com/articles/1</link><description>Jarnebrant, Peter; Toubia, Olivier; Johnson, Eric
The silver lining effect predicts that segregating a small gain from a larger loss results in greater psychological value than does integrating them into a smaller loss. Using a generic prospect theory value function, we formalize this effect and derive conditions under which it should occur. We show analytically that if the gain is smaller than a certain threshold, segregation is optimal. This threshold increases with the size of the loss and decreases with the degree of loss aversion of the decision maker. Our formal analysis results in a set of predictions suggesting that the silver lining effect is more likely to occur when (i) the gain is smaller (for a given loss), (ii) the loss is larger (for a given gain), and (iii) the decision maker is less loss averse. We test and confirm these predictions in two studies of preferences, both in a nonmonetary and a monetary setting, analyzing the data in a hierarchical Bayesian framework.</description><author>Jarnebrant, Peter; Toubia, Olivier; Johnson, Eric</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Composition of Electricity Generation Portfolios, Pivotal Dynamics, and Market Prices</title><link>http://www.example.com/articles/1</link><description>Banal-Estanol, Albert; Ruperez Micola, Augusto
We use simulations to study how the diversification of electricity generation portfolios influences wholesale prices. We find that the relationship between technological diversification and market prices is mediated by the supply-to-demand ratio. In each demand case there is a threshold where pivotal dynamics change. Pivotal dynamics pre- and post-threshold are the cause of nonlinearities in the influence of diversification on market prices. The findings are robust to changes in the main market assumptions.</description><author>Banal-Estanol, Albert; Ruperez Micola, Augusto</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Volatility Spreads and Expected Stock Returns</title><link>http://www.example.com/articles/1</link><description>Bali, Turan G.; Hovakimian, Armen
This paper investigates whether realized and implied volatilities of individual stocks can predict the cross-sectional variation in expected returns. Although the levels of volatilities from the physical and risk-neutral distributions cannot predict future returns, there is a significant relation between volatility spreads and expected stock returns. Portfolio level analyses and firm-level cross-sectional regressions indicate a negative and significant relation between expected returns and the realized-implied volatility spread that can be viewed as a proxy for volatility risk. The results also provide evidence for a significantly positive link between expected returns and the call-put options' implied volatility spread that can be considered as a proxy for jump risk. The parameter estimates from the VAR-bivariate-GARCH model indicate significant information flow from individual equity options to individual stocks, implying informed trading in options by investors with private information.</description><author>Bali, Turan G.; Hovakimian, Armen</author><pubDate>Sun, 01 Nov 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient Committed Budget for Implementing Target Audit Probability for Many Inspectees</title><link>http://www.example.com/articles/1</link><description>Yim, Andrew
Strategic models of auditor-inspectee interaction have neglected implementation details in multiple-inspectee settings. With multiple inspectees, the target audit probability derived from the standard analysis can be implemented with sampling plans differing in the budgets committed to support them. Overly committed audit budgets tie up unneeded resources that could have been allocated for better uses. This paper studies the minimum committed budget required to implement a target audit probability when (i) the audit sample can be contingent on "red flags" due to signals of inspectees' private information (e.g., from self-reporting) and (ii) the number of inspectees is large. It proposes an audit rule called bounded simple random sampling (SRS), which is shown to require no more committed budget to support than two other rules naturally generalized from the one-to-one analysis. When the number of inspectees is large enough, bounded SRS is nearly as good as any efficient audit rule, which demands the lowest committed budget necessary to implement the target audit probability. The results offer insights on how audit sampling plans may be formulated to reduce inefficiency and what budget usage ratios should be expected accordingly.</description><author>Yim, Andrew</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Total-Cost Procurement Auctions: Impact of Suppliers' Cost Adjustments on Auction Format Choice</title><link>http://www.example.com/articles/1</link><description>Kostamis, Dimitris; Beil, Damian R.; Duenyas, Izak
We consider sealed- and open-bid total-cost procurement auctions where two attributes are used for contract award decisions: price, which is bid by the supplier, and a fixed cost adjustment, which is included by the buyer to capture nonprice factors such as logistics costs. Suppliers know only their own true production cost and their own cost adjustment, and the buyer does not know the suppliers' true production costs but does know all suppliers' cost adjustments, which she herself sets in order to make an informed total-cost decision. The buyer, who seeks to minimize her total (price and cost adjustment) procurement cost, can choose to run a first-price sealed-bid auction, where suppliers' bids are affected by their beliefs about each other's total costs, or a descending open-bid auction, where only the actual realizations of suppliers' total costs drive the auction outcome. We characterize the buyer's choice between the two formats as a threshold decision over suppliers' cost adjustments and analyze the effect of supplier beliefs on her decision. We also study the impact of additional suppliers on the buyer's decision, the effect of correlation between suppliers' production costs and their cost adjustments, and additive as well as multiplicative total-cost functions. The results suggest that procurement managers can use their evaluations of suppliers' cost adjustments to make better auction format decisions.</description><author>Kostamis, Dimitris; Beil, Damian R.; Duenyas, Izak</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Duration of Patent Examination at the European Patent Office</title><link>http://www.example.com/articles/1</link><description>Harhoff, Dietmar; Wagner, Stefan
We analyze the duration and outcomes of patent examination at the European Patent Office utilizing an unusually rich data set covering a random sample of 215,265 applications filed between 1982 and 1998. In our empirical analysis, we distinguish between three groups of determinants: applicant characteristics, indicators of patent quality and value, and determinants that affect the complexity of the examination task. The results from an accelerated failure time model indicate that more controversial claims lead to slower grants but faster withdrawals, whereas well-documented applications are approved faster and withdrawn more slowly. We find strong evidence that applicants accelerate grant proceedings for their most valuable patents, but that they also prolong the battle for such patents if a withdrawal or refusal is imminent. This paper develops implications of these results for managerial decision making in research and development and innovation management.</description><author>Harhoff, Dietmar; Wagner, Stefan</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contracting in Supply Chains: A Laboratory Investigation</title><link>http://www.example.com/articles/1</link><description>Katok, Elena; Wu, Diana Yan
The coordination of supply chains by means of contracting mechanisms has been extensively explored theoretically but not tested empirically. We investigate the performance of three commonly studied supply chain contracting mechanisms: the wholesale price contract, the buyback contract, and the revenue-sharing contract. The simplified setting we consider utilizes a two-echelon supply chain in which the retailer faces the newsvendor problem, the supplier has no capacity constraints, and delivery occurs instantaneously. We compare the three mechanisms in a laboratory setting using a novel design that fully controls for strategic interactions between the retailer and the supplier. Results indicate that although the buyback and revenue-sharing contracts improve supply chain efficiency relative to the wholesale price contract, the improvement is smaller than the theory predicts. We also find that although the buyback and revenue-sharing contracts are mathematically equivalent, they do not generally result in equivalent supply chain performance.</description><author>Katok, Elena; Wu, Diana Yan</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multiattribute Utility Satisfying a Preference for Combining Good with Bad</title><link>http://www.example.com/articles/1</link><description>Tsetlin, Ilia; Winkler, Robert L.
An important challenge in multiattribute decision analysis is the choice of an appropriate functional form for the utility function. We show that if a decision maker prefers more of any attribute to less and prefers to combine good lotteries with bad, as opposed to combining good with good and bad with bad, her utility function should be a weighted average (a mixture) of multiattribute exponential utilities ("mixex utility"). In the single-attribute case, mixex utility satisfies properties typically thought to be desirable and encompasses most utility functions commonly used in decision analysis. In the multiattribute case, mixex utility implies aversion to any multivariate risk. Risk aversion with respect to any attribute decreases as that attribute increases. Under certain restrictions, such risk aversion also decreases as any other attribute increases, and a multivariate one-switch property is satisfied. One of the strengths of mixex utility is its ability to represent cases where utility independence does not hold, but mixex utility can be consistent with mutual utility independence and take on a multilinear form. An example illustrates the fitting of mixex utility to preference assessments.</description><author>Tsetlin, Ilia; Winkler, Robert L.</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Ambiguity Aversion and the Preference for Established Brands</title><link>http://www.example.com/articles/1</link><description>Muthukrishnan, A. V.; Wathieu, Luc; Xu, Alison Jing
We propose that ambiguity aversion, as introduced in the literature on decision making under uncertainty, drives a preference for established brands in multiattribute choices among branded alternatives. Established brands are those for which belief in quality is held with greater confidence, even if specific attributes might be inferior to those of competing, less-established brands. In five experiments, we examine the role of ambiguity aversion in the preference for dominated, established brands. We first show a correlation between ambiguity aversion (revealed through choices among monetary lotteries) and the preference for established brands. We then show that the preference for established brands is enhanced when ambiguity aversion is made more salient in unrelated preceding lottery choices. Thus, ambiguity aversion carries across choices. In addition, ambiguity aversion and the preference for established brands are both enhanced when subjects anticipate that others will evaluate their lottery choices. Finally, ambiguous information about brand attributes tends to increase the preference for established brands.</description><author>Muthukrishnan, A. V.; Wathieu, Luc; Xu, Alison Jing</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Shape and Term Structure of the Index Option Smirk: Why Multifactor Stochastic Volatility Models Work So Well</title><link>http://www.example.com/articles/1</link><description>Christoffersen, Peter; Heston, Steven; Jacobs, Kris
State-of-the-art stochastic volatility models generate a "volatility smirk" that explains why out-of-the-money index puts have high prices relative to the Black-Scholes benchmark. These models also adequately explain how the volatility smirk moves up and down in response to changes in risk. However, the data indicate that the slope and the level of the smirk fluctuate largely independently. Although single-factor stochastic volatility models can capture the slope of the smirk, they cannot explain such largely independent fluctuations in its level and slope over time. We propose to model these movements using a two-factor stochastic volatility model. Because the factors have distinct correlations with market returns, and because the weights of the factors vary over time, the model generates stochastic correlation between volatility and stock returns. Besides providing more flexible modeling of the time variation in the smirk, the model also provides more flexible modeling of the volatility term structure. Our empirical results indicate that the model improves on the benchmark Heston stochastic volatility model by 24% in-sample and 23% out-of-sample. The better fit results from improvements in the modeling of the term structure dimension as well as the moneyness dimension.</description><author>Christoffersen, Peter; Heston, Steven; Jacobs, Kris</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Effects of Problem Structure and Team Diversity on Brainstorming Effectiveness</title><link>http://www.example.com/articles/1</link><description>Kavadias, Stylianos; Sommer, Svenja C.
Since Osborn's Applied Imagination book in 1953 (Osborn, A. F. 1953. Applied Imagination: Principles and Procedures of Creative Thinking. Charles Scribner's Sons, New York), the effectiveness of brainstorming has been widely debated. While some researchers and practitioners consider it the standard idea generation and problem-solving method in organizations, part of the social science literature has argued in favor of nominal groups, i.e., the same number of individuals generating solutions in isolation. In this paper, we revisit this debate, and we explore the implications that the underlying problem structure and the team diversity have on the quality of the best solution as obtained by the different group configurations. We build on the normative search literature of new product development, and we show that no group configuration dominates. Therefore, nominal groups perform better in specialized problems, even when the factors that affect the solution quality exhibit complex interactions (problem complexity). In cross-functional problems, the brainstorming group exploits the competence diversity of its participants to attain better solutions. However, their advantage vanishes for extremely complex problems.</description><author>Kavadias, Stylianos; Sommer, Svenja C.</author><pubDate>Tue, 01 Dec 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Labor Market Institutions and Global Strategic Adaptation: Evidence from Lincoln Electric</title><link>http://www.example.com/articles/1</link><description>Siegel, Jordan I.; Larson, Barbara Zepp
Although one of the central questions in the global strategy field is how multinational firms successfully navigate multiple and often conflicting institutional environments, we know relatively little about the effect of conflicting labor market institutions on multinational firms' strategic choice and operating performance. With its decision to invest in manufacturing operations in nearly every one of the world's largest welding markets, Lincoln Electric offers us a quasi-experiment. We leverage a unique data set covering 1996-2006 that combines data on each host country's labor market institutions with data on each subsidiary's strategic choices and historical operating performance. We find that Lincoln Electric performed significantly better in countries with labor laws and regulations supporting manufacturers' interests and in countries that allowed the free use of both piecework and a discretionary bonus. Furthermore, we find that in countries with labor market institutions unfriendly to manufacturers, Lincoln Electric was still able to overcome most (although not all) of the institutional distance by what we term flexible intermediate adaptation.</description><author>Siegel, Jordan I.; Larson, Barbara Zepp</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Poker Player Behavior After Big Wins and Big Losses</title><link>http://www.example.com/articles/1</link><description>Smith, Gary; Levere, Michael; Kurtzman, Robert
We find that experienced poker players typically change their style of play after winning or losing a big pot-most notably, playing less cautiously after a big loss, evidently hoping for lucky cards that will erase their loss. This finding is consistent with Kahneman and Tversky's (Kahneman, D., A. Tversky. 1979. Prospect theory: An analysis of decision under risk. Econometrica 47(2) 263-292) break-even hypothesis and suggests that when investors incur a large loss, it might be time to take a vacation or be monitored closely.</description><author>Smith, Gary; Levere, Michael; Kurtzman, Robert</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue Driven Resource Allocation: Funding Authority, Incentives, and New Product Development Portfolio Management</title><link>http://www.example.com/articles/1</link><description>Chao, Raul O.; Kavadias, Stylianos; Gaimon, Cheryl
The first step in transforming strategy from a hopeful statement about the future into an operational reality is to allocate resources to innovation and new product development (NPD) programs in a portfolio. Resource allocation and NPD portfolio decisions often span multiple levels of the organization's hierarchy, leading to questions about how much authority to bestow on managers and how to structure incentives for NPD. In this study, we explore how funding authority and incentives affect a manager's allocation of resources between existing product improvement (relatively incremental projects) and new product development (more radical projects). Funding may be either fixed or variable depending on the extent to which the manager has the authority to use revenue derived from existing product sales to fund NPD efforts. We find that the use of variable funding drives higher effort toward improving existing products and developing new products. However, variable funding has a subtle side effect: it induces the manager to focus on existing product improvement to a greater degree than new product development, and the relative balance in the NPD portfolio shifts toward incremental innovation. In addition, we highlight a substitution effect between explicit incentives (compensation parameters) and implicit incentives (career concerns). Explicit incentives are reduced as career concerns become more salient.</description><author>Chao, Raul O.; Kavadias, Stylianos; Gaimon, Cheryl</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A General Interindustry Relatedness Index</title><link>http://www.example.com/articles/1</link><description>Bryce, David J.; Winter, Sidney G.
For empirical work in the resource-based view of the firm, characterizing the resources that are responsible for firm growth is difficult because valuable resources are often tacit, ambiguous, or difficult to identify. This is a particular problem for empirical assessments that rely upon the concept of relatedness between resources to characterize the direction of growth of the firm. We tackle the problem for the general case by developing a general interindustry relatedness index. The index harnesses the relatedness information embedded in the multiproduct organization decisions of every diversified firm in the U. S. manufacturing economy. The index is general in that it can be used across industry contexts without requiring explicit identification of resources and it provides a percentile relatedness rank for every possible pair of four-digit Standard Industrial Classification manufacturing industries. The general index is tested for predictive validity and found to perform as expected. Applications of the index in strategy research are suggested.</description><author>Bryce, David J.; Winter, Sidney G.</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competing Retailers and Inventory: An Empirical Investigation of General Motors' Dealerships in Isolated US Markets</title><link>http://www.example.com/articles/1</link><description>Olivares, Marcelo; Cachon, Gerard P.
We study the following question: How does competition influence the inventory holdings of General Motors' dealerships operating in isolated U. S. markets? We wish to disentangle two mechanisms by which local competition influences a dealer's inventory: (1) the entry or exit of a competitor can change a retailer's demand (a sales effect); and (2) the entry or exit of a competitor can change the amount of buffer stock a retailer holds, which influences the probability that a consumer finds a desired product in stock (a service-level effect). Theory is clear on the sales effect-an increase in sales leads to an increase in inventory (albeit a less than proportional increase). However, theoretical models of inventory competition are ambiguous on the expected sign of the service-level effect. Via a Web crawler, we obtained data on inventory and sales for more than 200 dealerships over a six-month period. Using cross-sectional variation, we estimated the effect of the number and type of local competitors on inventory holdings. We used several instrumental variables to control for the endogeneity of market entry decisions. Our results suggest that the service-level effect is strong, nonlinear, and positive. Hence, we observe that dealers carry more inventory (controlling for sales) when they face additional competition.</description><author>Olivares, Marcelo; Cachon, Gerard P.</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quality Disclosure Formats in a Distribution Channel</title><link>http://www.example.com/articles/1</link><description>Guo, Liang
Firms normally disclose quality information to consumers using two alternative formats: either directly to consumers or indirectly through downstream retailers. This study investigates optimal disclosure strategies/formats in a channel setting with bilateral monopolies. It shows that retail disclosure leads to more equilibrium information revelation. This is because the manufacturer can, through wholesale price cuts, partially absorb the retailer's effective disclosure cost and thus increase the retailer's incentive for disclosure. The conditions under which a particular disclosure format arises as the manufacturer's optimal choice are also examined. Even though direct disclosure is the ex post dominated option, the manufacturer may benefit from committing ex ante to the direct disclosure format when the cost of disclosure is sufficiently high.</description><author>Guo, Liang</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service Interruptions in Large-Scale Service Systems</title><link>http://www.example.com/articles/1</link><description>Pang, Guodong; Whitt, Ward
Large-scale service systems, where many servers respond to high demand, are appealing because they can provide great economy of scale, producing a high quality of service with high efficiency. Customer waiting times can be short, with a majority of customers served immediately upon arrival, while server utilizations remain close to 100%. However, we show that this confluence of quality and efficiency is not achieved without risk, because there can be severe congestion if the system does not operate as planned. In particular, we show that the large scale makes the system more vulnerable to service interruptions when (i) most customers remain waiting until they can be served, and (ii) when many servers are unable to function during the interruption, as may occur with a system-wide computer failure. Increasing scale leads to higher server utilizations, which in turn leads to longer recovery times from service interruptions and worse performance during such events. We quantify the impact of service interruptions with increasing scale by introducing and analyzing approximating deterministic fluid models. We also show that these fluid models can be obtained from many-server heavy-traffic limits.</description><author>Pang, Guodong; Whitt, Ward</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Impact of Workload on Service Time and Patient Safety: An Econometric Analysis of Hospital Operations</title><link>http://www.example.com/articles/1</link><description>Kc, Diwas S.; Terwiesch, Christian
Much of prior work in the area of service operations management has assumed service rates to be exogenous to the level of load on the system. Using operational data from patient transport services and cardiothoracic surgery-two vastly different health-care delivery services-we show that the processing speed of service workers is influenced by the system load. We find that workers accelerate the service rate as load increases. In particular, a 10% increase in load reduces length of stay by two days for cardiothoracic surgery patients, whereas a 20% increase in the load for patient transporters reduces the transport time by 30 seconds. Moreover, we show that such acceleration may not be sustainable. Long periods of increased load (overwork) have the effect of decreasing the service rate. In cardiothoracic surgery, an increase in overwork by 1% increases length of stay by six hours. Consistent with prior studies in the medical literature, we also find that overwork is associated with a reduction in quality of care in cardiothoracic surgery-an increase in overwork by 10% is associated with an increase in likelihood of mortality by 2%. We also find that load is associated with an early discharge of patients, which is in turn correlated with a small increase in mortality rate.</description><author>Kc, Diwas S.; Terwiesch, Christian</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cause Marketing: Spillover Effects of Cause-Related Products in a Product Portfolio</title><link>http://www.example.com/articles/1</link><description>Krishna, Aradhna; Rajan, Uday
The number of firms carrying a cause-related product has significantly increased in recent years. We consider a duopoly model of competition between firms in two products to determine which products a firm will link to a cause. We first test the behavioral underpinnings of our model in two laboratory experiments to demonstrate the existence of both a direct utility benefit to consumers from cause marketing ( CM) and a spillover benefit onto other products in the portfolio. Linking one product in a product portfolio to a cause can therefore increase sales both of that product and, via a spillover effect, of other products in the firm's portfolio. We construct a CM game in which each firm chooses which products, if any, to place on CM. In the absence of a spillover benefit, a firm places a product on CM if and only if it can increase its price by enough to compensate for the cost of CM. Thus, in equilibrium, firms either have both products or neither product on CM. However, with the introduction of a spillover benefit to the second product, this result changes. We show that if a single firm in the market links only one product to a cause, it can raise prices on both products and earn a higher profit. We assume each firm has an advantage in one product and show that there is an equilibrium in which each firm links only its disadvantaged product to a cause. If the spillover effect is strong, there is a second equilibrium in which each firm links only its advantaged product to a cause. In each case, firms raise their prices on both products and earn higher profits than when neither firm engages in CM. We also show that a firm will never place its entire portfolio on CM. Overall, our work implies that, by carrying cause-related products, companies can not only improve their image in the public eye but also increase profits.</description><author>Krishna, Aradhna; Rajan, Uday</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Dynamic Model for Posttraumatic Stress Disorder Among US Troops in Operation Iraqi Freedom</title><link>http://www.example.com/articles/1</link><description>Atkinson, Michael P.; Guetz, Adam; Wein, Lawrence M.
We develop a dynamic model in which Operation Iraqi Freedom (OIF) servicemembers incur a random amount of combat stress during each month of deployment, develop posttraumatic stress disorder (PTSD) if their cumulative stress exceeds a servicemember-specific threshold, and then develop symptoms of PTSD after an additional time lag. Using Department of Defense deployment data and Mental Health Advisory Team PTSD survey data to calibrate the model, we predict that-because of the long time lags and the fact that some surveyed servicemembers experience additional combat after being surveyed-the fraction of Army soldiers and Marines who eventually suffer from PTSD will be approximately twice as large as in the raw survey data. We cannot put a confidence interval around this estimate, but there is considerable uncertainty (perhaps +/-30%). The estimated PTSD rate translates into approximate to 300,000 PTSD cases among all Army soldiers and Marines in OIF, with approximate to 20,000 new cases each year the war is prolonged. The heterogeneity of threshold levels among servicemembers suggests that although multiple deployments raise an individual's risk of PTSD, in aggregate, multiple deployments lower the total number of PTSD cases by approximate to 30% relative to a hypothetical case in which the war was fought with many more servicemembers (i.e., a draft) deploying only once. The time lag dynamics suggest that, in aggregate, reserve servicemembers show symptoms approximate to 1-2 years before active servicemembers and predict that &gt;75% of OIF servicemembers who self-reported symptoms during their second deployment were exposed to the PTSD-generating stress during their first deployment.</description><author>Atkinson, Michael P.; Guetz, Adam; Wein, Lawrence M.</author><pubDate>Tue, 01 Sep 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technical Specialized Knowledge and Secondary Shares in Initial Public Offerings</title><link>http://www.example.com/articles/1</link><description>Junkunc, Marc T.; Eckhardt, Jonathan T.
This paper utilizes an understudied but often utilized aspect of initial public offerings (IPOs), secondary shares, to examine whether the knowledge conditions of firms give rise to agency problems that limit the ability of founders and venture capitalists to sell equity at IPO. In an analysis of 2,190 IPOs spanning from 1992 through 2002, we find that private owners are less likely to be observed selling their equity at IPO in ventures that are highly dependent on technical specialized knowledge as measured by the count of individuals with Ph.D.s in the top management team and board of directors. However, we find that this limit on the financial liquidity of founders and venture capitalists is alleviated when the venture's output has received greater market acceptance. Hence, the findings suggest that the financial liquidity of individuals involved in entrepreneurship is likely to be influenced by the knowledge conditions of their venture.</description><author>Junkunc, Marc T.; Eckhardt, Jonathan T.</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An Empirical Analysis of Search Engine Advertising: Sponsored Search in Electronic Markets</title><link>http://www.example.com/articles/1</link><description>Ghose, Anindya; Yang, Sha
The phenomenon of sponsored search advertising-where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results-is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most pro. table ones-profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.</description><author>Ghose, Anindya; Yang, Sha</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An Empirical Analysis of Scarcity Strategies in the Automobile Industry</title><link>http://www.example.com/articles/1</link><description>Balachander, Subramanian; Liu, Yan; Stock, Axel
Recent product introductions such as the Xbox 360, Sony Playstation 2, and PT Cruiser have been characterized by shortage of these products. Some experts have suggested that such scarcity can be a deliberate strategy for making the product more desirable. In this paper, we empirically examine the relationship between introductory inventory levels and consumer preference in the U. S. automobile industry and show that relative scarcity of a car at the time of introduction is associated with higher consumer preference for the product. Furthermore, we perform an empirical test of alternative theories about the rationale for introductory product scarcity. Specifically, we consider two theories of supplier-induced scarcity, namely the buying frenzy theory and the signaling theory, and an alternative theory that suggests that demand uncertainty causes introductory product scarcity. We find more support for the signaling theory of supplier-induced scarcity than the buying frenzy theory or the demand uncertainty theory in our analysis of the automobile market.</description><author>Balachander, Subramanian; Liu, Yan; Stock, Axel</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Breath of Fresh Air? Firm Type, Scale, Scope, and Selection Effects in Drug Development</title><link>http://www.example.com/articles/1</link><description>Arora, Ashish; Gambardella, Alfonso; Magazzini, Laura; Pammolli, Fabio
This paper compares the innovation performance of established pharmaceutical firms and biotech companies, controlling for differences in the scale and scope of research. We develop a structural model to analyze more than 3,000 drug research and development projects advanced to preclinical and clinical trials in the United States between 1980 and 1994. Key to our approach is careful attention to the issue of selection. Firms choose which compounds to advance into clinical trials. This choice depends not only on the technical promise of the compound, but also on commercial considerations such as the expected profitability of the market or concerns about product cannibalization. After controlling for selection, we find that (a) even after controlling for scale and scope in research, established pharmaceutical firms are more innovative than newly entered biotech firms; (b) older biotech firms display selection behaviors and innovation performances similar to established pharmaceutical firms; and (c) compounds licensed during preclinical trials are as likely to succeed as internal compounds of the licensor, which is inconsistent with the "lemons" hypothesis in technology markets.</description><author>Arora, Ashish; Gambardella, Alfonso; Magazzini, Laura; Pammolli, Fabio</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incentives for Retailer Forecasting: Rebates vs. Returns</title><link>http://www.example.com/articles/1</link><description>Taylor, Terry A.; Xiao, Wenqiang
This paper studies a manufacturer that sells to a newsvendor retailer who can improve the quality of her demand information by exerting costly forecasting effort. In such a setting, contracts play two roles: providing incentives to influence the retailer's forecasting decision and eliciting information obtained by forecasting to inform production decisions. We focus on two forms of contracts that are widely used in such settings and are mirror images of one another: a rebates contract, which compensates the retailer for the units she sells to end consumers, and a returns contract, which compensates the retailer for the units that are unsold. We characterize the optimal rebates contracts and returns contracts. Under rebates, the retailer, manufacturer, and total system may benefit from the retailer having inferior forecasting technology; this never occurs under returns. Although one might conjecture that returns would be inferior because its provision of "insurance" would discourage the retailer from forecasting, we show that returns are superior.</description><author>Taylor, Terry A.; Xiao, Wenqiang</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decision Support for Retirement Portfolio Management: Overcoming Myopic Loss Aversion via Technology Design</title><link>http://www.example.com/articles/1</link><description>Looney, Clayton Arlen; Hardin, Andrew M.
As firms continue to abandon pensions in favor of employee-managed retirement plans, tremendous demands are being placed on the decision-making proficiency of future retirees. As reflected in the equity premium puzzle, individual investors tend to hold overly conservative portfolios that provide meager payoffs over time. Consequently, there is growing concern that the vast majority of retirement accounts might be insufficiently funded when employees reach retirement. Given that most retirement plans can now be managed online, a potential solution lies in designing a Web-based decision support system (DSS) that helps future retirees make more-profitable portfolio management decisions. This paper reports the results of a study in which 159 retirement plan participants were asked to use an experimental website to manage a portfolio of retirement investments over a simulated 30-year period. Using a psychological approach toward designing the DSS, myopic loss aversion is put forth as a theoretical explanation for the psychological mechanisms that encourage investors to hold overly conservative portfolios. Armed with this knowledge, three design features-information horizon, system restrictiveness, and decisional guidance-are implemented as part of an overarching design strategy targeted at increasing investors' willingness to take calculated risks. The results indicate that investor conservatism diminishes when the DSS presents prospective probabilities and payoffs over long time horizons. In contrast, short-term information horizons constitute a major stumbling block for investors. However, when confronted with short-term information horizons, risk aversion can be successfully counteracted by configuring a DSS to either restrict the frequency of decisions or to suggest a relatively aggressive portfolio allocation. These findings carry important implications for theory and practice.</description><author>Looney, Clayton Arlen; Hardin, Andrew M.</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product Line Pricing in a Supply Chain</title><link>http://www.example.com/articles/1</link><description>Dong, Lingxiu; Narasimhan, Chakravarthi; Zhu, Kaijie
A vertically integrated channel would prefer to coordinate the pricing of its products. In this paper, we investigate drivers of product line pricing decisions in a bilateral monopoly where a manufacturer produces and sells two substitutable or complementary products to a retailer. In a two-stage game, each firm commits credibly in the first stage to a pricing scheme within its own organization: product line pricing (PLP) or non-product line pricing (NPLP). In the second stage, depending on the relative balance of power in the supply chain, the firms engage in either a Nash or a leader-follower pricing game. We study the equilibrium of the two-stage game under a general symmetric demand function. With strategic interaction between firms, a firm may choose NPLP as the equilibrium pricing strategy. In particular, when the second stage is a leader-follower game, the price leader chooses PLP, and the follower may choose NPLP only if the inefficiency of using NPLP empowers the follower by increasing the demand sensitivity to the leader's margin. When the second stage is a vertical Nash game, whether NPLP occurs in equilibrium depends on the nature of coupling between demand interdependence and vertical strategic dependence: NPLP can be an equilibrium only if products are demand substitutes (complements) and vertical strategic dependencies are complementary (substitutable). We find that prisoner's dilemma exists in the first stage for both types of second-stage pricing games. In those cases, one firm may have the incentive to commit to a pricing scheme in the first stage prior to its channel partner and steer the supply chain away from prisoner's dilemma.</description><author>Dong, Lingxiu; Narasimhan, Chakravarthi; Zhu, Kaijie</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Branch-and-Price Approach to the Share-of-Choice Product Line Design Problem</title><link>http://www.example.com/articles/1</link><description>Wang, Xinfang; Camm, Jeffrey D.; Curry, David J.
We develop a branch-and-price algorithm for constructing an optimal product line using partworth estimates from choice-based conjoint analysis. The algorithm determines the specific attribute levels for each multiattribute product in a set of products to maximize the resulting product line's share of choice, i.e., the number of respondents for whom at least one new product's utility exceeds the respondent's reservation utility. Computational results using large commercial and simulated data sets demonstrate that the algorithm can identify provably optimal, robust solutions to realistically sized problems.</description><author>Wang, Xinfang; Camm, Jeffrey D.; Curry, David J.</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Real-Time Delay Estimation in Overloaded Multiserver Queues with Abandonments</title><link>http://www.example.com/articles/1</link><description>Ibrahim, Rouba; Whitt, Ward
We use heavy-traffic limits and computer simulation to study the performance of alternative real-time delay estimators in the overloaded GI/GI/s+GI multiserver queueing model, allowing customer abandonment. These delay estimates may be used to make delay announcements in call centers and related service systems. We characterize performance by the expected mean squared error in steady state. We exploit established approximations for performance measures with a nonexponential abandonment-time distribution to obtain new delay estimators that effectively cope with nonexponential abandonment-time distributions.</description><author>Ibrahim, Rouba; Whitt, Ward</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the Relative Performance of Linear vs. Piecewise-Linear-Threshold Intertemporal Incentives</title><link>http://www.example.com/articles/1</link><description>Chen, Joseph Y.; Miller, Bruce L.
This paper employs numerical simulations to compare the relative performance of linear contracts with piecewise-linear-threshold contracts in the case where the agent chooses actions over time. These contracts are restricted to be functions of the ending value of aggregate output. We find strong evidence that only linear contracts need be considered in comparison with piecewise-linear-threshold contracts in the situation where cumulative output is only updated periodically and the agent's utility function is exponential. This finding holds even when there are only two periods and hence one change of action by the agent. However, we find that the best piecewise-linear-threshold contract is significantly superior to the best linear contract when the agent has a power utility function. These numerical simulations also call into question the use of a cap when the agent's compensation is based on the ending value of aggregate output and the agent's effort takes place over time.</description><author>Chen, Joseph Y.; Miller, Bruce L.</author><pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>SEC Rule 10b5-1 and Insiders' Strategic Trade</title><link>http://www.example.com/articles/1</link><description>Jagolinzer, Alan D.
The U.S. Securities and Exchange Commission enacted Rule 10b5-1 to deter insiders from trading with private information, yet also protect insiders' preplanned, non-information-based trades from litigation. Despite its requirement that insiders plan trades when not privately informed, the rule appears to enable strategic trade. Participating insiders' sales systematically follow positive and precede negative firm performance, generating abnormal forward-looking returns larger than those earned by nonparticipating colleagues. The observed association does not appear to be explained by market transaction disclosure response, "predictable" reversion following positive performance, or general periodic price declines. There is evidence, however, that a substantive proportion of randomly drawn plan initiations are associated with pending adverse news disclosures. There is also evidence that early sales plan terminations are associated with pending positive performance shifts, reducing the likelihood that insiders' sales execute at low prices. Collectively, this suggests that, on average, trading within the rule does not solely reflect uninformed diversification.</description><author>Jagolinzer, Alan D.</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Maintaining Diagnostic Knowledge-Based Systems: A Control-Theoretic Approach</title><link>http://www.example.com/articles/1</link><description>Bensoussan, Alain; Mookerjee, Radha; Mookerjee, Vijay; Yue, Wei T.
Diagnostic knowledge-based systems are used in a variety of application domains to support classification decisions. The effectiveness of such systems often decreases as the application environment or user preferences change over time. Hence, frequent adjustments to the system knowledge by a human expert become necessary. We study the problem of determining the optimal amount of effort that should be exerted to maintain the system over a planning horizon (finite or infinite). Using the receiver operating characteristic curve to derive a measure for system performance, we maximize system value by balancing system benefits with maintenance costs. The problem is cast as an optimal control model in which the goal is to choose the timing and extent of maintenance that must be expended to maximize system value. We find that the optimal solution usually possesses a steady-state component. The maintenance problem is also solved as a discrete, impulse control problem, as well as one where maintenance effort has a nonlinear impact on system performance.</description><author>Bensoussan, Alain; Mookerjee, Radha; Mookerjee, Vijay; Yue, Wei T.</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Simulating Sensitivities of Conditional Value at Risk</title><link>http://www.example.com/articles/1</link><description>Hong, L. Jeff; Liu, Guangwu
Conditional value at risk (CVaR) is both a coherent risk measure and a natural risk statistic. It is often used to measure the risk associated with large losses. In this paper, we study how to estimate the sensitivities of CVaR using Monte Carlo simulation. We first prove that the CVaR sensitivity can be written as a conditional expectation for general loss distributions. We then propose an estimator of the CVaR sensitivity and analyze its asymptotic properties. The numerical results show that the estimator works well. Furthermore, we demonstrate how to use the estimator to solve optimization problems with CVaR objective and/or constraints, and compare it to a popular linear programming-based algorithm.</description><author>Hong, L. Jeff; Liu, Guangwu</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling of Workflow Congestion and Optimization of Flow Routing in a Manufacturing/Warehouse Facility</title><link>http://www.example.com/articles/1</link><description>Zhang, Min; Batta, Rajan; Nagi, Rakesh
This paper discusses the notion of work flow congestion in the context of material handling equipment interruptions in a manufacturing or warehousing facility. Development of a combination of probabilistic and physics-based models for workflow interruptions permits evaluation of the expected link travel time. The problem is then of routing in a way that minimizes total expected travel time. The rerouting problem is modeled as a multicommodity flow problem with link capacity design. A greedy upper bounding and Lagrangean relaxation algorithm are developed to solve this efficiently. To calibrate our modeling process we develop an object-oriented simulation model that explicitly considers various workflow interruptions. Our major finding is that rerouting traffic in a congested facility can significantly alleviate congestion delays and improve the efficiency of material movement. A managerial insight derived from this work is that rerouting is most effective in medium traffic-intensity situations.</description><author>Zhang, Min; Batta, Rajan; Nagi, Rakesh</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic Programming Approach for Valuing Options in the GARCH Model</title><link>http://www.example.com/articles/1</link><description>Ben-Ameur, Hatem; Breton, Michele; Martinez, Juan-Manuel
In this paper, we develop an efficient algorithm to value options under discrete-time GARCH processes. We propose a procedure based on dynamic programming coupled with piecewise polynomial approximation to compute the value of a given option, at all observation dates and levels of the state vector. The method can be used for the large GARCH family of models based on Gaussian innovations and may accommodate all low-dimensional European as well as American derivatives. Numerical implementations show that this method competes very advantageously with other available valuation methods.</description><author>Ben-Ameur, Hatem; Breton, Michele; Martinez, Juan-Manuel</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance Analysis of a Queue with Congestion-Based Staffing Policy</title><link>http://www.example.com/articles/1</link><description>Zhang, Zhe George
This paper analyzes a waiting line system that is motivated by the operations of border-crossing stations between the United States and Canada. There are two main conflicting goals in such a system: high security level, which often leads to a longer line; and good customer service, which requires a shorter line. Thus, unlike other queueing systems, maintaining the average queue length within a certain range is the primary objective. This is achieved using a staffing policy, called "congestion-based staffing," or CBS, where the number of servers (inspection booths) is adjusted according to the queue length during a planning period. We first present an exact benchmark model of Markovian type based on the matrix-geometric solution. For practical CBS policies, we develop a set of closed-form formulas for the major performance measures based on regenerative cycle analysis and fluid limit approximation. Numerical examples show that these approximation formulas are simple, accurate, and robust for practitioners to use in designing CBS policies.</description><author>Zhang, Zhe George</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Scanning the Commons? Evidence on the Benefits to Startups Participating in Open Standards Development</title><link>http://www.example.com/articles/1</link><description>Waguespack, David M.; Fleming, Lee
This paper contributes large-sample evidence to an emerging discussion on open innovation and firm strategy. We ask why a startup should participate in an open standards community. We propose four ways that participation might increase a startup's chances of a liquidity event: gaining endorsement of the startup's technology standard, openly developing the startup's technology within the community (but not necessarily gaining endorsement), simply attending physical meetings of the community, and having startup members elected to leadership positions. Examination of venture-funded startups in the networking/data communications industry sectors reveals that those startups that participate in an open standards community (the Internet Engineering Task Force (IETF)) have a greater likelihood of an initial public offering or acquisition. The strongest effects are due to attendance, and conditional on high levels of attendance, holding leadership positions within the IETF. Surprisingly, standards endorsement is insignificant when controlling for simple physical attendance. These results are robust to instrumental variable methods and alternative coding of variables. In two-stage models we also find that prominent venture capitalists might help their portfolio companies by steering them to effective technology strategies, in this case active participation in the IETF, and not simply by lending status.</description><author>Waguespack, David M.; Fleming, Lee</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply Disruptions, Asymmetric Information, and a Backup Production Option</title><link>http://www.example.com/articles/1</link><description>Yang, Zhibin; Aydin, Goeker; Babich, Volodymyr; Beil, Damian R.
We study a manufacturer that faces a supplier privileged with private information about supply disruptions. We investigate how risk-management strategies of the manufacturer change and examine whether risk-management tools are more or less valuable in the presence of such asymmetric information. We model a supply chain with one manufacturer and one supplier, in which the supplier's reliability is either high or low and is the supplier's private information. On disruption, the supplier chooses to either pay a penalty to the manufacturer for the shortfall or use backup production to fill the manufacturer's order. Using mechanism design theory, we derive the optimal contract menu offered by the manufacturer. We find that information asymmetry may cause the less reliable supplier type to stop using backup production while the more reliable supplier type continues to use it. Additionally, the manufacturer may stop ordering from the less reliable supplier type altogether. The value of supplier backup production for the manufacturer is not necessarily larger under symmetric information; for the more reliable supplier type, it could be negative. The manufacturer is willing to pay the most for information when supplier backup production is moderately expensive. The value of information may increase as supplier types become uniformly more reliable. Thus, higher reliability need not be a substitute for better information.</description><author>Yang, Zhibin; Aydin, Goeker; Babich, Volodymyr; Beil, Damian R.</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Masters of War: Rivals' Product Innovation and New Advertising in Mature Product Markets</title><link>http://www.example.com/articles/1</link><description>Fosfuri, Andrea; Giarratana, Marco S.
We investigate the impact of rivals' product innovation and new advertising on a firm's financial market value in mature product markets. Our test bed is the carbonated soft drink market between 1999 and 2003, a period characterized by a near duopoly between Coca-Cola and Pespi. Empirically, we focus on new product announcements as a proxy of product innovation and on filed trademarks as a measure of new advertising. We find that rival product announcements decrease a firm's financial market value, and that rival filed trademarks increase it. Finally, we find that the effect of new advertising is channeled through market size dynamics, while that of product innovation operates through market share dynamics. Results are robust across different estimation techniques (event study, Tobin's q) and model specifications.</description><author>Fosfuri, Andrea; Giarratana, Marco S.</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Top Management Team Diversity and Firm Performance: Moderators of Functional-Background and Locus-of-Control Diversity</title><link>http://www.example.com/articles/1</link><description>Boone, Christophe; Hendriks, Walter
Past research on the relationship between top management team (TMT) compositional diversity and organizational performance has paid insufficient attention to the nature of TMT team processes in interaction with TMT diversity. We fill this gap by studying how three team mechanisms (collaborative behavior, accurate information exchange, and decision-making decentralization) moderate the impact of TMT diversity on financial performance of 33 information technology firms. We focus on two fundamentally different forms of TMT diversity: functional-background (FB) and locus-of-control (LOC). We argue that the former has the potential to enhance decision quality and organizational performance, whereas the latter might trigger relational conflict, and is, therefore, potentially detrimental to firm effectiveness. The ultimate aim of our study is to analyze which team processes help to transform distributed FB knowledge into high-quality decisions and organizational effectiveness, and which help avoid the potential detrimental effects of LOC diversity. We find that a TMT's collaborative behavior and information exchange are necessary conditions to unleash the performance benefits of FB diversity, but do not interact with LOC diversity. In addition, decentralized decision making spurs the effectiveness of functionally diverse teams, while at the same time reinforces the negative consequences of LOC diversity on firm performance.</description><author>Boone, Christophe; Hendriks, Walter</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Team Familiarity, Role Experience, and Performance: Evidence from Indian Software Services</title><link>http://www.example.com/articles/1</link><description>Huckman, Robert S.; Staats, Bradley R.; Upton, David M.
Much of the literature on team learning views experience as a unidimensional concept captured by the cumulative production volume of, or the number of projects completed by, a team. Implicit in this approach is the assumption that teams are stable in their membership and internal organization. In practice, however, such stability is rare, because the composition and structure of teams often change over time (e.g., between projects). In this paper, we use detailed data from an Indian software services firm to examine how such changes may affect the accumulation of experience within, and the performance of teams. We and that the level of team familiarity (i.e., the average number of times that each member has worked with every other member of the team) has a significant positive effect on performance, but we observe that conventional measures of the experience of individual team members (e.g., years at the. firm) are not consistently related to performance. We do find, however, that the role experience of individuals in a team (i.e., years in a given role within a team) is associated with better team performance. Our results offer an approach for capturing the experience held by. fluid teams and highlight the need to study context-specific measures of experience, including role experience. In addition, our findings provide insight into how the interactions of team members may contribute to the development of broader firm capabilities.</description><author>Huckman, Robert S.; Staats, Bradley R.; Upton, David M.</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Complementarity Among Vertical Integration Decisions: Evidence from Automobile Product Development</title><link>http://www.example.com/articles/1</link><description>Novak, Sharon; Stern, Scott
This paper examines complementarity among vertical integration decisions in automobile product development. Though most research assumes that contracting choices are independent of each other, contracting complementarity arises when the returns to a single vertical integration decision are increasing in the level of vertical integration associated with other contracting choices. First, effective coordination may depend on the level of (noncontractible) effort on the part of each agent; contracting complementarity results if coordination efforts are interdependent and vertical integration facilitates a higher level of noncontractible effort. Second, effective coordination may require the disclosure of proprietary trade secrets, and the potential for expropriation by external suppliers may induce complementarity among vertical integration choices. We provide evidence for complementarity in product development contracting by taking advantage of a detailed data set that includes the level of vertical integration and the contracting environment for individual automobile systems in the luxury automobile segment. Using an instrumental variables framework that distinguishes complementarity from unobserved firm-level factors, the evidence is consistent with the hypothesis that contracting complementarity is an important driver of vertical integration choices. The findings suggest that contracting complementarity may be particularly important when coordination is important to achieve but difficult to monitor.</description><author>Novak, Sharon; Stern, Scott</author><pubDate>Sun, 01 Feb 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal Auditing with Scoring: Theory and Application to Insurance Fraud</title><link>http://www.example.com/articles/1</link><description>Dionne, Georges; Giuliano, Florence; Picard, Pierre
This article makes a bridge between the theory of optimal auditing and the scoring methodology in an asymmetric information setting. Our application is meant for insurance claims fraud, but it can be applied to many other activities that use the scoring approach. Fraud signals are classified based on the degree to which they reveal an increasing probability of fraud. We show that the optimal auditing strategy takes the form of a "red flags strategy," which consists in referring claims to a special investigative unit (SIU) when certain fraud indicators are observed. The auditing policy acts as a deterrence device, and we explain why it requires the commitment of the insurer and how it should affect the incentives of SIU staffs. The characterization of the optimal auditing strategy is robust to some degree of signal manipulation by defrauders as well as to the imperfect information of defrauders about the audit frequency. The model is calibrated with data from a large European insurance company. We show that it is possible to improve our results by separating different groups of insureds with different moral costs of fraud. Finally, our results indicate how the deterrence effect of the audit scheme can be taken into account and how it affects the optimal auditing strategy.</description><author>Dionne, Georges; Giuliano, Florence; Picard, Pierre</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition Between Local and Electronic Markets: How the Benefit of Buying Online Depends on Where You Live</title><link>http://www.example.com/articles/1</link><description>Forman, Chris; Ghose, Anindya; Goldfarb, Avi
Our paper shows that the parameters in existing theoretical models of channel substitution such as offline transportation cost, online disutility cost, and the prices of online and offline retailers interact to determine consumer choice of channels. In this way, our results provide empirical support for many such models. In particular, we empirically examine the trade-off between the benefits of buying online and the benefits of buying in a local retail store. How does a consumer's physical location shape the relative benefits of buying from the online world? We explore this problem using data from Amazon.com on the top-selling books for 1,497 unique locations in the United States for 10 months ending in January 2006. We show that when a store opens locally, people substitute away from online purchasing, even controlling for product-specific preferences by location. These estimates are economically large, suggesting that the disutility costs of purchasing online are substantial and that offline transportation costs matter. We also show that offline entry decreases consumers' sensitivity to online price discounts. However, we find no consistent evidence that the breadth of the product line at a local retail store affects purchases.</description><author>Forman, Chris; Ghose, Anindya; Goldfarb, Avi</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic Pricing in the Presence of Strategic Consumers and Oligopolistic Competition</title><link>http://www.example.com/articles/1</link><description>Levin, Yuri; McGill, Jeff; Nediak, Mikhail
We present a dynamic pricing model for oligopolistic firms selling differentiated perishable goods to multiple finite segments of strategic consumers who are aware that pricing is dynamic and may time their purchases accordingly. This model encompasses strategic behavior by both firms and consumers in a unified stochastic dynamic game in which each firm's objective is to maximize its total expected revenues, and each consumer responds according to a shopping-intensity-allocation consumer choice model. We prove the existence of a unique subgame-perfect equilibrium, provide equilibrium optimality conditions, and prove monotonicity results for special cases. The model provides insights about equilibrium price dynamics under different levels of competition, asymmetry between firms, and multiple market segments with varying properties. We demonstrate that strategic behavior by consumers can have serious impacts on revenues if firms ignore that behavior in their dynamic pricing policies. Moreover, ideal equilibrium responses to consumer strategic behavior can recover only a portion of the lost revenues. A key conclusion is that firms may benefit more from limiting the information available to consumers than from allowing full information and responding to the resulting strategic behavior in an optimal fashion.</description><author>Levin, Yuri; McGill, Jeff; Nediak, Mikhail</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Volume and Price Patterns Around a Stock's 52-Week Highs and Lows: Theory and Evidence</title><link>http://www.example.com/articles/1</link><description>Huddart, Steven; Lang, Mark; Yetman, Michelle H.
We provide large sample evidence that past price extremes influence investors' trading decisions. Volume is strikingly higher, in both economic and statistical terms, when the stock price crosses either the upper or lower limit of its past trading range. This increase in volume is more pronounced the longer the time since the stock price last achieved the price extreme, the smaller the firm, the higher the individual investor interest in the stock, and the greater the ambiguity regarding valuation. These results are robust across model specifications and controls for past returns and news arrival. Volume spikes when price crosses either the upper or lower limit of the past trading range, then gradually subsides. After either event, returns are reliably positive and, among small investors, trades classified as buyer-initiated are elevated. Overall, results are more consistent with bounded rationality than with other candidate explanations.</description><author>Huddart, Steven; Lang, Mark; Yetman, Michelle H.</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Metrics-When and Why Nonaveraging Statistics Work</title><link>http://www.example.com/articles/1</link><description>Shugan, Steven M.; Mitra, Debanjan
Good metrics are well-defined formulae (often involving averaging) that transmute multiple measures of raw numerical performance (e. g., dollar sales, referrals, number of customers) to create informative summary statistics (e. g., average share of wallet, average customer tenure). Despite myriad uses (benchmarking, monitoring, allocating resources, diagnosing problems, explanatory variables), most uses require metrics that contain information summarizing multiple observations. On this criterion, we show empirically (with people data) that although averaging has remarkable theoretical properties, supposedly inferior nonaveraging metrics (e. g., maximum, variance) are often better. We explain theoretically (with exact proofs) and numerically (with simulations) when and why. For example, when the environment causes a correlation between observed sample sizes (e. g., number of past purchases, projects, observations) and latent underlying parameters (e. g., the likelihood of favorable outcomes), the maximum statistic is a better metric than the mean. We refer to this environmental effect as the Muth effect, which occurs when rational markets provide more opportunities (i.e., more observations) to individuals and organizations with greater innate ability. Moreover, when environments are adverse (e. g., failure-rich), nonaveraging metrics correctly overweight favorable outcomes. We refer to this environmental effect as the Anna Karenina effect, which occurs when less-favorable outcomes convey less information. These environmental effects impact metric construction, selection, and employment.</description><author>Shugan, Steven M.; Mitra, Debanjan</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Vision for Management Science</title><link>http://www.example.com/articles/1</link><description>Cachon, Gerard P.
nan</description><author>Cachon, Gerard P.</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Purchasing, Pricing, and Quick Response in the Presence of Strategic Consumers</title><link>http://www.example.com/articles/1</link><description>Cachon, Gerard P.; Swinney, Robert
We consider a retailer that sells a product with uncertain demand over a. finite selling season. The retailer sets an initial stocking quantity and, at some predetermined point in the season, optimally marks down remaining inventory. We modify this classic setting by introducing three types of consumers: myopic consumers, who always purchase at the initial full price; bargain-hunting consumers, who purchase only if the discounted price is sufficiently low; and strategic consumers, who strategically choose when to make their purchase. A strategic consumer chooses between a purchase at the initial full price and a later purchase at an uncertain markdown price. In equilibrium, strategic consumers and the retailer make optimal decisions given their rational expectations regarding future prices, availability of inventory, and the behavior of other consumers. We. find that the retailer stocks less, takes smaller price discounts, and earns lower profit if strategic consumers are present than if there are no strategic consumers. We. find that a retailer should generally avoid committing to a price path over the season (assuming such commitment is feasible)-committing to a markdown price (or to not mark down at all) is often too costly (inventory may remain unsold) even in the presence of strategic consumers; the better approach is to be cautious with the initial quantity and then mark down optimally. Furthermore, we discuss the value of quick response (the ability to procure additional inventory after obtaining updated demand information, albeit at a higher unit cost than the initial order). We. find that the value of quick response to a retailer is generally much greater in the presence of strategic consumers than without them: on average 67% more valuable and as much as 558% more valuable in our sample. In other words, although it is well established in the literature that quick response provides value by allowing better matching of supply with demand, it provides more value, often substantially more value, by allowing a retailer to control the negative consequences of strategic consumer behavior.</description><author>Cachon, Gerard P.; Swinney, Robert</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dirichlet Bridge Sampling for the Variance Gamma Process: Pricing Path-Dependent Options</title><link>http://www.example.com/articles/1</link><description>Kaishev, Vladimir K.; Dimitrova, Dimitrina S.
The authors develop a new Monte Carlo-based method for pricing path-dependent options under the variance gamma (VG) model. The gamma bridge sampling method proposed by Avramidis et al. (Avramidis, A. N., P. L'Ecuyer, P. A. Tremblay. 2003. Efficient simulation of gamma and variance-gamma processes. Proc. 2003 Winter Simulation Conf. IEEE Press, Piscataway, NJ, 319-326) and Ribeiro and Webber (Ribeiro, C., N. Webber. 2004. Valuing path-dependent options in the variance-gamma model by Monte Carlo with a gamma bridge. J. Computational Finance 7(2) 81-100) is generalized to a multivariate (Dirichlet) construction, bridging "simultaneously" over all time partition points of the trajectory of a gamma process. The generation of the increments of the gamma process, given its value at the terminal point, is interpreted as a Dirichlet partition of the unit interval. The increments are generated in a decreasing stochastic order and, under the Kingman limit, have a known distribution. Thus, simulation of a trajectory from the gamma process requires generating only a small number of uniforms, avoiding the expensive simulation of beta variates via numerical probability integral inversion. The proposed method is then applied in simulating the trajectory of a VG process using its difference-of-gammas representation. It has been implemented in both plain Monte Carlo and quasi-Monte Carlo environments. It is tested in pricing lookback, barrier, and Asian options and is shown to provide consistent efficiency gains, compared to the sequential method and the difference-of-gammas bridge sampling proposed by Avramidis and L'Ecuyer (Avramidis, A. N., P. L'Ecuyer. 2006. Efficient Monte Carlo and quasi-Monte Carlo option pricing under the variance gamma model. Management Sci. 52(12) 1930-1944).</description><author>Kaishev, Vladimir K.; Dimitrova, Dimitrina S.</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An Investigation of Time Inconsistency</title><link>http://www.example.com/articles/1</link><description>Sayman, Serdar; Onculer, Ayse
Preference between two future outcomes may change over time-a phenomenon labeled as time inconsistency. The term "time inconsistency" is usually used to refer to cases in which a larger-later outcome is preferred over a smaller-sooner one when both are delayed by some time, but then with the passage of time a preference switches to the smaller-sooner outcome. The current paper presents four empirical studies showing that time inconsistency in the other direction is also possible: A person may prefer the smaller-sooner outcome when both options are in the future, but decide to wait for the larger-later one when the smaller option becomes immediately available. We. find that such "reverse time inconsistency" is more likely to be observed when the delays to and between the two outcomes are short (up to a week). We propose that reverse time inconsistency may be associated with a reversed-S shape discount function, and provide evidence that such a discount function captures part of the variation in intertemporal preferences.</description><author>Sayman, Serdar; Onculer, Ayse</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Persistency Model and Its Applications in Choice Modeling</title><link>http://www.example.com/articles/1</link><description>Natarajan, Karthik; Song, Miao; Teo, Chung-Piaw
Given a discrete maximization problem with a linear objective function where the coefficients are chosen randomly from a distribution, we would like to evaluate the expected optimal value and the marginal distribution of the optimal solution. We call this the persistency problem for a discrete optimization problem under uncertain objective, and the marginal probability mass function of the optimal solution is named the persistence value. In general, this is a difficult problem to solve, even if the distribution of the objective coefficient is well specified. In this paper, we solve a subclass of this problem when the distribution is assumed to belong to the class of distributions defined by given marginal distributions, or given marginal moment conditions. Under this model, we show that the persistency problem maximizing the expected objective value over the set of distributions can be solved via a concave maximization model. The persistency model solved using this formulation can be used to obtain important qualitative insights to the behavior of stochastic discrete optimization problems. We demonstrate how the approach can be used to obtain insights to problems in discrete choice modeling. Using a set of survey data from a transport choice modeling study, we calibrate the random utility model with choice probabilities obtained from the persistency model. Numerical results suggest that our persistency model is capable of obtaining estimates that perform as well, if not better, than classical methods, such as logit and cross-nested logit models. We can also use the persistency model to obtain choice probability estimates for more complex choice problems. We illustrate this on a stochastic knapsack problem, which is essentially a discrete choice problem under budget constraint.</description><author>Natarajan, Karthik; Song, Miao; Teo, Chung-Piaw</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic Information Management Under Leakage in a Supply Chain</title><link>http://www.example.com/articles/1</link><description>Anand, Krishnan S.; Goyal, Manu
The importance of material. flow management for a profit-maximizing. firm has been well articulated in the supply chain literature. We demonstrate in our analytical model that a. firm must also actively manage information. flows within the supply chain, which translates to controlling what it knows, as well as what its competitors and suppliers know. In our model of horizontal competition between an informed and an uninformed. firm with a common upstream supplier, material and information. flows intersect through leakage of demand (order) information to unintended recipients. As a result, the informed. firm's drive to control information. flows within the supply chain can trigger operational losses through material. flow distortion. These losses can be so severe that the. firm may prefer not to acquire information even when it is costless to do so. Our results underscore the importance of strategic information management-actively managing the supply chain's information. flows, and making trade-offs with material. flows where appropriate, to maximize profits.</description><author>Anand, Krishnan S.; Goyal, Manu</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Economic Analysis of Simulation Selection Problems</title><link>http://www.example.com/articles/1</link><description>Chick, Stephen E.; Gans, Noah
Ranking and selection procedures are standard methods for selecting the best of a. finite number of simulated design alternatives based on a desired level of statistical evidence for correct selection. But the link between statistical significance and. financial significance is indirect, and there has been little or no research into it. This paper presents a new approach to the simulation selection problem, one that maximizes the expected net present value of decisions made when using stochastic simulation. We provide a framework for answering these managerial questions: When does a proposed system design, whose performance is unknown, merit the time and money needed to develop a simulation to infer its performance? For how long should the simulation analysis continue before a design is approved or rejected? We frame the simulation selection problem as a "stoppable" version of a Bayesian bandit problem that treats the ability to simulate as a real option prior to project implementation. For a single proposed system, we solve a free boundary problem for a heat equation that approximates the solution to a dynamic program that. finds optimal simulation project stopping times and that answers the managerial questions. For multiple proposed systems, we extend previous Bayesian selection procedures to account for discounting and simulation-tool development costs.</description><author>Chick, Stephen E.; Gans, Noah</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Asymptotic Optimality of Order-Up-To Policies in Lost Sales Inventory Systems</title><link>http://www.example.com/articles/1</link><description>Huh, Woonghee Tim; Janakiraman, Ganesh; Muckstadt, John A.; Rusmevichientong, Paat
We study a single-product single-location inventory system under periodic review, where excess demand is lost and the replenishment lead time is positive. The performance measure of interest is the long-run average holding cost and lost sales penalty cost. For a large class of demand distributions, we show that when the lost sales penalty becomes large compared to the holding cost, the relative difference between the cost of the optimal policy and the best order-up-to policy converges to zero. For any given cost parameters, we establish a bound on this relative difference. Numerical experiments show that the best order-up-to policy performs well, yielding an average cost that is within 1.5% of the optimal cost when the ratio between the lost sales penalty and the holding cost is 100. We also propose a heuristic order-up-to level using two newsvendor expressions; in our experiments, the cost of this order-up-to policy is 2.52% higher, on an average, than the best order-up-to policy.</description><author>Huh, Woonghee Tim; Janakiraman, Ganesh; Muckstadt, John A.; Rusmevichientong, Paat</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Causal Ambiguity, Complexity, and Capability-Based Advantage</title><link>http://www.example.com/articles/1</link><description>Ryall, Michael D.
This paper presents the. first formal examination of role of causal ambiguity as a barrier to imitation. Here, the aspiring imitator faces a knowledge (i.e., "capabilities-based") barrier to imitation that is both causal and ambiguous in a precise sense of both words. Imitation conforms to a well-explicated process of learning by observing. I provide a precise distinction between the intrinsic causal ambiguity associated with a particular strategy and the subjective ambiguity perceived by a challenger. I. find that intrinsic ambiguity is a necessary but insufficient condition for a sustained capability-based advantage. I also demonstrate that combinatorial complexity, a phenomenon that has attracted the recent attention of strategy theorists, and causal ambiguity are distinct barriers to imitation. The former acts as a barrier to explorative/active learning and the latter as one to absorptive/passive learning. One implication of this is that learning by doing and learning by observing are complementary strategic activities, not substitutes-in most cases, we should expect. firm strategies to seek performance enhancement using efforts of both types.</description><author>Ryall, Michael D.</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Spillover Asymmetry and Why It Matters</title><link>http://www.example.com/articles/1</link><description>Knott, Anne Marie; Posen, Hart E.; Wu, Brian
Although spillovers are a crucial factor in determining the optimal environment for innovation, there is no consensus regarding their impact on. firm behavior. One reason for this may be that models differ in their assumptions for the functional form of the spillover pool. In industrial organization and economic geography, for example, the predominant convention is that all innovation within an industry/region contributes to a spillover pool that has a common value for all. firms. An alternative convention prevalent in endogenous growth and evolutionary economics is that spillovers have directionality-the size of the relevant pool differs across. firms. Knowing the correct functional form may facilitate theoretical consensus, either analytically (by modifying models' assumptions) or empirically (by supporting a critical test of competing theories). We characterize and test the functional form of spillover pools for efficiency-enhancing innovation across 50 markets in the banking industry. Our results in that setting are consistent with expectations for asymmetric spillovers but inconsistent with expectations for pooled spillovers.</description><author>Knott, Anne Marie; Posen, Hart E.; Wu, Brian</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventories with Multiple Supply Sources and Networks of Queues with Overflow Bypasses</title><link>http://www.example.com/articles/1</link><description>Song, Jing-Sheng; Zipkin, Paul
Consider an inventory system with multiple supply sources and Poisson demand. The replenishment lead times from each source are stochastic, representing congestion and disruption. We develop performance evaluation and optimization tools for a family of reasonable order policies. These policies take into account real-time supply information, which can be obtained through tracking technologies such as global positioning systems and radio frequency identification. Performance evaluation of such state-dependent policies is generally hard. The main thrust of this paper is to show that, under these policies, the supply system becomes a network of queues with a special routing mechanism called an over flow bypass. The solution has a simple product form. Thus, we obtain closed-form performance measures. These results reinterpret and extend the existing analysis of a system with two sources having deterministic lead times. We further extend the analysis to batch ordering policies, non-Poisson demand processes, and multiple demand classes.</description><author>Song, Jing-Sheng; Zipkin, Paul</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Searching for Google's Value: Using Prediction Markets to Forecast Market Capitalization Prior to an Initial Public Offering</title><link>http://www.example.com/articles/1</link><description>Berg, Joyce E.; Neumann, George R.; Rietz, Thomas A.
We conducted prediction markets designed to forecast post-initial public offering (IPO) valuations before a particularly unique IPO: Google. The prediction markets forecast Google's post-IPO market capitalization relatively accurately. While Google's auction-based IPO price was 15.3% below the first-day closing market capitalization, the final prediction market forecast was only 4.0% above it. The forecast also accorded with the level of over-subscription in the IPO auction. Evidence available to both outsiders (from the prediction market forecasts) and insiders (through the orders in Google's auction) predicted similar degrees of underpricing. We argue that, with repetition, such markets could provide useful information for understanding the IPO process.</description><author>Berg, Joyce E.; Neumann, George R.; Rietz, Thomas A.</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effects of E-Waste Regulation on New Product Introduction</title><link>http://www.example.com/articles/1</link><description>Plambeck, Erica; Wang, Qiong
This paper investigates the impact of e-waste regulation on new product introduction in a stylized model of the electronics industry. Manufacturers choose the development time and expenditure for each new version of a durable product, which together determine its quality. Consumers purchase the new product and dispose of the last-generation product, which becomes e-waste. The price of a new product strictly increases with its quality and consumers' rational expectation about the time until the next new product will be introduced. "Feeupon-sale" types of e-waste regulation cause manufacturers to increase their equilibrium development time and expenditure, and thus the incremental quality for each new product. As new products are introduced ( and disposed of) less frequently, the quantity of e-waste decreases and, even excluding the environmental benefits, social welfare may increase. Consumers pay a higher price for each new product because they anticipate using it for longer, which increases manufacturers' profits. Unfortunately, existing "fee-upon-sale" types of e-waste regulation fail to motivate manufacturers to design for recyclability. In contrast, "fee-upon-disposal" types of e-waste regulation such as individual extended producer responsibility motivate design for recyclability but, in competitive product categories, fail to reduce the frequency of new product introduction.</description><author>Plambeck, Erica; Wang, Qiong</author><pubDate>Sun, 01 Mar 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Satisficing Measures for Analysis of Risky Positions</title><link>http://www.example.com/articles/1</link><description>Brown, David B.; Sim, Melvyn
In this work we introduce a class of measures for evaluating the quality of financial positions based on their ability to achieve desired financial goals. In the spirit of Simon (Simon, H. A. 1959. Theories of decision-making in economics and behavioral science. Amer. Econom. Rev. 49(3) 253-283), we call these measures satisficing measures and show that they are dual to classes of risk measures. This approach has the advantage that aspiration levels, either competing benchmarks or fixed targets, are often much more natural to specify than risk tolerance parameters. In addition, we propose a class of satisficing measures that reward diversification. Finding optimal portfolios for such satisficing measures is computationally tractable. Moreover, this class of satisficing measures has an ambiguity interpretation in terms of robust guarantees on the expected performance because the underlying distribution deviates from the investor's reference distribution. Finally, we show some promising results for our approach compared to traditional methods in a real-world portfolio problem against a competing benchmark.</description><author>Brown, David B.; Sim, Melvyn</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploring the Relationship Between Scientist Human Capital and Firm Performance: The Case of Biomedical Academic Entrepreneurs in the SBIR Program</title><link>http://www.example.com/articles/1</link><description>Toole, Andrew A.; Czarnitzki, Dirk
There is an emerging debate in the scholarly literature regarding the extent to which academic human capital contributes to firm performance. This debate centers on the nature of an academic scientist's human capital and its institutional specificity. Using data on the human capital of biomedical scientists developed during their careers in academe, this paper analyzes how the depth of their scientifically and commercially oriented academic human capital contributes to firm performance when these scientists subsequently start or join for-profit firms. We find that the scientific and commercial components of an academic scientist's human capital have differential effects on the performance of research and invention tasks at the. firm. We also find that the contribution of an academic scientist to a firm's patent productivity is decreasing with the depth of their scientifically oriented human capital, all else constant. These results support the view that academic human capital is heterogeneous and has an institutional specificity that mediates its value when applied in a commercialization environment.</description><author>Toole, Andrew A.; Czarnitzki, Dirk</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Value of Partial Resource Pooling: Should a Service Network Be Integrated or Product-Focused?</title><link>http://www.example.com/articles/1</link><description>Ata, Baris; Van Mieghem, Jan A.
We investigate how dynamic resource substitution in service systems impacts capacity requirements and responsiveness. Inspired by the contrasting network strategies of FedEx and United Parcel Service (UPS), we study when two service classes (e. g., express or regular) should be served by dedicated resources (e. g., air or ground) or by an integrated network (e. g., air also serves regular). Using call center terminology, the question is whether to operate two independent queues or one N-network. We present analytic expressions for the delay distributions and the value of network integration through partial resource pooling. These show how the value of network integration depends on service quality (speed and reliability of service) and demand characteristics (volume averages and covariance matrix). Our results suggest that network integration is of little value and operating dedicated networks is a fine strategy if the firm primarily serves express requests with high reliability and if the correlation with regular requests is not strongly negative. In contrast, network integration offers significant gains for firms serving primarily regular requests, almost independent of correlation. Our analysis provides the intuition behind these findings in terms of three main drivers of integration value: arrival pooling, the substitution effect, and the correlation effect.</description><author>Ata, Baris; Van Mieghem, Jan A.</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Role of Robust Optimization in Single-Leg Airline Revenue Management</title><link>http://www.example.com/articles/1</link><description>Birbil, S. Ilker; Frenk, J. B. G.; Gromicho, Joaquim A. S.; Zhang, Shuzhong
In this paper, we introduce robust versions of the classical static and dynamic single-leg seat allocation models. These robust models take into account the inaccurate estimates of the underlying probability distributions. As observed by simulation experiments, it turns out that for these robust versions the variability compared to their classical counterparts is considerably reduced with a negligible decrease in average revenue.</description><author>Birbil, S. Ilker; Frenk, J. B. G.; Gromicho, Joaquim A. S.; Zhang, Shuzhong</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Allocating Objects in a Network of Caches: Centralized and Decentralized Analyses</title><link>http://www.example.com/articles/1</link><description>Tawarmalani, Mohit; Kannan, Karthik; De, Prabuddha
We analyze the allocation of objects in a network of caches that collaborate to service requests from customers. A thorough analysis of this problem in centralized and decentralized setups, both of which occur in practice, is essential for understanding the benefits of collaboration. A key insight offered by this paper is that an efficient implementation of cooperative cache management is possible because, in the centralized scenario, the object allocation resulting in the best social welfare can be found easily as a solution to a transportation problem. For the decentralized scenario involving selfish caches, it is shown that pure equilibria exist and that the cache network always reaches a pure equilibrium in a finite number of steps, starting from any point in the strategy space. An auction mechanism is developed to derive prices that motivate the caches to hold objects in a manner such that the optimal social welfare is attained. In the special case of symmetric caches, simple algorithms are devised to find the optimal social welfare allocation, the best pure equilibrium, and the prices for sharing objects. The results obtained in this paper should be valuable in developing and evaluating cache-management policies. Resource-sharing problems with a similar cost structure exist in a variety of other domains, and the insights gained here are expected to extend to those scenarios as well.</description><author>Tawarmalani, Mohit; Kannan, Karthik; De, Prabuddha</author><pubDate>Thu, 01 Jan 2009 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A queueing analysis to determine how many additional beds are needed for the detention and removal of illegal aliens</title><link>http://www.example.com/articles/1</link><description>Liu, Yifan; Wein, Lawrence M.
Due to lack of detention capacity ( the U. S. government measures capacity by the number of detention beds), tens of thousands of apprehended illegal aliens are released into the U. S. interior each year, instead of being removed from the country. This vulnerability can be exploited by terrorist groups wanting to enter the United States. We construct a queueing model of the U. S. detention and removal operations, and derive approximate analytical expressions for key performance measures, including a simple normal approximation for the required number of beds. Due to shortcomings in the U. S. government's data collection procedures, we cannot directly estimate all of the model's parameter values. Consequently, we use the approximate analytical expressions and the 2003 U. S. government data quantifying these key performance measures to estimate several unknown parameter values. Although current funding is for approximately 21,000 detention beds, we estimate that approximately 34,500 beds are needed to remove all potential detainees ( this does not include nonviolent, noncriminal Mexicans, who are returned to Mexico within several hours) based on 2003 data. The dramatic increase in the arrivals of potential detainees since 2003 suggests that approximately 50,000 beds are currently required, although the estimation of future arrival rates is very difficult due to uncertainties about the future direction of U. S. immigration policy. Our estimated bed requirements are approximately 25% higher than naive estimates that fail to account for right censoring of residence times due to some detainees being released from detention before removal to make way for higher-priority detainees.</description><author>Liu, Yifan; Wein, Lawrence M.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>When rational sellers face nonrational buyers: Evidence from herding on eBay</title><link>http://www.example.com/articles/1</link><description>Simonsohn, Uri; Ariely, Dan
People often observe others' decisions before deciding themselves. Using eBay data for DVD auctions we explore the consequences of neglecting nonsalient information when making such inferences. We show that bidders herd into auctions with more existing bids, even if these are a signal of no-longer-available lower starting prices rather than of higher quality. Bidders bidding a given dollar amount are less likely to win low starting price auctions, and pay more for them when they do win. Experienced bidders are less likely to bid on low starting price auctions. Remarkably, the seller side of the market is in equilibrium, because expected revenues are nearly identical for high and low starting prices.</description><author>Simonsohn, Uri; Ariely, Dan</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrating the number and location of retail outlets on a line with replenishment decisions</title><link>http://www.example.com/articles/1</link><description>Naseraldin, Hussein; Herer, Yale T.
We research the management approach that quantitatively combines decisions that affect different planning horizons-namely, the strategic and operational ones-and simultaneously derive the optimal values of these decisions. The system we investigate comprises retail outlets and customers in an in finite-horizon setting. Both retail outlets and customers are located on a finite homogenous line segment. The total demand posed by customers is normally distributed with known mean and variance. To optimally design and operate such a system, we need to determine the optimal values of the number of retail outlets, the location of each retail outlet, and the replenishment inventory levels maintained at each retail outlet. We analyze the system from an expected cost point of view, considering the fixed costs of operating the retail outlets, the expected holding and shortage costs, and the expected delivery costs. We show that all decisions can be represented as a function of the number of retail outlets. Moreover, we show that the system's expected cost function is quasi-convex in the number of retail outlets. We compare our model to a model that does not integrate these decisions at once. We show the advantage of our approach on both the solution and objective spaces. We propose an exact quanti. cation of this advantage in terms of the cost and problem parameters. In addition, we point out several managerial insights.</description><author>Naseraldin, Hussein; Herer, Yale T.</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Look before you leap: Market opportunity identification in emerging technology firms</title><link>http://www.example.com/articles/1</link><description>Gruber, Marc; MacMillan, Ian C.; Thompson, James D.
Entrepreneurs play a fundamental role in bringing new technologies to market. Because technologies are often configurable to serve a variety of different markets, it is possible for entrepreneurs to identify multiple market opportunities prior to the first market entry of their emerging firms, and if they elect to do so, to therefore have a choice of which market to enter first. The empirical results presented in this paper offer three new insights regarding this important early-stage choice in new firm creation. First, they reveal that serial entrepreneurs have learned through prior start-up experience to generate a "choice set" of alternative market opportunities before deciding which one to pursue in their new firm creation. Second, the analysis indicates that entrepreneurs who identify a "choice set" of market opportunities prior to first entry derive performance benefits by doing so. Third, the positive relationship between the number of market opportunities identified prior to first entry and new firm performance is nonlinear and subject to decreasing marginal return. The research literature has yet to acknowledge the notion of multiple opportunity identification prior to entry, and the related idea of selecting the most favorable market opportunity for the creation of a new technology firm.</description><author>Gruber, Marc; MacMillan, Ian C.; Thompson, James D.</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Choice interactions and business strategy</title><link>http://www.example.com/articles/1</link><description>Ghemawat, Pankaj; Levinthal, Daniel
Choice settings are strategic to the extent that they entail cross-sectional or intertemporal linkages. These same factors may impose daunting demands on decision makers. We develop a graph-theoretic generalization of the NK model of fitness landscapes to model the way in which policy choices may be more or less strategic. We use this structure to examine, through simulation, how fully articulated a strategy or set of policy choices must be to achieve a high level of performance and how feasible it is to offset past strategic mistakes through tactical adjustments (instead of alignment). Our analysis highlights the role of asymmetry in the interaction of strategic choices and in particular the degree to which choices vary in terms of being influential, dependent, or autonomous from other choices.</description><author>Ghemawat, Pankaj; Levinthal, Daniel</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue ranking of discriminatory and uniform auctions with an unknown number of bidders</title><link>http://www.example.com/articles/1</link><description>Pekec, Aleksandar Sasa; Tsetlin, Ilia
An important managerial question is the choice of the pricing rule. We study whether this choice depends on the uncertainty about the number of participating bidders by comparing expected revenues under discriminatory and uniform pricing within an auction model with affiliated values, stochastic number of bidders, and linear bidding strategies. We show that if uncertainty about the number of bidders is substantial, then the discriminatory pricing generates higher expected revenues than the uniform pricing. In particular, the first-price auction might generate higher revenues than the second-price auction. Therefore, uncertainty about the number of bidders is an important factor to consider when choosing the pricing rule. We also study whether eliminating this uncertainty, i.e., revealing the number of bidders, is in the seller's interests, and discuss the existence of an increasing symmetric equilibrium.</description><author>Pekec, Aleksandar Sasa; Tsetlin, Ilia</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management with limited demand information</title><link>http://www.example.com/articles/1</link><description>Lan, Yingjie; Gao, Huina; Ball, Michael O.; Karaesmen, Itir
In this paper, we consider the classical multifare, single-resource (leg) problem in revenue management for the case where demand information is limited. Our approach employs a competitive analysis, which guarantees a certain performance level under all possible demand scenarios. The only information required about the demand for each fare class is lower and upper bounds. We consider both competitive ratio and absolute regret performance criteria. For both performance criteria, we derive the best possible static policies, which employ booking limits that remain constant throughout the booking horizon. The optimal policies have the form of nested booking limits. Dynamic policies, which employ booking limits that may be adjusted at any time based on the history of bookings, are also obtained. We provide extensive computational experiments and compare our methods to existing ones. The results of the experiments demonstrate the effectiveness of these new robust methods.</description><author>Lan, Yingjie; Gao, Huina; Ball, Michael O.; Karaesmen, Itir</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service adoption and pricing of content delivery network (CDN) services</title><link>http://www.example.com/articles/1</link><description>Hosanagar, Kartik; Chuang, John; Krishnan, Ramayya; Smith, Michael D.
Content delivery networks (CDNs) are a vital component of the Internet's content delivery value chain, servicing nearly a third of the Internet's most popular content sites. However, in spite of their strategic importance, little is known about the optimal pricing policies or adoption drivers of CDNs. We address these questions using analytic models of CDN pricing and adoption under Markovian traffic and extend the results to bursty traffic using numerical simulations. When traffic is Markovian, we find that CDNs should provide volume discounts to content providers. In addition, the optimal pricing policy entails lower emphasis on value-based pricing and greater emphasis on cost-based pricing as the relative density of content providers with high outsourcing costs increases. However, when traffic is bursty and content providers have varying levels of traffic burstiness, volume discounts may be suboptimal and may even be replaced by volume taxes. Finally, when there is heterogeneity in burstiness across content providers, a pricing policy that accounts for both the mean and variance in traffic such as percentile-based pricing is more pro. table than traditional volume-based pricing (metering bytes delivered in a given time window). This finding is in contrast to the current practices of many CDN firms that use traditional volume-based pricing.</description><author>Hosanagar, Kartik; Chuang, John; Krishnan, Ramayya; Smith, Michael D.</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Videoconferencing in the field: A heuristic processing model</title><link>http://www.example.com/articles/1</link><description>Ferran, Carlos; Watts, Stephanie
This research uses dual-process cognitive theory to describe how people process information differently when it is delivered via videoconference rather than when it is delivered face-to-face. According to this theory, relative to face-to-face communication, people in videoconferences tend to be more influenced by heuristic cues-such as how likeable they perceive the speaker to be-than by the quality of the arguments presented by the speaker. This is due to the higher cognitive demands that videoconferencing places on participants. We report on a field study of medical professionals in which we found differences in information processing as predicted: participants attending a seminar via videoconference were more influenced by the likeability of the speaker than by the quality of the arguments presented, whereas the opposite pattern was true for participants attending in-person. We also found that differences in cognitive load explain these effects. The discussion on the theoretical model and associated findings explains why prior videoconference studies have not consistently found main effects for media. The findings also show that videoconferencing is not like face-to-face communication, despite apparent similarities.</description><author>Ferran, Carlos; Watts, Stephanie</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The influence of situational learning orientation, autonomy, and voice on error making: The case of resident physicians</title><link>http://www.example.com/articles/1</link><description>Stern, Zvi; Katz-Navon, Tal; Naveh, Eitan
Every organization is confronted by employee errors. Situational learning orientation may mitigate an employee's tendency to err. A situational learning orientation refers to the extent to which employees share the perception that there is an emphasis on improvement and on actively searching for additional knowledge and feedback in their department. The present study suggests that situational learning orientation interacts with two other organizational factors-autonomy and voice-as it influences employee errors, using the example of resident physicians' medical treatment errors. Senior nurses tallied the number of errors made by 123 residents from 25 departments over a three-month period. Results demonstrated that encouraging employee autonomy and voice fosters the benefits of learning by decreasing the number of treatment errors. Specifically, when the situational learning orientation was high, the number of errors decreased at an increasing rate as the perceived autonomy increased. When the learning orientation was low, there was a u-shaped relationship between autonomy and number of errors. In addition, only when the situational learning orientation was high did a higher employee voice lead to fewer treatment errors.</description><author>Stern, Zvi; Katz-Navon, Tal; Naveh, Eitan</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimizing product line designs: Efficient methods and comparisons</title><link>http://www.example.com/articles/1</link><description>Belloni, Alexandre; Freund, Robert; Selove, Matthew; Simester, Duncan
We take advantage of recent advances in optimization methods and computer hardware to identify globally optimal solutions of product line design problems that are too large for complete enumeration. We then use this guarantee of global optimality to benchmark the performance of more practical heuristic methods. We use two sources of data: (1) a conjoint study previously conducted for a real product line design problem, and (2) simulated problems of various sizes. For both data sources, several of the heuristic methods consistently find optimal or near-optimal solutions, including simulated annealing, divide-and-conquer, product-swapping, and genetic algorithms.</description><author>Belloni, Alexandre; Freund, Robert; Selove, Matthew; Simester, Duncan</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Innovation contests, open innovation, and multiagent problem solving</title><link>http://www.example.com/articles/1</link><description>Terwiesch, Christian; Xu, Yi
In an innovation contest, a firm (the seeker) facing an innovation-related problem (e. g., a technical R&amp;D problem) posts this problem to a population of independent agents (the solvers) and then provides an award to the agent that generated the best solution. In this paper, we analyze the interaction between a seeker and a set of solvers. Prior research in economics suggests that having many solvers work on an innovation problem will lead to a lower equilibrium effort for each solver, which is undesirable from the perspective of the seeker. In contrast, we establish that the seeker can benefit from a larger solver population because he obtains a more diverse set of solutions, which mitigates and sometimes outweighs the effect of the solvers' underinvestment in effort. We demonstrate that the inefficiency of the innovation contest resulting from the solvers' underinvestment can further be reduced by changing the award structure from a fixed-price award to a performance-contingent award. Finally, we compare the quality of the solutions and seeker profits with the case of an internal innovation process. This allows us to predict which types of products and which cost structures will be the most likely to benefit from the contest approach to innovation.</description><author>Terwiesch, Christian; Xu, Yi</author><pubDate>Mon, 01 Sep 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal allocation of risk-reduction resources in event trees</title><link>http://www.example.com/articles/1</link><description>Sherali, Hanif D.; Desai, Jitamitra; Glickman, Theodore S.
In this paper, we present a novel quantitative analysis for the strategic planning decision problem of allocating certain available prevention and protection resources to, respectively, reduce the failure probabilities of system safety measures and the total expected loss from a sequence of events. Using an event tree optimization approach, the resulting risk-reduction scenario problem is modeled and then reformulated as a specially structured nonconvex factorable program. We derive a tight linear programming relaxation along with related theoretical insights that serve to lay the foundation for designing a tailored branch-and-bound algorithm that is proven to converge to a global optimum. Computational experience is reported for a hypothetical case study, as well as for several realistic simulated test cases, based on different parameter settings. The results on the simulated test cases demonstrate that the proposed approach dominates the commercial software BARON v7.5 when the latter is applied to solve the original model by more robustly yielding provable optimal solutions that are at an average of 16.6% better in terms of objective function value; and it performs competitively when both models are used to solve the reformulated problem, particularly for larger test instances.</description><author>Sherali, Hanif D.; Desai, Jitamitra; Glickman, Theodore S.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge sharing ambidexterity in long-term interorganizational relationships</title><link>http://www.example.com/articles/1</link><description>Im, Ghiyoung; Rai, Arun
Although past research has investigated the impact of exploration and exploitation on firm performance, there is limited research on these effects in interorganizational relationships. We examine whether the boundary condition for ambidextrous learning can be extended from firms to long-term interorganizational relationships. Specifically, we focus on a particular aspect of learning - namely, explorative and exploitative knowledge sharing - and examine its impact on the performance of long-term relationships. We also theorize how ambidextrous management of the relationship and ontological commitment to span the syntactic, semantic, and pragmatic knowledge boundaries between partners enable knowledge sharing. Our theoretical predictions are tested using data collected from both account managers at customer firms responsible for the relationship with a leading supply chain vendor and account managers at the vendor firm responsible for relationships with customers. The findings suggest that both exploratory and exploitative knowledge sharing lead to relationship performance gains, that such sharing is enabled by the ambidextrous management of the relationship, and that such sharing is facilitated by ontological commitment. Interesting differences in the enablers and consequences of both forms of knowledge sharing are detected between customers and the vendor.</description><author>Im, Ghiyoung; Rai, Arun</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning, knowledge transfer, and technology implementation performance: A study of time-to-build in the global semiconductor industry</title><link>http://www.example.com/articles/1</link><description>Salomon, Robert; Martin, Xavier
Organizational growth and performance hinge upon the effective deployment of productive knowledge in new facilities. However, getting those facilities fully operational can be difficult and time consuming. Interestingly, we understand little about what determines the performance of that process. In this paper we help fill this gap by analyzing multiple determinants of time-to-build - i.e., the time it takes a firm to build and ramp up operations at a new manufacturing facility. Theoretically, we develop predictions regarding the effects of competitive, firm, and technology characteristics on time-to-build. Empirically, we test our predictions on a sample of plant construction projects in the memory segment of the global semiconductor industry. We find that competition from rivals with superior technology is associated with shorter time-to-build, at least up to a point. Firm and industry experience are associated with shorter time-to-build. International projects, and those that push the technological frontier, take longer. Findings from this study enrich the literatures on corporate growth, international expansion, and technology strategy. We discuss implications for research and practice.</description><author>Salomon, Robert; Martin, Xavier</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does it matter where countries are? Proximity to knowledge, markets and resources, and MNE location choices</title><link>http://www.example.com/articles/1</link><description>Nachum, Lilach; Zaheer, Srilata; Gross, Shulamith
W e suggest that the proximity of a country to other countries is a factor that affects its choice as a multinational enterprise (MNE) location. We introduce the concept of a country's proximity to the global distribution of knowledge, markets, and resources, and frame this concept as a function of both geographic distance and the worldwide spatial distribution of these factors. We test our location model on a data set comprising 138,050 investments undertaken by U. S. MNEs worldwide. Our findings show that the proximity of a country to the rest of the world has a positive impact on MNEs choosing that country as a location. Proximity to the world's knowledge and markets are stronger drivers of location choice than is proximity to the world's resources, after accounting for the country's own endowments. Larger firms are able to benefit more from remote locations than smaller firms are.</description><author>Nachum, Lilach; Zaheer, Srilata; Gross, Shulamith</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Red Queen, success bias, and organizational inertia</title><link>http://www.example.com/articles/1</link><description>Barnett, William P.; Pontikes, Elizabeth G.
Why do successful organizations often move in new directions and then fail? We propose that this pattern is especially likely among organizations that have survived a history of competition. Such experience adapts organizations to their environment, through so-called "Red Queen" evolution, but being well adapted for one context makes moving into new contexts more hazardous. Meanwhile, managers in such organizations infer from their histories of competitive success a biased assessment of their organization's ability to change. Consequently, although surviving competition makes organizational change especially hazardous, managers in surviving organizations are especially inclined to such initiatives. We develop these ideas in an empirically testable model, and find supportive evidence in estimates of the model using data from the history of the U. S. computer industry.</description><author>Barnett, William P.; Pontikes, Elizabeth G.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning and knowledge depreciation in professional services</title><link>http://www.example.com/articles/1</link><description>Boone, Tonya; Ganeshan, Ram; Hicks, Robert L.
Organizational knowledge is a critical source of competitive advantage for professional service firms. Learning from experience and sustaining past knowledge are critical to the success of such knowledge-driven firms. We use learning curve theory to evaluate learning and depreciation in professional services. Our results, based on seven years of project data collected from an architectural engineering (A/E) firm, show that ( a) professional services exhibit learning curves, (b) there is virtually no depreciation of knowledge and, ( c) the rate of learning accelerates with experience.</description><author>Boone, Tonya; Ganeshan, Ram; Hicks, Robert L.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Can they take it with them? The portability of star knowledge workers' performance</title><link>http://www.example.com/articles/1</link><description>Groysberg, Boris; Lee, Linda-Eling; Nanda, Ashish
This paper examines the portability of star security analysts' performance. Star analysts who switched employers experienced an immediate decline in performance that persisted for at least five years. This decline was most pronounced among star analysts who moved to firms with lesser capabilities and those who moved solo, without other team members. Star analysts who moved between two firms with equivalent capabilities also exhibited a drop in performance, but only for two years. Those who switched to firms with better capabilities and those who moved with other team members exhibited no significant decline in short-term or long-term performance. These findings suggest that firm-specific skills and firms' capabilities both play important roles in star analysts' performance. In addition, we find that firms that hire star analysts from competitors with better capabilities suffered more extreme negative stock-market reactions than those that hire from comparable or lesser firms. These findings suggest that hiring stars may be perceived as value destroying and may not improve a firm's competitive advantage.</description><author>Groysberg, Boris; Lee, Linda-Eling; Nanda, Ashish</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effect of delays on complexity of organizational learning</title><link>http://www.example.com/articles/1</link><description>Rahmandad, Hazhir
We examine how delays between actions and their consequent payoffs affect the process of organizational adaptation. Formal conceptions of organizational learning typically include the assumption that payoffs immediately follow their antecedent actions, making the search for better strategies relatively straightforward. However, previous actions influence current organizational performance through their effects on organizational resources and capabilities. These resources and capabilities cannot be modified instantly, so delays - from actions to changes in resources and capabilities to altered organizational performance - are inevitable. Our computational experiments show that delays increase learning complexity and performance heterogeneity through two mechanisms. First, complexity of state-space and, therefore, of learning grows exponentially with delay length. Second, the time required to experience the benefits of long-term strategies means the intermediate steps of those strategies are initially undervalued, prompting premature abandonment of potentially fruitful regions of the strategy space. We find that these mechanisms often cause organizations to converge to suboptimal, routine-like cycles of actions, based on organizations' continually updated cognitive maps of how actions influence payoffs. Furthermore, the evolution of these cognitive maps exhibits path dependence, leading to heterogeneity across organizations. Implications for overcoming temporal complexity and the impact of initial cognitive maps are discussed.</description><author>Rahmandad, Hazhir</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An empirical test of gain-loss separability in prospect theory</title><link>http://www.example.com/articles/1</link><description>Wu, George; Markle, Alex B.
We investigate a basic premise of prospect theory: that the valuation of gains and losses is separable. In prospect theory, gain-loss separability implies that a mixed gamble is valued by summing the valuations of the gain and loss portions of that gamble. Two experimental studies demonstrate a systematic violation of the double-matching axiom, an axiom that is necessary for gain-loss separability. We document a reversal between preferences for mixed gambles and the associated gain and loss gambles-mixed gamble A is preferred to mixed gamble B, but the gain and loss portions of B are preferred to the gain and loss portions of A. The observed choice patterns are consistent with a process in which individuals are less sensitive to probability differences when choosing among mixed gambles than when choosing among either gain or loss gambles.</description><author>Wu, George; Markle, Alex B.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multiple criteria decision making, multiattribute utility theory: Recent accomplishments and what lies ahead</title><link>http://www.example.com/articles/1</link><description>Wallenius, Jyrki; Dyer, James S.; Fishburn, Peter C.; Steuer, Ralph E.; Zionts, Stanley; Deb, Kalyanmoy
This paper is an update of a paper that five of us published in 1992. The areas of multiple criteria decision making (MCDM) and multiattribute utility theory (MAUT) continue to be active areas of management science research and application. This paper extends the history of these areas and discusses topics we believe to be important for the future of these fields.</description><author>Wallenius, Jyrki; Dyer, James S.; Fishburn, Peter C.; Steuer, Ralph E.; Zionts, Stanley; Deb, Kalyanmoy</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The distribution of the sample minimum-variance frontier</title><link>http://www.example.com/articles/1</link><description>Kan, Raymond; Smith, Daniel R.
In this paper, we present a finite sample analysis of the sample minimum-variance frontier under the assumption that the returns are independent and multivariate normally distributed. We show that the sample minimum-variance frontier is a highly biased estimator of the population frontier, and we propose an improved estimator of the population frontier. In addition, we provide the exact distribution of the out-of-sample mean and variance of sample minimum-variance portfolios. This allows us to understand the impact of estimation error on the performance of in-sample optimal portfolios.</description><author>Kan, Raymond; Smith, Daniel R.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interactive coordination of objective decompositions in multiobjective programming</title><link>http://www.example.com/articles/1</link><description>Engau, Alexander; Wiecek, Margaret M.
To remedy challenges resulting from a high number of objectives in multiobjective programming and multicriteria decision making, this paper chooses to decompose the vector objective function and characterizes the relationships between solutions for the original problem and the collection of decomposed subproblems. In particular, it is shown how solutions that are found using this decomposition approach relate to solutions found by traditional scalarization techniques. For the selection of a final solution, two interactive coordination methods are proposed that allow to find any solution for the original problem by merely solving the smaller-sized subproblems, while integrating both preferences of the decision maker and trade-off information obtained from a sensitivity analysis. A theoretical foundation for the procedures is established, and their application is illustrated for portfolio optimization and a design selection problem.</description><author>Engau, Alexander; Wiecek, Margaret M.</author><pubDate>Tue, 01 Jul 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Confidentiality and information sharing in supply chain coordination</title><link>http://www.example.com/articles/1</link><description>Li, Lode; Zhang, Hongtao
We consider information sharing in a decentralized supply chain where one manufacturer supplies to multiple retailers competing in price. Each retailer has some private information about the uncertain demand function which he may choose to disclose to the manufacturer. The manufacturer then sets a wholesale price based on the information received. The information exchange is said to be confidential if the manufacturer keeps the received information to herself, or nonconfidential if she discloses the information to some or all other retailers. Without confidentiality, information sharing is not possible because it benefits the manufacturer but hurts the retailers. With confidentiality, all parties have incentive to engage in information sharing if retail competition is intense. Under confidentiality, the retailers infer the shared information from the wholesale price and this gives rise to a signaling effect that makes the manufacturer's demand more price elastic, resulting in a lower equilibrium wholesale price and a higher supply chain profit. When all retailers share their information confidentially, they will truthfully report the information and the supply chain profit will achieve its maximum in equilibrium.</description><author>Li, Lode; Zhang, Hongtao</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing the inventory of an item with a replacement warranty</title><link>http://www.example.com/articles/1</link><description>Huang, Wei; Kulkarni, Vidyadhar; Swaminathan, Jayashankar M.
In this paper, we study a firm that faces demand from two sources: demand for new items and demand to replace failed items under warranty. We model this setting as a multiperiod single-product inventory problem where the demands for new items in different periods are independent and the demands for replacing failed items depend on the number and ages of the items under warranty. We consider backlogging and emergency supply cases, and study both discounted cost and average cost criteria. We prove the optimality of the w-dependent base-stock ordering policy where the base-stock level is a function of w, the vector representing the number of items at different ages currently under warranty. For the special case where the demand for new products is identically distributed, we prove the optimality of a stationary w-dependent base stock policy for the finite-horizon discounted and the infinite-horizon discounted and average cost cases. In our computational study, we find that an optimal w-dependent policy can lead to 69% average improvement in expected costs when compared to a policy that neglects demands from items under warranty.</description><author>Huang, Wei; Kulkarni, Vidyadhar; Swaminathan, Jayashankar M.</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How near-misses influence decision making under risk: A missed opportunity for learning</title><link>http://www.example.com/articles/1</link><description>Dillon, Robin L.; Tinsley, Catherine H.
Although organizations appear to learn from obvious failures, we argue that it is harder for them to learn from "near-misses"-events in which chance played a role in averting failure. In this paper, we formalize the concept of near-misses and hypothesize that organizations and managers fail to learn from near-misses because they evaluate such events as successes and thus feel safer about the situation. We distinguish perceived ("felt") risk from calculated statistical risk and propose that lower levels of perceived risk encourage people with near-miss information to make riskier subsequent decisions compared to people without near-miss information. In our first study, we confirm the tendency to evaluate near-misses as successes by having participants rate a project manager whose decisions result in either (a) mission success, (b) near-miss, or (c) failure. Participants (both students and NASA employees and contractors) give similar ratings to managers whose decisions produced near-misses and to managers whose decisions resulted in successes, and both ratings are significantly different from ratings of managers who experienced failures. We suggest that the failure to hold managers accountable for near-misses is a foregone learning opportunity for both the manager and the organization. In our second set of studies, we confirm that near-miss information leads people to choose a riskier alternative because of a lower perceived risk following near-miss events. We explore several alternative explanations for these findings, including the role of Bayesian updating in processing near-miss data. Ultimately, the analysis suggests that managers and organizations are reducing their perception of the risk, although not necessarily updating (lowering) the statistical probability of the failure event. We speculate that this divergence arises because perceived risk is the product of associative processing, whereas statistical risk arises from rule-based processing.</description><author>Dillon, Robin L.; Tinsley, Catherine H.</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic allocation of airline check-in counters: A queueing optimization approach</title><link>http://www.example.com/articles/1</link><description>Parlar, Mahmut; Sharafali, Moosa
This paper was motivated by an observation in an international airport with regard to allocation of resources for check-in counters. In an exclusive check-in counter system, each flight has a dedicated number of counters that will be open until at least a half-hour before the scheduled departure of that flight. Currently, in many of the airports around the world, the decision to open or close check-in counters is done on an ad hoc basis by human schedulers. In doing so, the schedulers are almost always forced to perform a balancing act in meeting the quality of service stipulated by the airport authority vis-a-vis the optimal allocation of the resources to the counters. There appear to be very few academic and application papers in counter management, and most of those that have looked into this problem have resorted to simulation to study the queue characteristics. Ours is the first paper to show that for a specific flight, this complicated problem is amenable to analytical treatment. We first propose a multicounter queueing model with a special type of arrival process reflecting reality from the population of passengers booked for the flight. Most importantly, we derive the time-dependent operating characteristics to the queueing process under a specified time-window constraint. Then a stochastic dynamic programming model is formulated to determine the optimal numbers of counters to open over the time window specified. A numerical example is provided to illustrate the model solution and gain managerial insights.</description><author>Parlar, Mahmut; Sharafali, Moosa</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An integrated decision-making approach for improving European air traffic management</title><link>http://www.example.com/articles/1</link><description>Grushka-Cockayne, Yael; De Reyck, Bert; Degraeve, Zeger
We develop a multistakeholder, multicriteria decision-making framework for Eurocontrol, the European air traffic management organization, for evaluating and selecting operational improvements to the air traffic management system. The selected set of improvements will form the master plan of the Single European Sky initiative for harmonizing air traffic, in an effort to cope with the forecasted increase in air traffic, while maintaining safety, protecting the environment, and improving predictability and efficiency. The challenge is to select the set of enhancements such that the required performance targets are met and all key stakeholders are committed to the decisions. In this paper, we develop and implement a model to identify a preferred set of improvements to the arrival and departure procedures to and from airports. We provide an integrated approach for valuing a large number of alternatives, while considering interactions among them. The model combines quantitative and qualitative expert assessments of the possible enhancements and identifies commonalities and differences in the stakeholders' perspectives, ultimately recommending a preferred course of action. The model is currently being adopted by Eurocontrol as the formal trade-off analysis methodology supporting all enhancements' decision-making discussions throughout the construction of the master plan.</description><author>Grushka-Cockayne, Yael; De Reyck, Bert; Degraeve, Zeger</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling a presidential prediction market</title><link>http://www.example.com/articles/1</link><description>Chen, M. Keith; Ingersoll, Jonathan E.; Kaplan, Edward H.
Prediction markets now cover many important political events. The 2004 presidential election featured an active online prediction market at Intrade.com, where securities addressing many different election-related outcomes were traded. Using the 2004 data from this market, we examined three alternative models for these security prices, with special focus on the electoral college rules that govern U. S. presidential elections to see which models are more (or less) consistent with the data. The data reveal dependencies in the evolution of the security prices across states over time. We show that a simple diffusion model provides a good description of the overall probability distribution of electoral college votes, and an even simpler ranking model provides excellent predictions of the probability of winning the presidency. Ignoring dependencies in the evolution of security prices across states leads to considerable underestimation of the variance of the number of electoral college votes received by a candidate, which in turn leads to overconfidence in predicting whether that candidate will win the election. Overall, the security prices in the Intrade presidential election prediction market appear jointly consistent with probability models that satisfy the rules of the electoral college.</description><author>Chen, M. Keith; Ingersoll, Jonathan E.; Kaplan, Edward H.</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory models for substitutable products: Optimal policies and heuristics</title><link>http://www.example.com/articles/1</link><description>Nagarajan, Mahesh; Rajagopalan, S.
In this paper, we examine the nature of optimal inventory policies in a system where a retailer manages substitutable products. We first consider a system with two products 1 and 2 whose total demand is D and individual demands are negatively correlated. A fixed proportion of the unsatisfied customers for an item will purchase the other item if it is available in inventory. For the single-period case, we show that the optimal inventory levels of the two items can be computed easily and follow what we refer to as "partially decoupled" policies, i.e., base stock policies that are not state dependent, in certain critical regions of interest both when D is known and random. Furthermore, we show that such a partially decoupled base-stock policy is optimal even in a multiperiod version of the problem for known D for a wide range of parameter values and in an N-product single-period model under some restrictive conditions. Using a numerical study, we show that heuristics based on the decoupled inventory policy perform well in conditions more general than the ones assumed to obtain the analytical results. The analytical and numerical results suggest that the approach presented here is most valuable in retail settings for product categories where the level of substitution between items in a category is not high, demand variation at the aggregate level is not high, and service levels or newsvendor ratios are high.</description><author>Nagarajan, Mahesh; Rajagopalan, S.</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A bargaining framework in supply chains: The assembly problem</title><link>http://www.example.com/articles/1</link><description>Nagarajan, Mahesh; Bassok, Yehuda
We examine a decentralized supply chain in which a single assembler buys complementary components from n suppliers and assembles the final product in anticipation of demand. Players take actions in the following sequence. First (stage 1), the suppliers form coalitions among themselves. Second (stage 2), the coalitions compete for a position in the negotiation sequence. Finally (stage 3), the coalitions negotiate with the assembler on allocations of the supply chain's profit. We model the multilateral negotiations between the suppliers and the assembler sequentially, i.e., the assembler negotiates with one coalition at a time. Each of these negotiations is modeled using the Nash bargaining concept. Further, in forming coalitions we assume that players are farsighted. We then predict at equilibrium the structure of the supply chain as a function of the players' relative negotiation powers. In particular, we show that the assembler always prefers the outcome where suppliers do not form coalitions. However, when the assembler is weak (low negotiation power) the suppliers join forces as a grand coalition, but when the assembler is powerful the suppliers stay independent, which is the preferred outcome to the assembler.</description><author>Nagarajan, Mahesh; Bassok, Yehuda</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Queuing for expert services</title><link>http://www.example.com/articles/1</link><description>Debo, Laurens G.; Toktay, L. Beril; Van Wassenhove, Luk N.
We consider a monopolist expert offering a service with a "credence" characteristic. A credence service is one in which the customer cannot verify, even after a purchase, whether or not the amount of prescribed service was appropriate; examples include legal, medical, or consultancy services, and car repair. This creates an incentive for the expert to "induce service," that is, to provide unnecessary services that add no value to the customer, but that allow the expert to increase his revenues. We focus on the impact of an operations phenomenon on service inducement-workload dynamics due to the stochasticity of interarrival and service times. To this end, we model the expert's service operation as a single-server queue. The expert determines the service price within a fixed and variable fee structure and determines the service inducement strategy. We characterize the expert's combined optimal price structure and service inducement strategy as a function of service capacity, market potential, inducement opportunity, value of service and waiting cost. We find that service inducement is a means to dynamically skim customer surplus with state-independent prices and provision of slower service to customers that arrive when the expert is idle. We conclude with design implications of our results in limiting service inducement.</description><author>Debo, Laurens G.; Toktay, L. Beril; Van Wassenhove, Luk N.</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fluid models for overloaded multiclass many-server queueing systems with first-come, first-served routing</title><link>http://www.example.com/articles/1</link><description>Talreja, Rishi; Whitt, Ward
Motivated by models of tenant assignment in public housing, we study approximating deterministic fluid models for overloaded queueing systems having multiple customer classes (classes of tenants) and multiple service pools (housing authorities), each with many servers (housing units). Customer abandonment acts to keep the system stable, yielding a proper steady-state description. Motivated by fairness considerations, we assume that customers are selected for service by newly available servers on a first-come,first-served (FCFS) basis from all classes the corresponding service pools are allowed to serve. In this context, it is challenging to determine stationary routing flow rates between customer classes and service pools. Given those routing flow rates, each single fluid queue can be analyzed separately using previously established methods. Our ability to determine the routing flow rates depends on the structure of the network routing graph. We obtain the desired routing flow rates in three cases: when the routing graph is (i) a tree (sparsely connected), (ii) complete bipartite (fully connected), and (iii) an appropriate combination of the previous two cases. Other cases remain unsolved. In the last two solved cases, the routing flow rates are actually not uniquely determined by the fluid model, but become so once we make stochastic assumptions about the queueing models that the fluid model approximates.</description><author>Talreja, Rishi; Whitt, Ward</author><pubDate>Fri, 01 Aug 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price Promotions in Asymmetric Duopolies with Heterogeneous Consumers</title><link>http://www.example.com/articles/1</link><description>Sinitsyn, Maxim
In this note I investigate the outcome of a static price competition between a strong firm ( a firm with an established loyal consumer base) and a weak firm ( a firm without loyal consumers). The consumers are divided into the strong firm's loyal segment and the switching segment, members of which have heterogeneous tastes for the firms' products. In the presence of loyal consumers, for a large set of the parameters of the demand function, the firms use mixed strategies over a finite number of prices. These strategies can be interpreted as occurrences of sales. The most common case is that of the strong firm's using two prices and the weak firm's using one price. However, when both firms use sales, the weak firm promotes more often than the strong firm.</description><author>Sinitsyn, Maxim</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Affine General Equilibrium Models</title><link>http://www.example.com/articles/1</link><description>Eraker, Bjorn
No-arbitrage models are extremely flexible modelling tools but often lack economic motivation. This paper describes an equilibrium consumption-based CAPM framework based on Epstein-Zin preferences, which produces analytic pricing formulas for stocks and bonds under the assumption that macro growth rates follow affine processes. This allows the construction of equilibrium pricing formulas while maintaining the same flexibility of state dynamics as in no-arbitrage models. In demonstrating the approach, the paper presents a model that incorporates inflation such that asset prices are nominal. The model takes advantage of the possibility of non-Gaussian shocks and model macroeconomic uncertainty as a jump-diffusion process. This leads to endogenous stock market crashes as stock prices drop to reflect a higher expected rate of return in response to sudden increases in risk. The nominal yield curve in this model has a positive slope if expected inflation growth negatively impacts real growth. This model also produces asset prices that are consistent with observed data, including a substantial equity premium at moderate levels of risk aversion.</description><author>Eraker, Bjorn</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>R&amp;D/Returns Causality: Absorptive Capacity or Organizational IQ</title><link>http://www.example.com/articles/1</link><description>Knott, Anne Marie
Absorptive capacity is the principle that assimilating new knowledge requires prior knowledge. The attendant prescription is to invest more in R&amp;D to derive greater benefit from the R&amp;D of others (spillovers). Empirical tests of R&amp;D productivity typically find absorptive capacity (R&amp;D* rival R&amp;D) to be significant. This result poses a puzzle, however: What can a firm conducting 50% of industry R&amp;D learn from a set of firms each conducting 5%? Aren't the laggard firms merely playing catch-up? Yet, if this is so, why is the interaction term significant? One possible resolution to this puzzle is that the correlation between R&amp;D spending and returns is really about innate ability (IQ) rather than investment behavior (absorptive capacity). In this view the causality between capability and behavior is reversed. It is not that firms obtain higher returns by investing more in R &amp; D; it is that some firms have higher returns to R&amp;D, thus they invest more. I conduct an empirical test of the competing views and find that (1) firms differ in the output elasticities of their own R&amp;D (IQ) as well as the elasticities of spillovers from rivals, (2) absorptive capacity becomes insignificant when accounting for that heterogeneity, (3) R&amp;D investment increases with IQ, but (4) R&amp;D investment has no impact on firm's ability to benefit from spillovers.</description><author>Knott, Anne Marie</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product-Line Competition: Customization vs. Proliferation</title><link>http://www.example.com/articles/1</link><description>Mendelson, Haim; Parlaktuerk, Ali K.
We study a market with customers who have heterogeneous preferences for product attributes. We consider two types of firms that compete on price and product variety: A traditional firm, which chooses a limited set of product configurations, and a customizing firm, which can produce any configuration to order. The traditional firm carries product inventories and experiences a lead-time delay. The customizing firm does not carry inventory, and its customers incur waiting costs until they receive their orders. We assume that the customizing firm has limited capacity in the short run ( e. g., when it does not outsource production to high-volume manufacturers). We derive the equilibrium for a duopoly competition between the customizing firm and the traditional firm, study its characteristics, and compare it to a monopoly. We characterize conditions that favor customization under competition. We find that the customizing firm's profit is not monotone in the market size and its ease of customization. Similarly, a decline in the traditional firm's holding cost may increase or decrease its profit. We show that the unit cost differential between the firms crucially affects the customizing firm's ideal market size, its returns from expanding capacity, its product variety, and the way operational improvements affect its performance.</description><author>Mendelson, Haim; Parlaktuerk, Ali K.</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Identifying Formal and Informal Influence in Technology Adoption with Network Externalities</title><link>http://www.example.com/articles/1</link><description>Tucker, Catherine E.
Firms introducing network technologies ( whose benefits depend on who installs the technology) need to understand which user characteristics confer the greatest network benefits on other potential adopters. To examine which adopter characteristics matter, I use the introduction of a video-messaging technology in an investment bank. I use data on its 2,118 employees, their adoption decisions, and their 2.4 million subsequent calls. The video-messaging technology can also be used to watch TV. Exogenous shocks to the benefits of watching TV are used to identify the causal ( network) externality of one individual user's adoption on others' adoption decisions. I allow this network externality to vary in size with a variety of measures of informal and formal influence. I find that adoption by either managers or workers in "boundary spanner" positions has a large impact on the adoption decisions of employees who wish to communicate with them. Adoption by ordinary workers has a negligible impact. This suggests that firms should target those who derive their informal influence from occupying key boundary-spanning positions in communication networks, in addition to those with sources of formal influence, when launching a new network technology.</description><author>Tucker, Catherine E.</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal Patenting and Licensing of Financial Innovations</title><link>http://www.example.com/articles/1</link><description>Kumar, Praveen; Turnbull, Stuart M.
Recent court decisions, starting with the State Street decision in 1998, allow business methods to be patentable and now give financial institutions the option to seek patent protection for financial innovations. This new patentability paradigm and the heterogeneity of characteristics associated with financial innovations pose an immediate decision problem for senior management: what to patent. We present a parsimonious decision framework that answers this question. We show that for innovations with certain characteristics, it is optimal not to patent, even if the option of patenting and licensing is available. Our model emphasizes the role of embedded real options that arise from certain types of financial innovations. The model provides an explanation of observed patenting behavior of financial institutions and the success of a wide class of innovations, including swaps, credit derivatives, and pricing algorithms.</description><author>Kumar, Praveen; Turnbull, Stuart M.</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Component Procurement Strategies in Decentralized Assemble-to-Order Systems with Time-Dependent Pricing</title><link>http://www.example.com/articles/1</link><description>Fang, Xiang; So, Kut C.; Wang, Yunzeng
We consider a contract manufacturer who procures multiple components from independent suppliers to produce an assemble-to-order customized product for a client. The unit price of the product depends on the manufacturer's delivery lead time. We explore how the manufacturer can use a vendor-managed consignment inventory (VMCI) scheme to manage the underlying risk and coordinate independent suppliers' decisions on the production quantities of their components under demand uncertainty. We formulate the problem as a Stackelberg game played by the manufacturer against her component suppliers to determine her pricing policy for suppliers' consignment inventories. We further develop an efficient algorithm for finding the manufacturer's optimal pricing scheme. Our results provide useful insights for managing components in these types of assemble-to-order environments and for understanding how component production cost and procurement lead times affect individual firms' performance in decentralized assembly channels.</description><author>Fang, Xiang; So, Kut C.; Wang, Yunzeng</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic Interaction Across Countries and Multinational Agglomeration: An Application to the Cement Industry</title><link>http://www.example.com/articles/1</link><description>Ghemawat, Pankaj; Thomas, Catherine
Agglomeration in foreign direct investment (FDI) is typically attributed to location-specific characteristics such as natural resource advantages or production-related spillovers between multinational firms. The increasing collocation of the largest global firms in the cement industry since the 1980s is not easily attributed to either of these explanations. This paper draws on theories of multimarket contact to test whether strategic interaction across national markets has influenced the successive market entry decisions generating the observed agglomeration. We first establish that there is indeed nonrandom agglomeration of the six largest cement firms. We next show that preexisting cross-market interaction with current incumbents helps predict which firm will enter a given market and also the choice of market a given firm enters. The association does not appear to be caused by strategic convergence or mimicry of recent entry events and cannot be explained by production side effects, which depend only on local conditions. The findings are consistent with multimarket contact models where collocation allows firms to sustain higher prices in all markets. This latter inference is also supported by evidence of an association between global firm market share and local cement price. The paper suggests that pricing spillovers can serve as an alternative motivation for FDI agglomeration.</description><author>Ghemawat, Pankaj; Thomas, Catherine</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How Does Outsourcing Affect Performance Dynamics? Evidence from the Automobile Industry</title><link>http://www.example.com/articles/1</link><description>Novak, Sharon; Stern, Scott
This paper examines the impact of vertical integration on the dynamics of performance over the automobile product development life cycle. Building on recent work in organizational economics and strategy, we evaluate the relationship between vertical integration and different dimensions of product performance. Outsourcing facilitates access to cutting-edge technology and the use of high-powered performance contracts. Vertical integration allows firms to adapt to unforeseen contingencies and customer feedback, maintain balanced incentives over the life cycle, and develop firm-specific capabilities over time. Together, these effects highlight a crucial trade-off: while outsourcing is associated with higher levels of initial performance, vertical integration will be associated with performance improvement over the product life cycle. We test these ideas using detailed data from the luxury automobile segment, establishing three key results. First, initial performance is declining in the level of vertical integration. Second, the level of performance improvement is significantly increasing in the level of vertical integration. Finally, the impact of vertical integration is mediated by the level of preexisting capabilities, by the salience of opportunities to access external technology leaders, and by the scope for learning over the product life cycle. Together, the findings highlight a strategic governance trade-off between short-term performance and the evolution of firm capabilities.</description><author>Novak, Sharon; Stern, Scott</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Management Science and the Science of Management</title><link>http://www.example.com/articles/1</link><description>Hopp, Wallace J.
For over half a century, Management Science has promoted scientific research into the practice of management. Because management is a vast and complex activity, early researchers tended to adopt a reductionist approach by concentrating on narrow subproblems. As a result, the journal was initially dominated by studies of tactical issues and quantitative solution techniques. But recent publication trends suggest that scholars are turning their attention to broader, more realistic management issues, and are using a wider range of research methods to address them. This is fueling a research renaissance that offers hope for significant progress toward creation of a legitimate science of management.</description><author>Hopp, Wallace J.</author><pubDate>Mon, 01 Dec 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Are Store-Brand Buyers Store Loyal? An Empirical Investigation</title><link>http://www.example.com/articles/1</link><description>Hansen, Karsten; Singh, Vishal
This paper tests the notion of whether high preference for store brands leads to higher store loyalty. Our work differs from the earlier empirical evidence on this issue in how we de. ne and measure the two key constructs: store-brand loyalty and store loyalty. To measure store-brand preference, we develop a multicategory brand choice to elicit the basic latent tendency for a household to buy store brands, while controlling for other factors such as price sensitivity. The household-specific measures on store-brand preference and price sensitivity (along with other control variables) are then used to assess household store loyalty in the context of entry by a Wal-Mart supercenter in the market. Results show that store-brand buying behavior is driven by households' underlying "value trait" over and beyond what is captured by price sensitivity alone. In addition, these value-oriented households, which show high preference for store brands in the pre-entry period, shift a significantly higher level of their expenditures to Wal-Mart. These results are in contradiction to the conventional wisdom that high store-brand patronage is associated with a higher level of store loyalty. However, this finding comes with the caveat that the entrant analyzed in the paper has a well-defined "value" positioning and might not hold if the new entrant does not dominate the incumbent in the value proposition. Implications for supermarket managers in a more general context are discussed.</description><author>Hansen, Karsten; Singh, Vishal</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Properties of the Social Discount Rate in a Benthamite Framework with Heterogeneous Degrees of Impatience (vol 54, pg 1822, 2008)</title><link>http://www.example.com/articles/1</link><description>Nocetti, Diego; Jouini, Elyes; Napp, Clotilde
This paper derives the properties of the discount rate that should be applied to a public-sector project when the affected population has heterogeneous degrees of impatience. We show that, for any distribution of discount rates, the social discount rate has the following properties: it decreases over time, it is lower than the average of the discount rates in the population, and it converges to the discount rate of the most patient individual in the economy. These properties hold for both constant and decreasing individual discount rates. Finally, we evaluate how changes in the distribution of individual discount rates affect the social discount rate.</description><author>Nocetti, Diego; Jouini, Elyes; Napp, Clotilde</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory Management Under Market Size Dynamics</title><link>http://www.example.com/articles/1</link><description>Olsen, Tava Lennon; Parker, Rodney P.
We investigate the situation where a customer experiencing an inventory stockout at a retailer potentially leaves the firm's market. In classical inventory theory, a unit stockout penalty cost has been used as a surrogate to mimic the economic effect of such a departure; in this study, we explicitly represent this aspect of consumer behavior, incorporating the diminishing effect of the consumers leaving the market upon the stochastic demand distribution in a time-dynamic context. The initial model considers a single firm. We allow for consumer forgiveness where customers may flow back to the committed purchasing market from a nonpurchasing "latent" market. The per-period decisions include a marketing mix to attract latent and new consumers to the committed market and the setting of inventory levels. We establish conditions under which the firm optimally operates a base-stock inventory policy. The subsequent two models consider a duopoly where the potential market for a firm is now the committed market of the other firm; each firm decides its own inventory level. In the first model, the only decisions are the stocking decisions and in the second model, a firm may also advertise to attract dissatisfied customers from its competitor's market. In both cases, we establish conditions for a base-stock equilibrium policy. We demonstrate comparative statics in all models.</description><author>Olsen, Tava Lennon; Parker, Rodney P.</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic Inventories in Vertical Contracts</title><link>http://www.example.com/articles/1</link><description>Anand, Krishnan; Anupindi, Ravi; Bassok, Yehuda
Classical reasons for carrying inventory include fixed (nonlinear) production or procurement costs, lead times, nonstationary or uncertain supply/demand, and capacity constraints. The last decade has seen active research in supply chain coordination focusing on the role of incentive contracts to achieve first-best levels of inventory. An extensive literature in industrial organization that studies incentives for vertical controls largely ignores the effect of inventories. Does the ability to carry inventory influence the problem of vertical control? Conversely, can inventories arise purely due to incentive effects? This paper explicitly considers both incentives and inventories, and their interplay, in a dynamic model of an upstream firm (supplier) and a downstream firm (buyer) who can carry inventories. In our model, none of the classical reasons for carrying inventory exists. However, as we prove, the buyer's optimal strategy in equilibrium is to carry inventories, and the supplier is unable to prevent this. These inventories arise out of purely strategic considerations not yet identified in the literature, and have a significant impact on the equilibrium solution as well as supplier, buyer, and channel profits. We prove that strategic inventories play a pivotal role under arbitrary contractual structures, general (arbitrary) demand functions and general (finite or infinite) horizon lengths. As one example, two-part tariff contracts do not lead to optimal channel performance, nor can the supplier extract away all of the channel profits, in our dynamic model. Our results imply that firms can and must carry inventories strategically, and that optimal vertical contracts must take the possibility of inventories into account.</description><author>Anand, Krishnan; Anupindi, Ravi; Bassok, Yehuda</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Estimating the Influence of Fairness on Bargaining Behavior</title><link>http://www.example.com/articles/1</link><description>De Bruyn, Arnaud; Bolton, Gary E.
The strength of bargainers' preferences for fair settlements has important implications for predicting negotiation outcomes and guiding bargaining strategy. Existing literature reports a few calibration exercises for social utility models, but the predictive accuracy of these models for out-of-sample forecasting remains unknown. Therefore, we investigate whether fairness considerations are stable enough across bargaining situations to be quantified and used to forecast bargaining behavior accurately. We develop a model that embeds a preference for fair treatment in a quantal response framework to account for noise and experience. In addition, we estimate preference for fairness (willingness to pay) using the simplest, one-round version of sequential bargaining games and then employ it to perform out-of-sample forecasts of multiple-round games of various lengths, discount factors, pie sizes, and levels of bargainer experience. Except in circumstances in which the bargaining pie is very small, the fitted model has significant and substantial out-of-sample explanatory power. The stability we find implies that the model and techniques might ultimately be extended to estimates of the influence of fairness on field negotiations, as well as across subpopulations.</description><author>De Bruyn, Arnaud; Bolton, Gary E.</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic Customer Behavior, Commitment, and Supply Chain Performance</title><link>http://www.example.com/articles/1</link><description>Su, Xuanming; Zhang, Fuqiang
This paper studies the impact of strategic customer behavior on supply chain performance. We start with a newsvendor seller facing forward-looking customers. The seller initially charges a regular price but may salvage the leftover inventory at a lower salvage price after random demand is realized. Customers anticipate future sales and choose purchase timing to maximize their expected surplus. We characterize the rational expectations equilibrium, where we find that the seller's stocking level is lower than that in the classic model without strategic customers. We show that the seller's profit can be improved by promising either that quantities available will be limited (quantity commitment) or that prices will be kept high (price commitment). In most cases, both forms of commitment are not credible in a centralized supply chain with a single seller. However, decentralized supply chains can use contractual arrangements as indirect commitment devices to attain the desired outcomes with commitment. Decentralization has generally been associated with coordination problems, but we present the contrasting view that disparate interests within a supply chain can actually improve overall supply chain performance. In particular, with strategic customer behavior, we find that (i) a decentralized supply chain with a wholesale price contract may perform strictly better than a centralized supply chain; (ii) contracts widely studied in the supply chain coordination literature (e.g., markdown money, sales rebates, and buyback contracts) can serve as a commitment device as well as an incentive-coordinating device; and (iii) some of the above contracts cannot allocate profits arbitrarily between supply chain members because of strategic customer behavior.</description><author>Su, Xuanming; Zhang, Fuqiang</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The Make-or-Buy Decision in the Presence of a Rival: Strategic Outsourcing to a Common Supplier</title><link>http://www.example.com/articles/1</link><description>Arya, Anil; Mittendorf, Brian; Sappington, David E. M.
Firms routinely decide whether to make essential inputs themselves or buy the inputs from independent suppliers. Conventional wisdom suggests that a firm will not buy an input for a price above its in-house cost of production. We show that this is not necessarily the case when a monopolistic input supplier also serves the firm's retail rival. In this case, the decision to buy the input (and thus become one of the supplier's customers) can limit the incentive the supplier would otherwise have to provide the input on particularly favorable terms to the retail rival. Thus, a retail competitor may pay a premium to outsource production to a common supplier in order to raise its rivals' costs.</description><author>Arya, Anil; Mittendorf, Brian; Sappington, David E. M.</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Remanufacturing as a Marketing Strategy</title><link>http://www.example.com/articles/1</link><description>Atasu, Atalay; Sarvary, Miklos; Van Wassenhove, Luk N.
The profitability of remanufacturing systems for different cost, technology, and logistics structures has been extensively investigated in the literature. We provide an alternative and somewhat complementary approach that considers demand-related issues, such as the existence of green segments, original equipment manufacturer competition, and product life-cycle effects. The profitability of a remanufacturing system strongly depends on these issues as well as on their interactions. For a monopolist, we show that there exist thresholds on the remanufacturing cost savings, the green segment size, market growth rate, and consumer valuations for the remanufactured products, above which remanufacturing is profitable. More important, we show that under competition remanufacturing can become an effective marketing strategy, which allows the manufacturer to defend its market share via price discrimination.</description><author>Atasu, Atalay; Sarvary, Miklos; Van Wassenhove, Luk N.</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Diversity in Resource Consumption Patterns and Robustness of Costing Systems to Errors</title><link>http://www.example.com/articles/1</link><description>Labro, Eva; Vanhoucke, Mario
Practitioners and academics hypothesize that when there is high diversity in resource consumption patterns, costing systems are more sensitive to errors. Given that firms' resources to enhance costing accuracy are typically constrained, it is argued that costing system refinement efforts should be focused on such cases, where they are likely to be most effective. However, little guidance is available on how to identify those situations where costing system refinement efforts (such as introducing an activity-based costing system) are likely to pay off most in terms of increased accuracy. Further, to our knowledge, the existing guidance provided by this high diversity rule of thumb has never been empirically tested. Using a simulation method, we address these issues in this paper. Specifically, we model various aspects, and degrees, of diversity in the resource consumption patterns to be reflected by the costing system and find that more diversity in resource consumption patterns only leads to increased costing system sensitivity to errors for some of the aspects of diversity studied. We also identify situations in which allocating costing system refinement resources to cases characterized by high diversity in resource consumption patterns is detrimental to improved accuracy.</description><author>Labro, Eva; Vanhoucke, Mario</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Financial Reporting and Conflicting Managerial Incentives: The Case of Management Buyouts</title><link>http://www.example.com/articles/1</link><description>Fischer, Paul E.; Louis, Henock
We analyze the effect of external financing concerns on managers' financial reporting behavior prior to management buyouts (MBOs). Prior studies hypothesize that managers intending to undertake an MBO have an incentive to manage earnings downward to reduce the purchase price. We hypothesize that managers also face a conflicting reporting incentive associated with their efforts to obtain external financing for the MBO and to lower their financing cost. Consistent with our hypothesis, we find that managers who rely the most on external funds to finance their MBOs tend to report less negative abnormal accrual prior to the MBOs. In addition, the relation between external financing and abnormal accruals is tempered when there are more fixed assets that can serve as collateral for debt financing.</description><author>Fischer, Paul E.; Louis, Henock</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Joint Bidding in the Name-Your-Own-Price Channel: A Strategic Analysis</title><link>http://www.example.com/articles/1</link><description>Amaldoss, Wilfred; Jain, Sanjay
In this paper, we study the name-your-own-price (NYOP) channel. We examine theoretically and empirically whether asking consumers to place a joint bid for multiple items, rather than bid one item at a time as practiced today, can increase NYOP retailers' profits. Relatedly, we also examine whether allowing consumers to self-select whether to place a joint bid or itemwise bids increases retailers' profits and consumers' surplus. We construct a dynamic model that incorporates both demand uncertainty and supply uncertainty to address these issues. Our theoretical analysis identifies the conditions under which joint bidding can increase both NYOP retailers' profits and consumers' surplus. We find that some consumers might bid more for the very same items when they place joint bids. The increase in bid amount is related to the fact that joint bidding reduces the chance of mismatch between NYOP retailers' costs and consumer bids. We conducted a laboratory study to assess the descriptive validity of some of the model predictions, because there are no field data on joint bidding in the NYOP channel. The results of the study are directionally consistent with our theory.</description><author>Amaldoss, Wilfred; Jain, Sanjay</author><pubDate>Wed, 01 Oct 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Friction in Related-Party Trade When a Rival Is Also a Customer</title><link>http://www.example.com/articles/1</link><description>Arya, Anil; Mittendorf, Brian; Yoon, Dae-Hee
There are many circumstances in which manufacturers provide inputs to wholesale customers only to subsequently compete with these wholesale customers in the retail realm. Such dual distribution arrangements commonly suffer from excessive encroachment in that the manufacturer's ex post retail aggression is harmful ex ante because it undercuts potential wholesale profits. This paper demonstrates that with dual distribution, a manufacturer can benefit from decentralized control and the use of transfer prices above marginal cost. Although these arrangements often create coordination concerns, a moderate presence of such concerns permits the manufacturer to credibly convey to its wholesale customer that it will not excessively encroach on its retail territory. This, in turn, permits the manufacturer to reap greater wholesale profits. We also note that this force can point to a silver lining in arm's-length (parity) requirements on transfer pricing in that they can solidify commitments to a particular retail posture.</description><author>Arya, Anil; Mittendorf, Brian; Yoon, Dae-Hee</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Social Preferences and Supply Chain Performance: An Experimental Study</title><link>http://www.example.com/articles/1</link><description>Loch, Christoph H.; Wu, Yaozhong
Supply chain contracting literature has traditionally focused on aligning incentives for economically rational players. Recent work has hypothesized that social preferences, as distinct from economic incentives, may influence behavior in supply chain transactions. Social preferences refer to intrinsic concerns for the other party's welfare, reciprocating a history of a positive relationship, and intrinsic desires for a higher relative payoff compared with the other party's when status is salient. This article provides experimental evidence that social preferences systematically affect economic decision making in supply chain transactions. Specifically, supply chain parties deviate from the predictions provided by self-interested profit-maximization models, such that relationship preference promotes cooperation, individual performance, and high system efficiency, sustainable over time; whereas status preference induces tough actions and reduces both system efficiency and individual performance.</description><author>Loch, Christoph H.; Wu, Yaozhong</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Market-Based Supply Chain Coordination by Matching Suppliers' Cost Structures with Buyers' Order Profiles</title><link>http://www.example.com/articles/1</link><description>Xia, Yu; Chen, Bintong; Kouvelis, Panos
We study competitive marketplaces with multiple suppliers and multiple buyers dealing with a single product. A buyer chooses the supplier who offers the best price for his order pro. le, as described by his order size and delivery frequency. A supplier's offering price reflects her logistic cost structure as captured by relevant economies of scale in a "setup cost" component, and storage- and distribution-related costs in a "holding cost per unit" component. We argue that the matching of buyers' order profiles to suppliers' cost structures is the main source of supply chain coordination benefit in this many-to-many supply chain. Such cost-effective matching can be achieved naturally through price competition among suppliers. We identify the segment of the buyer's order space that each supplier can win, and perform market share sensitivity analysis when a supplier's cost structure changes. The winning supplier, at the equilibrium of price competition, offers the lowest price of her closest competitor instead of the lowest price she can offer.</description><author>Xia, Yu; Chen, Bintong; Kouvelis, Panos</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capacity Investment Under Postponement Strategies, Market Competition, and Demand Uncertainty</title><link>http://www.example.com/articles/1</link><description>Anupindi, Ravi; Jiang, Li
We consider duopoly models where firms make decisions on capacity, production, and price under demand uncertainty. Capacity and price decisions are made, respectively, ex ante and ex post demand realizations. The interplay between the timings of demand realization and production decision endows firms with different capabilities. Flexible firms can postpone production decisions until the actual demand curve is observed, but inflexible firms cannot. Under general demand structures and cost functions, we characterize the equilibrium for a symmetric duopoly and establish the strategic equivalence of price and quantity competitions when firms are flexible. We investigate the stochastic order properties of capacity and profit and show that they both increase for a flexible firm when the market is more volatile. We find that flexibility allows a firm to increase investment in capacity and earn a higher profit while benefiting customers by keeping the price in a narrower range; strategic equivalence implies that these properties are robust to market conjectures. We also show that flexibility plays an important role in mitigating the destructive effect of competition when the demand shock is additive; the destructive effect is nonexistent for firms facing multiplicative demand shock. When flexibility decision is endogenous, a firm's strategic flexibility choice depends on the cost of technology as well as the nature of demand shock. In particular, faced with a multiplicative demand shock,firms always choose to be inflexible, whereas all the possible equilibria are observed under additive demand shocks.</description><author>Anupindi, Ravi; Jiang, Li</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning, Forgetting, and Sales</title><link>http://www.example.com/articles/1</link><description>Villas-Boas, Sofia Berto; Villas-Boas, J. Miguel
Sellers of almost any product or service rarely keep their prices constant through time and frequently offer price discounts or sales. This paper investigates an explanation of sales as a way for uninformed consumers to be willing to experience the product, and learn about its fit, and where informed consumers may forget about ( or change) their preferences. We investigate the role of the rate of consumer forgetting on the timing between sales, and of the rate of consumer learning and menu costs on the length of a sale. The rate of consumer forgetting can be linked to the length of purchase cycle and the level of consumer involvement. We show that the discount frequency and the discount depth are increasing in the rate of consumer forgetting, and that the discount frequency is increasing in the learning rate. The duration of a sale is increasing in the rate of consumer forgetting and the rate of consumer learning.</description><author>Villas-Boas, Sofia Berto; Villas-Boas, J. Miguel</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Natural Selection in Financial Markets: Does It Work?</title><link>http://www.example.com/articles/1</link><description>Yan, Hongjun
Can investors with incorrect beliefs survive in financial markets and have a significant impact on asset prices? My paper addresses this issue by analyzing a dynamic general equilibrium model where some investors have rational expectations, whereas others have incorrect beliefs concerning the mean growth rate of the economy. The main result is that an investor can survive if and only if he has the lowest survival index, which is a function of his belief accuracy, patience parameter, and relative risk aversion coefficient. If preferences are held constant across all investors, then those with incorrect beliefs cannot survive in the limit, although calibrations reveal that the selection process is excessively slow. However, if preferences vary across investors, even slightly, it becomes possible for an irrational investor to dominate the market even if his beliefs persistently and substantially deviate from the truth.</description><author>Yan, Hongjun</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Patents and the Performance of Voluntary Standard-Setting Organizations</title><link>http://www.example.com/articles/1</link><description>Rysman, Marc; Simcoe, Timothy
Voluntary standard-setting organizations (SSOs) are a common feature of systems industries, where firms supply interoperable components for a shared technology platform. These institutions promote coordinated innovation by providing a forum for collective decision making and a potential solution to the problem of fragmented and overlapping intellectual property rights. This paper examines the economic and technological significance of SSOs by analyzing the flow of citations to a sample of U. S. patents disclosed during the standard-setting process. Our main results show that the age distribution of SSO patent citations is shifted toward later years (relative to an average patent) and that citations increase substantially following standardization. These results suggest that SSOs identify promising technologies and influence their subsequent adoption.</description><author>Rysman, Marc; Simcoe, Timothy</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competing Through Cooperation: The Organization of Standard Setting in Wireless Telecommunications</title><link>http://www.example.com/articles/1</link><description>Leiponen, Aija Elina
This study examines cooperative standard setting in wireless telecommunications. Focusing on the competition among firms to influence formal standardization, the roles of standard-setting committees, private alliances, and industry consortia are highlighted. The empirical context is Third Generation Partnership Project (3GPP), an international standards-development organization in the wireless telecommunication industry. Panel data analyses exploiting natural experiments caused by a consortium merger and entry of Asian firms suggest that participation in industry consortia increases firms' contributions to the development of new technical specifications in 3GPP committees. Moreover, connections to standard-setting peers formed in consortia facilitate change requests to ongoing specifications. These results suggest that if firms in network technology industries want to influence the evolution of their industry, they should identify both formal standard-setting committees and industry consortia in which they can discuss, negotiate, and align positions on technical features with their peers. For policymakers, these results suggest that it is important to ensure that technical consortia remain open for all industry actors and that membership fees do not become prohibitive to small and resource-constrained players.</description><author>Leiponen, Aija Elina</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Ethical Spillovers in Firms: Evidence from Vehicle Emissions Testing</title><link>http://www.example.com/articles/1</link><description>Pierce, Lamar; Snyder, Jason
In this paper, we explore how organizations influence the unethical behavior of their employees. Using a unique data set of over three million vehicle emissions tests, we find strong evidence of ethical spillovers from firms to individuals. When inspectors work across different organizations, they adjust the rate at which they pass vehicles to the norms of those with whom they work. These spillovers are strongest at large facilities and corporate chains, and weakest for the large-volume inspectors. These results are consistent with the economics literature on productivity spillovers from organizations and peers and suggest that managers can influence the ethics of employee behavior through both formal norms and incentives. The results also suggest that employees have persistent ethics that limit the magnitude of this influence. These results imply that if ethical conformity is important to the financial and legal health of the organization, managers must be vigilant in their hiring, training, and monitoring to ensure that employee behavior is consistent with firm objectives.</description><author>Pierce, Lamar; Snyder, Jason</author><pubDate>Sat, 01 Nov 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service-level agreements in call centers: Perils and prescriptions</title><link>http://www.example.com/articles/1</link><description>Milner, Joseph M.; Olsen, Tava Lennon
nan</description><author>Milner, Joseph M.; Olsen, Tava Lennon</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimizing call center staffing using simulation and analytic center cutting-plane methods</title><link>http://www.example.com/articles/1</link><description>Atlason, Julius; Epelman, Marina A.; Henderson, Shane G.
nan</description><author>Atlason, Julius; Epelman, Marina A.; Henderson, Shane G.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revisiting "Retailer- vs. Vendor-Managed Inventory and Brand Competition"</title><link>http://www.example.com/articles/1</link><description>Kim, Hag-Soo
In a recent paper, Mishra and Raghunathan (Mishra, B. K.,. Raghunathan. 2004. Retailer- vs. vendor-managed inventory and brand competition. Management Sci. 50(4) 445-457) claimed that retailers prefer vendor-managed inventory (VMI) because it restores competition among manufacturers and encourages them to maintain a higher stock of their own brand under VMI than would be maintained under retailer-managed inventory (RMI). This paper shows that these results do not hold in general: each manufacturer's stocking level may be higher under RMI than under VMI and the retailer may lose by adopting VMI depending on the profit margin of the, retailer relative to that of the manufacturers, the magnitude of the holding costs, and the intensity of brand competition. Numerical examples show that the effect of brand competition on the system is not so significant and the transfer of holding cost to the vendor is the key incentive for the retailer's adoption of VMI.</description><author>Kim, Hag-Soo</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customized bundle pricing for information goods: A nonlinear mixed-integer programming approach</title><link>http://www.example.com/articles/1</link><description>Wu, Shin-yi; Hitt, Lorin M.; Chen, Pei-yu; Anandalingarn, G.
This paper proposes using nonlinear mixed-integer programming to solve the customized bundle-pricing problem in which consumers are allowed to choose up to N goods out of a larger pool of J goods. Prior work has suggested that this mechanism has attractive features for the pricing of information and other low-marginal cost goods. Although closed-form solutions exist for this problem for certain cases of consumer preferences, many interesting scenarios cannot be easily handled without a numerical solution procedure. In this paper, we investigate the efficiency gains created by customized bundling over the alternatives of pure bundling or individual sale under different assumptions about customer preferences and firm cost structure, as well as the potential loss of efficiency caused by pricing with incomplete information about consumer reservation values. Our analysis suggests that customized bundling enhances sellers' profits and enhances welfare when consumers do not place positive values on all goods, and that this consumer characteristic is much more important than the shape of the valuation distribution in determining the optimal pricing scheme. We also find that customized bundling outperforms both pure bundling and individual sale in the presence of incomplete information, and that customized bundling still outperforms other simpler pricing schemes evert when exact consumer valuations are not known ex ante.</description><author>Wu, Shin-yi; Hitt, Lorin M.; Chen, Pei-yu; Anandalingarn, G.</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Allocation models and heuristics for the outsourcing of repairs for a dynamic warranty population</title><link>http://www.example.com/articles/1</link><description>Ding, Li; Glazbrook, Kevin D.; Kirkbride, Christopher
W e consider a scenario in which a large equipment manufacturer wishes to outsource the work involved in repairing purchased goods while under warranty. Several external service vendors are available for this work. We develop models and analyses to support decisions concerning how responsibility for the warranty population should be divided between them. These also allow the manufacturer to resolve related questions concerning, for example, whether the service capacities of the contracted vendors are sufficient to deliver an effective post-sales service. Static allocation models yield information concerning the proportions of the warranty population for which the vendors should be responsible overall. Dynamic allocation models enable consideration of how such overall workloads might be delivered to the vendors over time in a way which avoids excessive variability in the repair burden. We apply dynamic programming policy improvement to develop an effective dynamic allocation heuristic. This is evaluated numerically and is also used as a yardstick to assess two simple allocation heuristics suggested by static models. A dynamic greedy allocation heuristic is found to perform well. Dividing the workload equally among vendors with different service capacities can lead to serious losses.</description><author>Ding, Li; Glazbrook, Kevin D.; Kirkbride, Christopher</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Mean variance vulnerability</title><link>http://www.example.com/articles/1</link><description>Eichner, Thomas
This paper transfers the concept of Gollier and Pratt's (Gollier, C., J. W. Pratt. 1996. Risk vulnerability and the tempering effect of background risk. Econometrica 64 1109-1123). risk vulnerability into mean variance preferences. Risk vulnerability is shown to be equivalent to the slope of the mean variance indifference curve being decreasing in mean and increasing in variance. Next, we introduce the notion of mean variance vulnerability to link the concepts of decreasing absolute disk aversion, risk vulnerability, properness, and standardness. All of the abovementioned concepts are characterized in terms of mean variance indifference curve properties and in terms of absolute risk measures.</description><author>Eichner, Thomas</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incorporating asymmetric distributional information in robust value-at-risk optimization</title><link>http://www.example.com/articles/1</link><description>Natarajan, Karthik; Pachamanova, Dessislava; Sim, Melvyn
Value-at-Risk (VaR) is one of the most widely accepted risk measures in the financial and insurance industries, yet efficient optimization of VaR remains a very difficult problem. We propose a computationally tractable approximation method for minimizing the VaR of a portfolio based on robust optimization techniques. The method results in the optimization of a modified VaR measure, Asymmetry-Robust VaR (ARVaR), that takes into consideration asymmetries in the distributions of returns and is coherent, which makes it desirable from a financial theory perspective. We show that ARVaR approximates the Conditional VaR of the portfolio as well. Numerical experiments with simulated and real market data indicate that the proposed approach results in lower realized portfolio VaR, better efficient frontier, and lower maximum realized portfolio loss than alternative approaches for quantile-based portfolio risk minimization.</description><author>Natarajan, Karthik; Pachamanova, Dessislava; Sim, Melvyn</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Scheduling arrivals to queues: A single-server model with no-shows</title><link>http://www.example.com/articles/1</link><description>Hassin, Refael; Mendel, Sharon
Queueing systems with scheduled arrivals, i.e., appointment systems, are typical for frontal service systems, e.g., health clinics. An aspect of customer behavior that influences the overall efficiency of such systems is the phenomenon of no-shows. The consequences of no-shows cannot be underestimated; e.g., British surveys reveal that in the United Kingdom alone more than 12 million general practitioner (GP) appointments are missed every year, costing the British health service an estimated 250 pound million annually. In this study we answer the following key questions: How should the schedule be computed when there are no-shows? Is it sufficiently accurate to use a schedule designed for the same expected number of customers without no-shows? How important is it to invest in efforts that reduce no-shows-i.e., given that we apply a schedule that takes no-shows into consideration, is the existence of no-shows still costly to the server and customers?</description><author>Hassin, Refael; Mendel, Sharon</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management of callable products</title><link>http://www.example.com/articles/1</link><description>Gallego, Guillermo; Kou, S. G.; Phillips, Robert
A callable product is a unit of capacity sold to self-selected low-fare customers who willingly grant the capacity provider the option to "call" the capacity at a prespecified recall price. We analyze callable products in a finite-capacity setting with two fare classes where low-fare customers book first, and show that callable products provide a riskless source of additional revenue to the capacity provider. An optimal recall price and an optimal discount-fare booking limit for the two-period problem are obtained. Numerical examples show the benefits from offering callable products can be significant, especially when high-fare demand uncertainty is large. Extensions to multifare structures, network models, overbooking, and to other industries are discussed.</description><author>Gallego, Guillermo; Kou, S. G.; Phillips, Robert</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A simultaneous model of consumer brand choice and negotiated price</title><link>http://www.example.com/articles/1</link><description>Chen, Yuxin; Yang, Sha; Zhao, Ying
In this paper, we develop a simultaneous model of consumer brand choice and negotiated price in the highly relevant marketing context of automobile transactions. Consumer brand choice is modeled as a multinomial probabilistic outcome, and the individual consumer-level transaction price is modeled to be consistent with the Nash bargaining solution. Our simultaneous model introduces a natural way to account for price endogeneity in a microlevel choice setting when the price paid is individual specific and correlated with the idiosyncratic preference of a buyer. It also accounts for the potential selectivity bias in studying the negotiated price by integrating consumer brand choice modeling. We apply the proposed model to a data set of 4,795 consumer automobile purchases. The empirical results show that the proposed approach not only better fits the data of consumer choice and negotiated price, but it also provides additional insights into buyers' and sellers' behaviors in comparison with models that study only consumer choice or the negotiated price, but not both simultaneously.</description><author>Chen, Yuxin; Yang, Sha; Zhao, Ying</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and operational recourse in coproduction systems</title><link>http://www.example.com/articles/1</link><description>Tomlin, Brian; Wang, Yimin
Coproduction systems, in which multiple products are produced simultaneously in a single production run, are prevalent in many industries. Such systems typically produce a random quantity of vertically differentiated products. This product hierarchy enables the firm to fill demand for a lower-quality product by converting a higher-quality product. In addition to the challenges presented by random yields and multiple products, coproduction systems often serve multiple customer classes that differ in their product valuations. Furthermore, the sizes of these classes are uncertain, Employing a utility-maximizing customer model, we investigate the production, pricing, downconversion, and allocation decisions in a two-class, stochastic-demand, stochastic-yield coproduction system. For the single-class case, we establish that downconversion will not occur if prices are set optimally. In contrast, we show that downconversion can be optimal in the two-class case, even if prices are set optimally. We consider the benefit of postponing certain operational decisions, e.g., the pricing or allocation-rule decisions, until after uncertainties are resolved. We use the term recourse to denote actions taken after uncertainties have been resolved. We find that recourse pricing benefits the firm much more than either downconversion or recourse allocation do, implying that recourse demand management is more valuable than recourse supply management. Special cases of our model include the single-class and two-class random-yield newsvendor models.</description><author>Tomlin, Brian; Wang, Yimin</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Corporate venturing, allocation of talent, and competition for star managers</title><link>http://www.example.com/articles/1</link><description>de Bettignies, Jean-Etienne; Chemla, Gilles
We provide new rationales for corporate venturing, based on competition for talented managers. As returns to venturing increase, firms engage in corporate venturing for reasons other than, capturing these returns. First, higher venturing returns increase managerial compensation, to which firms respond by increasing incentives. Managers increase effort, prompting firms to reallocate them to new ventures, where the marginal product of effort is highest. Second, as returns to venturing become large, corporate venturing emerges as a way to recruit/retain managers who would otherwise choose alternative employment. We derive several testable empirical predictions about the determinants and structure of corporate venturing.</description><author>de Bettignies, Jean-Etienne; Chemla, Gilles</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The predictive power of three prominent tournament formats</title><link>http://www.example.com/articles/1</link><description>Ryvkin, Dmitry; Ortmann, Andreas
Tournaments of heterogeneous candidates can be thought of as probabilistic mechanisms that select high-quality agents. We quantify the efficiency of such selection by the likelihood of selecting the best player, here termed the "predictive power." We study three widely used tournament formats: contests, binary elimination tournaments, and round-robin tournaments. Using a simple model, we demonstrate analytically, and through simulations, how the predictive power of these formats depends on the number of players,noise level, and distribution of players' types. We also present the results of exploratory simulations for two alternative criteria of selection efficiency: the expected ability of the winner and the expected rank of the winner. All three criteria may exhibit unexpected nonmonotonic behavior as functions of the number of players and/or noise level. We discuss the conditions under which different types of behavior should be expected, and their implications for managerial decisions.</description><author>Ryvkin, Dmitry; Ortmann, Andreas</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Online consumer review: Word-of-mouth as a news element of marketing communication mix</title><link>http://www.example.com/articles/1</link><description>Chen, Yubo; Xie, Jinhong
As a new type of word-of-mouth information, online consumer product review is an emerging market phenomenon that is playing an increasingly important role in consumers' purchase decisions. This paper argues that online consumer review, a type of product information created by users based on personal usage experience, can serve as a new element in the marketing communications mix and work as free "sales assistants" to help consumers identify the products that best match their idiosyncratic usage conditions. This paper develops a normative model to address several important strategic issues related to consumer reviews. First, we show when and how the seller should adjust its own marketing communication strategy in response to consumer reviews. Our results reveal that if the review information is sufficiently informative, the two types of product information, i.e., the seller-created product attribute information and buyer-created review information, will interact with each other. For example, when the product cost is low and/or there are sufficient expert (more sophisticated) product users, the two types of information are complements, and the seller's best response is to increase the amount of product attribute information conveyed via its marketing communications after the reviews become available. However, when the product cost is high and there are sufficient novice (less sophisticated) product users, the two types of information are substitutes, and the seller's best response is to reduce the amount of product attribute information it offers, even if it is cost-free to provide such information. We also derive precise conditions under which the seller can increase its profit by adopting a proactive strategy, i.e., adjusting its marketing strategies even before consumer reviews become available. Second, we identify product/market conditions under which the seller benefits from facilitating such buyer-created information(e.g., by allowing consumers to post user-based product reviews on the seller's website). Finally, we illustrate the importance of the timing of the introduction of consumer reviews available as a strategic variable and show that delaying the availability of consumer reviews for a given product can be beneficial if the number of expert (more sophisticated) product users is relatively large and cost of the product is low.</description><author>Chen, Yubo; Xie, Jinhong</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The sound of silence in online feedback: Estimating trading risks in the presence of reporting bias</title><link>http://www.example.com/articles/1</link><description>Dellarocas, Chrysanthos; Wood, Charles A.
Most online feedback mechanisms rely on voluntary reporting of privately observed outcomes. This introduces the potential for reporting bias, a situation where traders exhibit different propensities to report different outcome types to the system. Unless properly accounted for, reporting bias may severely distort the distribution of public feedback relative to the underlying distribution of private transaction outcomes and, thus, hamper the reliability of feedback mechanisms. This study offers a method that allows users of feedback mechanisms where both partners of a bilateral exchange are allowed to report their satisfaction to "see through" the distortions introduced by reporting bias and derive unbiased estimates of the underlying distribution of privately observed outcomes. A key aspect of our method lies in extracting information from the number of transactions where one or both trading partners choose to remain silent. We apply our method to a large data set of eBay feedback. Our results support the widespread belief that eBay traders are more likely to post feedback when satisfied than when dissatisfied and are consistent with the presence of positive and negative reciprocation among eBay traders. Most importantly, our analysis derives unbiased estimates of the risks that are associated with trading on eBay that, we believe, are more realistic than those suggested by a naive interpretation of the unusually high (&gt;99%) levels of positive feedback currently found on that system.</description><author>Dellarocas, Chrysanthos; Wood, Charles A.</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Foreign investments of US individual investors: Causes and consequences</title><link>http://www.example.com/articles/1</link><description>Bailey, Warren; Kumar, Alok; Ng, David
Using thousands of brokerage accounts of U.S. individual investors, we analyze the motivations and consequences of foreign equity investment. We find that diversification is not the only reason that investors trade foreign securities. While wealthier, more experienced investors enjoy an informational advantage and, thus, are more likely to invest overseas and experience good portfolio performance, other investors appear to venture. abroad for the wrong reasons. In particular, behaviorally biased investors often underuse or misuse foreign equity securities and experience poor portfolio performance. Some investors appear to use foreign securities for speculation or to improve upon poor domestic portfolio performance.</description><author>Bailey, Warren; Kumar, Alok; Ng, David</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managerial expertise, private information, and pay-performance sensitivity</title><link>http://www.example.com/articles/1</link><description>Dutta, Sunil
This paper characterizes optimal pay-performance sensitivities of compensation contracts for managers who have private information about their skills, and those skills affect their outside employment opportunities. The model presumes that the rate at which a manager's opportunity wage increases in his expertise depends on the nature of that expertise, i.e., whether it is general or-firm specific. The analysis demonstrates that when managerial expertise is largely firm specific (general), the optimal pay-performance sensitivity is lower (higher) than its optimal value in a benchmark setting of symmetric information. Furthermore, when managerial skills are largely firm specific (general), the optimal pay-performance sensitivity decreases (increases) as managerial skills become a more important determinant of firm performance. Unlike the standard agency-theoretic prediction of a negative trade-off between risk and pay-performance sensitivity, this paper identifies plausible circumstances under which risk and incentives are positively associated. In addition to providing an explanation for why: empirical tests of risk-incentive relationships hive produced mixed results, the analysis generates insights that can be useful in guiding future empirical research.</description><author>Dutta, Sunil</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Human capital and institutional effects in the compensation of information technology professionals in the United States</title><link>http://www.example.com/articles/1</link><description>Mithas, Sunil; Krishnan, M. S.
This paper studies the influence of supply-side and demand-side factors on the compensation of information technology (IT) professionals and considers the human capital and institutional explanations. We focus on returns to an MBA and the IT-related experience of IT professionals and use the largest data set of IT professionals that has been compiled to date in the United States to answer our research questions. We find that firms pay a significant premium for an MBA. Although firms value the IT experience of IT professionals, they value an MBA significantly more. The results of this study cast doubt on the belief that IT skills have a large firm-specific component. Although IT experience is valued more than non-IT experience for IT professionals, firms value IT experience at other firms much more than they value firm-specific IT experience. Likewise, contrary to popular perception, we do not find evidence for complementarities between an MBA education and IT experience. Among institutional effects, firms in IT and IT-intensive industries pay significantly more to IT professionals than do other firms, and dot-com firms also paid a significant premium in 1999 and 2000. However, these firms do not value an MBA or firm-specific IT experience any more than other firms. We discuss the implications of these findings for further research, for firms' compensation practices, and for individual IT professionals.</description><author>Mithas, Sunil; Krishnan, M. S.</author><pubDate>Sat, 01 Mar 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service-level differentiation in call centers with fully flexible servers</title><link>http://www.example.com/articles/1</link><description>Gurvich, Itay; Armony, Mor; Mandelbaum, Avishai
nan</description><author>Gurvich, Itay; Armony, Mor; Mandelbaum, Avishai</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling and analysis of call center arrival data: A Bayesian approach</title><link>http://www.example.com/articles/1</link><description>Soyer, Refik; Tarimcilar, M. Murat
nan</description><author>Soyer, Refik; Tarimcilar, M. Murat</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comparison of univariate time series methods for forecasting intraday arrivals at a call center</title><link>http://www.example.com/articles/1</link><description>Taylor, James W.
nan</description><author>Taylor, James W.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The debate on influencing doctors' decisions: Are drug characteristics the missing link? (vol 53, pg 1688, 2007)</title><link>http://www.example.com/articles/1</link><description>Venkataraman, Sriram; Stremersch, Stefan
nan</description><author>Venkataraman, Sriram; Stremersch, Stefan</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Public evacuation decisions and hurricane track uncertainty</title><link>http://www.example.com/articles/1</link><description>Regnier, Eva
Public officials with the authority to order hurricane evacuations face a difficult trade-off between risks to life and costly false alarms. Evacuation decisions must be made on the basis of imperfect information, in the form of forecasts. The quality of these decisions can be improved if they are also informed by measures of uncertainty about the forecast, including estimates of the value of waiting for updated, more accurate, forecasts. Using a stochastic model of storm motion derived from historic tracks, this paper explores the relationship between lead time and track uncertainty for Atlantic hurricanes and the implications of this relationship for evacuation decisions. Typical evacuation clearance times and track uncertainty imply that public officials who require no more than a 10% probability of failing to evacuate before a striking hurricane ( a false negative) must accept that at least 76%-and for some locations over 90%-of evacuations will be false alarms. Reducing decision lead times from 72 to 48 hours for major population centers could save an average of hundreds of millions of dollars in evacuation costs annually, with substantial geographic variation in savings.</description><author>Regnier, Eva</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A path-based approach for hazmat transport network design</title><link>http://www.example.com/articles/1</link><description>Verter, Vedat; Kara, Bahar Y.
The people living and working around the roads used for hazardous material (hazmat) shipments face the risk of suffering undesirable consequences of an accident. The main responsibility to mitigate the hazmat transport risk at a population zone belongs to the government agency with jurisdiction over that region. One of the common policy tools is to close certain road links to vehicles carrying hazmats. In effect, the road network available to dangerous goods carriers can be determined by the regulator. The transport risk in the region, however, is determined by the carriers' routing decisions over the available road network. Thus, the regulator needs to make the road closure decisions so that the total risk resulting from the carriers' route choices is minimized. We provide a path-based formulation for this network design problem. Alternative solutions can be generated by varying the routing options included in the model for each shipment. Each solution corresponds to a certain compromise between the two parties in terms of transport risk and economic viability. The proposed framework can be used for identifying mutually agreeable hazmat transport policies. We present two applications of the methodology to illustrate the insights that can be gained through its use: The first application focuses on hazmat shipments through the highway network of Western Ontario, Canada, whereas the second application studies the problem in a much larger geographical region that covers the provinces of Ontario and Quebec.</description><author>Verter, Vedat; Kara, Bahar Y.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Structural estimation of the newsvendor model: An application to reserving operating room time</title><link>http://www.example.com/articles/1</link><description>Olivares, Marcelo; Terwiesch, Christian; Cassorla, Lydia
The newsvendor model captures the trade-off faced by a decision maker that needs to place a firm bet prior to the occurrence of a random event. Previous research in operations management has mostly focused on deriving the decision that minimizes the expected mismatch costs. In contrast, we present two methods that estimate the unobservable cost parameters characterizing the mismatch cost function. We present a structural estimation framework that accounts for heterogeneity in the uncertainty faced by the newsvendor as well as in the cost parameters. We develop statistical methods that give consistent estimates of the model primitives, and derive their asymptotic distribution, which is useful to do hypothesis testing. We apply our econometric model to a hospital that balances the costs of reserving too much versus too little operating room capacity to cardiac surgery cases. Our results reveal that the hospital places more emphasis on the tangible costs of having idle capacity than on the costs of schedule overrun and long working hours for the staff. We also extend our structural models to incorporate external information on forecasting biases and mismatch costs reported by the medical literature. Our analysis suggests that overconfidence and incentive conflicts are important drivers of the frequency of schedule overruns observed in our sample.</description><author>Olivares, Marcelo; Terwiesch, Christian; Cassorla, Lydia</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Generating objectives: Can decision makers articulate what they want?</title><link>http://www.example.com/articles/1</link><description>Bond, Samuel D.; Carlson, Kurt A.; Keeney, Ralph L.
Objectives have long been considered a basis for sound decision making. This research examines the ability of decision makers to generate self-relevant objectives for consequential decisions. In three empirical studies, participants consistently omitted nearly half of the objectives that they later identified as personally relevant. More surprisingly, omitted objectives were perceived to be almost as important as those generated by participants on their own. These empirical results were replicated in a real-world case study of strategic decision making at a high-tech firm. Overall, our research suggests that decision makers are considerably deficient in utilizing personal knowledge and values to form objectives for the decisions they face.</description><author>Bond, Samuel D.; Carlson, Kurt A.; Keeney, Ralph L.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Predicting product purchase from inferred customer similarity: An autologistic model approach</title><link>http://www.example.com/articles/1</link><description>Moon, Sangkil; Russell, Gary J.
Product recommendation models are key tools in customer relationship management (CRM). This study develops a product recommendation model based on the principle that customer preference similarity stemming from prior purchase behavior is a key element in predicting current product purchase. The proposed recommendation model is dependent on two complementary methodologies: joint space mapping (placing customers and products on the same psychological map) and spatial choice modeling (allowing observed choices to be correlated across customers). Using a joint space map based on past purchase behavior, a predictive model is calibrated in which the probability of product purchase depends on the customer's relative distance to other customers on the map. An empirical study demonstrates that the proposed approach provides excellent forecasts relative to benchmark models for a customer database provided by an insurance firm.</description><author>Moon, Sangkil; Russell, Gary J.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the recoverability of choice behaviors with random coefficients choice models in the context of limited data and unobserved effects</title><link>http://www.example.com/articles/1</link><description>Andrews, Rick L.; Ainslie, Andrew; Currim, Imran S.
Random coefficients choice models are seeing widespread adoption in marketing research, partly because of their ability to generate household-level parameter estimates with limited data. However, the power of such models may tempt researchers to trust that they continue to produce reasonable estimates, when in fact either model misspecification or insufficient data limits the models' ability to recover household-level parameters successfully. If household-level choice behaviors are not recovered successfully, managerial decisions such as marketing-mix planning and targeting, direct marketing, segmentation, and forecasting may not produce the desired results. This study addresses the following questions. First, can random coefficients choice models correctly identify markets characterized by preference and response heterogeneity, state dependence, the use of alternative decision heuristics that result in reduced choice sets, and combinations of these effects? If so, how much data is required, and is this realistic given the size of data sets typically used in marketing analyses? Which model selection criteria should be used to identify these markets? When there is spurious market identification, which parameters contribute to the spurious result? An extensive simulation experiment is conducted wherein random coefficients logit models with varying specifications of parameter heterogeneity, state dependence effects, and choice set heterogeneity are applied to 128 experimental conditions. The results show which types of markets can be identified reliably and which cannot. Based on the results of the simulation, the authors develop a model selection heuristic that identifies the correct market in 81% of the experimental conditions. In contrast, strict application of the best model selection criterion alone results in correct market identification in at most 34% of experimental conditions. Interestingly, we find that the amount of data (number of households or number of purchases per household) does not affect our ability to identify the correct market type with this heuristic, so there is a good chance of identifying the correct market type even when little data is available.</description><author>Andrews, Rick L.; Ainslie, Andrew; Currim, Imran S.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customer lifetime value measurement</title><link>http://www.example.com/articles/1</link><description>Borle, Sharad; Singh, Siddharth S.; Jain, Dipak C.
The measurement of customer lifetime value is important because it is used as a metric in evaluating decisions in the context of customer relationship management. For a firm, it is important to form some expectations as to the lifetime value of each customer at the time a customer starts doing business with the firm, and at each purchase by the customer. In this paper, we use a hierarchical Bayes approach to estimate the lifetime value of each customer at each purchase occasion by jointly modeling the purchase timing, purchase amount, and risk of defection from the firm for each customer. The data come from a membership-based direct marketing company where the times of each customer joining the membership and terminating it are known once these events happen. In addition, there is an uncertain relationship between customer lifetime and purchase behavior. Therefore, longer customer lifetime does not necessarily imply higher customer lifetime value. We compare the performance of our model with other models on a separate validation data set. The models compared are the extended NBD-Pareto model, the recency, frequency, and monetary value model, two models nested in our proposed model, and a heuristic model that takes the average customer lifetime, the average interpurchase time, and the average dollar purchase amount observed in our estimation sample and uses them to predict the present value of future customer revenues at each purchase occasion in our hold-out sample. The results show that our model performs better than all the other models compared both at predicting customer lifetime value and in targeting valuable customers. The results also show that longer interpurchase times are associated with larger purchase amounts and a greater risk of leaving the firm. Both male and female customers seem to have similar interpurchase time intervals and risk of leaving; however, female customers spend less compared with male customers.</description><author>Borle, Sharad; Singh, Siddharth S.; Jain, Dipak C.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cultivating trust and harvesting value in virtual communities</title><link>http://www.example.com/articles/1</link><description>Porter, Constance Elise; Donthu, Naveen
Although previous scholars have examined the value of virtual communities to customers, in this study we investigate the role of a firm's efforts in cultivating trust and harvesting value for themselves via the virtual communities that they sponsor. We hypothesize that the perceptions of a firm's efforts to provide quality content, to foster member embeddedness, and to encourage interaction foster favorable customer beliefs about and trust in a virtual community sponsor. Further, we hypothesize that trust motivates customers to behave relationally toward the sponsoring firm by sharing information with, coproducing new products with, and granting loyalty to, the sponsoring firm. Data from 663 customers are analyzed using structural equation modeling techniques. We find that efforts to provide quality content and foster member embeddedness have positive effects on customer beliefs about the sponsor. In fact, fostering member embeddedness has a stronger explanatory effect on customer beliefs than does providing quality content. However, despite the fact that previous studies show that customers value interaction in virtual communities, our findings suggest that firms must do more than encourage interaction among their community members if they hope to create value from their virtual communities.</description><author>Porter, Constance Elise; Donthu, Naveen</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Building brand awareness in dynamic oligopoly markets</title><link>http://www.example.com/articles/1</link><description>Naik, Prasad A.; Prasad, Ashutosh; Sethi, Suresh P.
Companies spend hundreds of millions of dollars annually on advertising to build and maintain awareness for their brands in competitive markets. However, awareness formation models in the marketing literature ignore the role of competition. Consequently, we lack both the empirical knowledge and normative understanding of building brand awareness in dynamic oligopoly markets. To address this gap, we propose an N-brand awareness formation model, design an extended Kalman filter to estimate the proposed model using market data for five car brands over time, and derive the optimal closed-loop Nash equilibrium strategies for every brand. The empirical results furnish strong support for the proposed model in terms of both goodness-of-fit in the estimation sample and cross-validation in the out-of-sample data. In addition, the estimation method offers managers a systematic way to estimate ad effectiveness and forecast awareness levels for their particular brands as well as competitors' brands. Finally, the normative analysis reveals an inverse allocation principle that suggests-contrary to the proportional-to-sales or competitive parity heuristics -that large (small) brands should invest in advertising proportionally less (more) than small (large) brands.</description><author>Naik, Prasad A.; Prasad, Ashutosh; Sethi, Suresh P.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory management with auctions and other sales channels: Optimality of (s, S) policies</title><link>http://www.example.com/articles/1</link><description>Huh, Woonghee Tim; Janakiraman, Ganesh
We study periodic-review inventory replenishment problems with fixed ordering costs, and show the optimality of (s, S) inventory replenishment policies. Inventory replenishment is instantaneous, i.e., the lead time is zero. We consider several sales mechanisms, e. g., auction mechanisms, name-your-own-price mechanisms, and multiple heterogeneous sales channels. We prove this result by showing that these models satisfy a recently-established sufficient condition for the optimality of (s, S) policies. Thus, this paper shows that the optimality of (s, S) policies extends well beyond the traditional sales environments studied so far in the inventory literature.</description><author>Huh, Woonghee Tim; Janakiraman, Ganesh</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Financing the entrepreneurial venture</title><link>http://www.example.com/articles/1</link><description>de Bettignies, Jean-Etienne
We model financial contracting in entrepreneurial ventures. In our incomplete contracts framework, the entrepreneur can design contracts contingent on three possible control right allocations: entrepreneur control, investor control, and joint control, with each allocation inducing different effort levels by both the entrepreneur and the investor. We find that a variety of contracts resembling financial instruments commonly used in practice, such as common stock, straight and convertible preferred equity, and secured and unsecured debt, can emerge as optimal, depending on two key factors: entrepreneur/investor effort complementarity and investors' opportunity cost of capital. The results of our model are consistent with, and yield new explanations for, empirical regularities such as (a) the prevalence of equity-type contracts in high-growth ventures and of debt-type contracts in lifestyle ventures; (b) geographical and temporal differences in equity-type instruments used in high-growth ventures; and (c) the impact of firm and loan characteristics on the choice between secured and unsecured debt.</description><author>de Bettignies, Jean-Etienne</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Is the tendency to engage in entrepreneurship genetic?</title><link>http://www.example.com/articles/1</link><description>Nicolaou, Nicos; Shane, Scott; Cherkas, Lynn; Hunkin, Janice; Spector, Tim D.
We used quantitative genetics techniques to compare the entrepreneurial activity of 870 pairs of monozygotic (MZ) and 857 pairs of same-sex dizygotic (DZ) twins from the United Kingdom. We ran model-fitting analyses to estimate the genetic, shared environmental and nonshared environmental effects on the propensity of people to become entrepreneurs. We found relatively high heritabilities for entrepreneurship across different operationalizations of the phenomenon, with little effect of family environment and upbringing. Our findings suggest the importance of considering genetic factors in explanations for why people engage in entrepreneurial activity.</description><author>Nicolaou, Nicos; Shane, Scott; Cherkas, Lynn; Hunkin, Janice; Spector, Tim D.</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Code reuse in open source software</title><link>http://www.example.com/articles/1</link><description>Haefliger, Stefan; von Krogh, Georg; Spaeth, Sebastian
Code reuse is a form of knowledge reuse in software development that is fundamental to innovation in many fields. However, to date there has been no systematic investigation of code reuse in open source software projects. This study uses quantitative and qualitative data gathered from a sample of six open source software projects to explore two sets of research questions derived from the literature on software reuse in firms and open source software development. We find that code reuse is extensive across the sample and that open source software developers, much like developers in firms, apply tools that lower their search costs for knowledge and code, assess the quality of software components, and have incentives to reuse code. Open source software developers reuse code because they want to integrate functionality quickly, because they want to write preferred code, because they operate under limited resources in terms of time and skills, and because they can mitigate development costs through code reuse.</description><author>Haefliger, Stefan; von Krogh, Georg; Spaeth, Sebastian</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Maximum commonality problems: Applications and analysis</title><link>http://www.example.com/articles/1</link><description>Dawande, Milind; Kumar, Subodha; Mookerjee, Vijay; Sriskandarajah, Chelliah
Recently, an agile software development technique called extreme programming has caught the attention of practitioners and researchers in the software industry. A core practice of extreme programming is pair programming, where two developers work on the same piece of code. We introduce the problem of assigning pairs of developers to modules so as to maximize the commonality -a measure of the extent to which common developers work on related modules -subject to a load-balancing constraint that is motivated by the need to control the completion time of the project. We consider two variants of this problem. In MCAP(n), a developer is teamed up with exactly one other developer to form a pair that works together for the entire duration of the project. In MCAP(s), we allow a developer to pair with more than one other developer during the project. This "pair-splitting" version of the problem facilitates knowledge dissemination among developers, but can increase the effort needed for a developer to adjust to the work habits of several partners. The difference between the commonality achieved with and without pair splitting crucially depends on the underlying structure of the problem. For trees, we show that the value of the maximum commonality is the same for both MCAPn and MCAPs. Additionally, we obtain polynomial-time algorithms for both of these variants. For general graphs, both problems MCAPn and MCAPs are shown to be strongly NP-complete. We prove that the maximum commonality for MCAPs is at most 3 2 times the maximum commonality of MCAPn. We also provide polynomial-time algorithms and approximation results for a number of special cases of these problems.</description><author>Dawande, Milind; Kumar, Subodha; Mookerjee, Vijay; Sriskandarajah, Chelliah</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risk aversion in cumulative prospect theory</title><link>http://www.example.com/articles/1</link><description>Schmidt, Ulrich; Zank, Horst
This paper characterizes the conditions for strong risk aversion and second-order stochastic dominance for cumulative prospect theory. Strong risk aversion implies a convex weighting function for gains and a concave one for losses. It does not necessarily imply a concave utility function. The latter does follow if the weighting functions are continuous. By investigating the exact relationship between loss aversion and strong risk aversion, a natural index for the degree of loss aversion is derived.</description><author>Schmidt, Ulrich; Zank, Horst</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Should consumers use the halo to form product evaluations?</title><link>http://www.example.com/articles/1</link><description>Boatwright, Peter; Kalra, Ajay; Zhang, Wei
In purchase situations where attribute information is either missing or difficult to judge, a well-known heuristic that consumers use to form evaluations is the halo effect. The psychology literature has widely considered the halo a reflection of consumers' inability to discriminate between different attributes and have therefore labeled it the "halo error" or the "logical error." The objective of this paper is to offer a rationale for the halo effect. We use a decision-theory framework to show that the halo is consistent with the goal of minimizing estimation risk. Contrary to conventional wisdom, we demonstrate that a decision using the halo has lower estimation risk compared to not using the halo heuristic. Therefore, using the halo results in utility maximization and is indicative of rational behavior.</description><author>Boatwright, Peter; Kalra, Ajay; Zhang, Wei</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of information technology investments and diversification strategies on firm performance</title><link>http://www.example.com/articles/1</link><description>Chari, Murali D. R.; Devaraj, Sarv; David, Parthiban
As companies continue to make large investments in information technology ( IT), questions about how and in what contexts such investments pay off have gained importance. We develop a theoretical framework to explain how IT investments could pay off in the economically significant context of corporate diversification, and empirically find that the performance pay off to IT investments is greater for firms with greater levels of diversification. We also find that the performance payoff to IT investments is greater in related diversification than in unrelated diversification.</description><author>Chari, Murali D. R.; Devaraj, Sarv; David, Parthiban</author><pubDate>Tue, 01 Jan 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call center outsourcing contract analysis and choice</title><link>http://www.example.com/articles/1</link><description>Aksin, O. Zeynep; de Vericourt, Francis; Karaesmen, Fikri
This paper considers a call center outsourcing contract analysis and choice problem faced by a contractor and a service provider. The service provider receives an uncertain call volume over multiple periods and is considering outsourcing all or part of these calls to a contractor. Each call brings in a fixed revenue to the service provider. Answering calls requires having service capacity; thus implicit in the outsourcing decision is a capacity decision. Insufficient capacity implies that calls cannot be answered, which in turn means there will be a revenue loss. Faced, with a choice between a volume-based and a capacity-based contract offered by a contractor that has pricing power, the service provider determines optimal capacity levels. The optimal price and capacity of the contractor together with the optimal capacity of the service provider determine optimal profits of each party under the two contracts being considered. This paper characterizes optimal capacity levels and partially characterizes optimal pricing decisions under each contract. The impact of demand variability and the economic parameters on contract choice are explored through numerical examples. It is shown that no contract type is universally preferred and that operating environments as well as cost-revenue structures have an important effect.</description><author>Aksin, O. Zeynep; de Vericourt, Francis; Karaesmen, Fikri</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An exact and efficient algorithm for the constrained dynamic operator staffing problem for call centers</title><link>http://www.example.com/articles/1</link><description>Bhandari, Atul; Scheller-Wolf, Alan; Harchol-Balter, Mor
Call center managers are facing increasing pressure to reduce costs while maintaining acceptable service quality Consequently, they often face constrained stochastic optimization problems, minimizing cost subject to service-level constraints. Complicating this problem is the fact that customer-arrival rates to call centers are often time varying. Thus, to satisfy their service goals in a cost-effective manner, call centers may employ permanent operators who always provide service, and temporary operators who provide service only when the call center is busy, i.e., when the number of customers in system increases beyond a threshold level. This provides flexibility to dynamically adjust the number of operators providing service in response to the timevarying arrival rate. The constrained dynamic operator staffing (CDOS) problem involves determining the number of permanent and temporary operators, and the threshold value(s) that minimize time-average hiring and opportunity costs subject to service-level constraints. We model the CDCS problem as a constrained Markov decision process (MDP) and seek the optimal nonrandomized policy. The only exact method in the literature to obtain the optimal nonrandomized policy for a constrained MDP is enumeration, which is often computationally prohibitive. We provide a novel exact and efficient solution method, the modified balance equations disjunctive constraints (MBEDC) algorithm, yielding a mixed-integer program formulation; the computation times of this algorithm for sample problems are lower than enumeration by up to a factor of 200, and by a factor of 10 on average. Using our algorithm, we quickly solve diverse instances of the CDOS problem, generating managerial insights. into the effects of temporary operators and service-level constraints.</description><author>Bhandari, Atul; Scheller-Wolf, Alan; Harchol-Balter, Mor</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Staffing of time-varying queues to achieve time-stable performance</title><link>http://www.example.com/articles/1</link><description>Feldman, Zohar; Mandelbaum, Avishai; Massey, William A.; Whitt, Ward
This paper develops methods to determine appropriate staffing levels in call centers and other many-server queueing systems with time-varying arrival rates. The goal is to achieve targeted time-stable performance, even in the presence of significant time variation in the arrival rates. The main contribution is a flexible simulation-based iterative-staffing algorithm (ISA) for the M-t/G/s(t) + G model-with nonhomogeneous Poisson arrival process (the M,) and customer abandonment (the +G). For Markovian M-t/M/s(t) + M special cases, the ISA is shown to converge. For that M-t/M/s(t) + M model; simulation experiments show that the ISA yields time-stable delay probabilities across a wide range of target delay probabilities. With ISA, other performance measures-such as agent utilizations, abandonment probabilities, and average waiting times-are stable as well. The ISA staffing and performance agree closely with the modified-offered-load. approximation, which was previously shown to be an effective staffing algorithm without customer abandonment. Although the ISA algorithm so far has only been extensively tested for M-t/M/s(t) + M models, it can be applied much more generally-to M-t/G/s(t) + G models and beyond.</description><author>Feldman, Zohar; Mandelbaum, Avishai; Massey, William A.; Whitt, Ward</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Staffing multiskill call centers via linear programming and simulation</title><link>http://www.example.com/articles/1</link><description>Cezik, Mehmet Tolga; L'Ecuyer, Pierre
We study an iterative cutting-plane algorithm on an integer program for minimizing the staffing costs of a multiskill call center subject to service-level requirements that are estimated by simulation. We solve a sample average version of the problem, where the service levels are expressed as functions of the staffing for a fixed sequence of random numbers driving the simulation. An optimal solution of this sample problem is also an optimal solution to the original problem when the sample size is large enough, Several difficulties are encountered when solving the sample problem, especially for large problem instances, and we propose practical heuristics to deal with these difficulties. We report numerical experiments with examples of different sizes. The largest example corresponds to a real-life call center with 65 types of calls and 89 types of agents (skill groups).</description><author>Cezik, Mehmet Tolga; L'Ecuyer, Pierre</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimizing call center staffing using simulation and analytic center cutting-plane methods</title><link>http://www.example.com/articles/1</link><description>Atlason, Julius; Epelman, Marina A.; Henderson, Shane G.
We consider the problem of minimizing staffing costs in an inbound call center, while maintaining an acceptable level of service in multiple time periods. The problem is complicated by the fact that staffing level in one time period can affect the service levels in. subsequent periods. Moreover, staff schedules typically take the form of shifts covering several periods. Interactions between staffing levels in different time periods, as well as the impact of shift requirements on the staffing levels and cost, should be considered in the planning. Traditional staffing methods based on stationary queueing formulas do not take this into account. We present a simulation-based analytic center cutting-plane method to solve a sample average approximation of the problem. We establish convergence of the method when the service-level functions are discrete pseudoconcave. An extensive numerical study of a moderately large call center shows that the method is robust and, in most of the test cases, outperforms traditional staffing heuristics that are based on analytical queueing methods.</description><author>Atlason, Julius; Epelman, Marina A.; Henderson, Shane G.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service-level differentiation in call centers with fully flexible servers</title><link>http://www.example.com/articles/1</link><description>Gurvich, Itay; Armony, Mor; Mandelbaum, Avishai
We study large-scale service systems with multiple customer classes and many statistically identical servers. The following question is addressed: How many servers are required (staffing) and how does one match them with customers (control) to minimize staffing cost, subject to class-level quality-of-service constraints? We tackle this question by characterizing scheduling and staffing schemes that are asymptotically optimal in the limit, as system load grows to infinity. The asymptotic regimes considered are consistent with the efficiency-driven (ED), quality-driven (QD), and quality-and-efficiency-driven (QED) regimes, first introduced in the context of a single-class service system. Our main findings are as follows: (a) Decoupling of staffing and control, namely, (i) staffing disregards the multiclass nature of the system and is analogous to the staffing of a single-class system with the same aggregate demand and a single global quality-of-service constraint, and (ii) class-level service differentiation is obtained by using a simple idle-server-based threshold-priority (ITP) control (with state-independent thresholds); and (b) robustness of the staffing and control rules: our proposed single-class staffing (SCS) rule and ITP control are approximately optimal under various problem formulations and model assumptions. Particularly, although our solution is shown to be asymptotically optimal for large systems, we numerically demonstrate that it performs well also for relatively small systems.</description><author>Gurvich, Itay; Armony, Mor; Mandelbaum, Avishai</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling and analysis of call center arrival data: A Bayesian approach</title><link>http://www.example.com/articles/1</link><description>Soyer, Refik; Tarimcilar, M. Murat
In this paper, we present a modulated Poisson process model to describe and analyze arrival data to a call center. The attractive feature of this model is that it takes into account both covariate and time effects on the call volume intensity, and in so doing, enables us to assess the effectiveness of different advertising strategies along with predicting the arrival patterns. A Bayesian analysis of the model is developed and an extension of the model is presented to describe potential heterogeneity in arrival patterns. The proposed model and the methodology are implemented using real call center arrival data.</description><author>Soyer, Refik; Tarimcilar, M. Murat</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comparison of univariate time series methods for forecasting intraday arrivals at a call center</title><link>http://www.example.com/articles/1</link><description>Taylor, James W.
Predictions of call center arrivals are a key input to staff scheduling models. It is, therefore, surprising that simplistic forecasting methods dominate practice, and that the research literature on forecasting arrivals is so small. In this paper, we evaluate univariate time series methods for forecasting intraday arrivals for lead times from one half-hour ahead to two weeks ahead. We analyze five series of intraday arrivals for call centers operated by a retail bank in the United Kingdom. A notable feature of these series is the presence of both an intraweek and an intraday seasonal cycle. The methods considered include seasonal autoregressive integrated moving average (ARIMA) modeling; periodic autoregressive modeling; an extension of Holt-Winters exponential smoothing for the case of two seasonal cycles; robust exponential smoothing based on exponentially weighted least absolute deviations regression; and dynamic harmonic regression, which is a form of unobserved component state-space modeling. Our results indicate strong potential for the use of seasonal ARIMA modeling and the extension of Holt-Winters for predicting up to about two to three days ahead and that, for longer lead times, a simplistic historical average is difficult to beat. We find a similar ranking of methods for call center data from an Israeli bank.</description><author>Taylor, James W.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service-level agreements in call centers: Perils and prescriptions</title><link>http://www.example.com/articles/1</link><description>Milner, Joseph M.; Olsen, Tava Lennon
A call center with both contract and noncontract customers was giving priority to the contract customers only in off-peak hours, precisely when having priority was least important. In this paper, we investigate whether this is rational behavior on the part of the call center and what the implications are for customers. In particular, we show that under contracts on the percentile of delay, which are commonly used in the call center industry, this is rational behavior, at least under the approximating asymptotic regime considered in this paper. We then suggest other contracts that do not result in this type of undesirable behavior from a contract customer's perspective. We compare the performance of the different contracts in terms of mean, variance, and outer percentiles of delay for both customer types using both numerical and asymptotic heavy-traffic analyses. We argue that including terms reflecting the second moment of delay in a contract would be beneficial to contract customers and, in a sense, fairer.</description><author>Milner, Joseph M.; Olsen, Tava Lennon</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>