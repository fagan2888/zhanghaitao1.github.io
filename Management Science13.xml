<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Management Science13</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:07:42 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An empirical analysis of forecast sharing in the semiconductor equipment supply chain</title><link>http://www.example.com/articles/1</link><description>Terwiesch, C.; Ren, Z. J.; Ho, T. H.; Cohen, M. A.
We study the demand forecast-sharing process between a buyer of customized production equipment and a set of equipment suppliers. Based on a large data collection we undertook in the semiconductor equipment supply chain, we empirically investigate the relationship between the buyer's forecasting behavior and the supplier's delivery performance. The buyer's forecasting behavior is characterized by the frequency and magnitude of forecast revisions it requests (forecast volatility) as well as by the fraction of orders that were forecasted but never actually purchased (forecast inflation). The supplier's delivery performance is measured by its ability to meet delivery dates requested by the customers. Based on a duration analysis, we are able to show that suppliers penalize buyers for unreliable forecasts by providing lower service levels. Vice versa, we also show that buyers penalize suppliers that have a history of poor service by providing them with overly inflated forecasts.</description><author>Terwiesch, C.; Ren, Z. J.; Ho, T. H.; Cohen, M. A.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An experimental investigation of the impact of information on competitive decision making</title><link>http://www.example.com/articles/1</link><description>Abramson, C.; Currim, L. S.; Sarin, R.
Managers often employ market response models as decision aids and historical information of competitors' market outcomes to aid their competitive decisions in oligopolistic settings. However, little is known about how access to a decision aid or the availability of competitors' market outcomes impact a firm's competitive decisions (e.g., prices) or market outcomes resulting from those decisions (e.g., profits), or how managers make these decisions across such informational conditions. Hence, the objective of this paper is twofold. First, we investigate whether access to a decision aid and historical information of competitors' outcomes yields more-or less-competitive decisions and outcomes. Second, we determine which learning constructs, such as choice reinforcement and beliefs about projected profits, best explain competitive actions across various information conditions. We find that relative to the availability of competitive information, access to a decision aid has a larger effect on lowering prices and profits. We also find that in two-firm markets, price competition is even more intense than in five-firm markets. Similarly, the availability of market share information leads to more aggressive pricing even when profits are held constant. Finally, we outline the implications of our findings in making managerial resource allocations to market research endeavors.</description><author>Abramson, C.; Currim, L. S.; Sarin, R.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A branch-and-price algorithm and new test problems for spectrum auctions</title><link>http://www.example.com/articles/1</link><description>Gunluk, O.; Ladanyi, L.; de Vries, S.
W hen combinatorial bidding is permitted in auctions, such as the proposed FCC Auction #31, the resulting full valuations and winner-determination problem can be computationally challenging. We present a branch-and-price algorithm based on a set-packing formulation originally proposed by Dietrich and Forrest (2002, "A column generation approach for combinatorial auctions," in Mathematics of the Internet: E-Auction and Markets. The IMA, Volumes in Mathematics and Its Applications Vol. 127, Springer-Verlag, New York, 15-26). This formulation has a variable for, every possible combination of winning bids for. each bidder. Our algorithm exploits the structure of the XOR-of-OR bidding language used by the FCC. We also present a new methodology to produce realistic test problems based on the round-by-round results of FCC Auction #4. We generate 2,639 test problems, which involve 99 items and are substantially larger than most of the previously used benchmark problems. Because there are no real-life test problems for combinatorial spectrum auctions with the XOR-of-OR language, we used these test problems to observe the computational behavior of our algorithm. Our algorithm can solve all but one test problem within 10 minutes, appears to be very robust, and for difficult instances compares favorably to the natural formulation solved using a commercial optimization package with default settings. Although spectrum auctions are used as the guiding example to S describe the merits of branch and price for combinatorial. auctions, our approach applies to auctions of multiple goods in other scenarios similarly.</description><author>Gunluk, O.; Ladanyi, L.; de Vries, S.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decentralized supply chains with competing retailers under demand uncertainty</title><link>http://www.example.com/articles/1</link><description>Bernstein, F.; Federgruen, A.
In this paper, we investigate the equilibrium behavior of decentralized supply chains with competing retailers under demand uncertainty. We also design contractual arrangements between the parties that allow the decentralized chain to perform as well as a centralized one. We address these questions in the context of two-echelon supply chains with a single supplier servicing a network of (competing) retailers, considering the following general model: Retailers face random demands, the distribution of which may depend only on its own retail price (noncompeting retailers) or on its own price as well as those of the other retailers (competing retailers), according to general stochastic demand functions.</description><author>Bernstein, F.; Federgruen, A.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sell the plant? The impact of contract manufacturing on innovation, capacity, and profitability</title><link>http://www.example.com/articles/1</link><description>Plambeck, E. L.; Taylor, T. A.
In the electronics industry and others, original equipment manufacturers (OEMs) are selling their production facilities to contract manufacturers (CMs). The CMs achieve high capacity utilization through pooling (supplying many different OEMs). Meanwhile, the OEMs focus on innovation: research and development, product design, and marketing. We examine how this change in industry structure affects investment in innovation and capacity, and thus profitability. In particular, innovation is noncontractible, so OEMs will invest less in innovation than is ideal for the industry as a whole. Hence, although contract manufacturing improves capacity utilization, it may reduce the profitability of the industry as a whole by weakening the incentives for innovation. Contract manufacturing is not the only means to achieve capacity pooling. Alternatively, the OEMs can pool capacity with one another through supply contracts or a joint venture. This may result in underinvestment or overinvestment in innovation and capacity, but always increases profitability. We find that the sale of production facilities to a CM improves profitability for the industry as a whole if and only if OEMs are subsequently in a strong bargaining position vis-a-vis the CM. If the OEMs are indeed very strong, the gain from pooling capacity via contract manufacturing is maximized in industries with moderate cost of capacity.</description><author>Plambeck, E. L.; Taylor, T. A.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Agency costs in a supply chain with demand uncertainty and price competition</title><link>http://www.example.com/articles/1</link><description>Narayanan, V. G.; Raman, A.; Singh, J.
In this paper, we model a manufacturer that contracts with two retailers, who then choose retail prices and stocking quantities endogenously in a Bayesian Nash equilibrium. If the manufacturer designs a contract that is accepted by both retailers, it sets the wholesale price as a compromise between two conflicting roles: reducing intrabrand retail price competition and inducing retailers to stock closer to first-best levels (that is, optimum for the supply chain as a whole). In equilibrium, fill rates are less than first best. If, on the other hand, the manufacturer eliminates retail competition by designing a contract accepted by only one retailer, the assignment of consumers to retailers is inefficient. In either equilibrium, the performance of the supply chain is strictly less than first best. However, the manufacturer achieves first-best retail prices and fill rates if it can subsidize the retailers' leftover inventory. Absent such subsidies, the two-retailer equilibrium arises when the two retailers compete less intensively. In that equilibrium, numerical results indicate that the value of subsidizing unsold inventory is increasing in demand uncertainty, intensity of retail competition, and salvage value of inventory, and is decreasing in manufacturing cost and opportunity cost of shelf space.</description><author>Narayanan, V. G.; Raman, A.; Singh, J.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A principal-agent model for product specification and production</title><link>http://www.example.com/articles/1</link><description>Iyer, A. V.; Schwarz, L. B.; Zenios, S. A.
This paper develops and analyzes a principal-agent model for product specification and production motivated by "core buying" decisions at an automobile manufacturer. The model focuses on two important elements of the "core" buyer's responsibility: (1) assessing the supplier's capability, and (2) allocating some or all of a fixed level of some buyer-internal resource to help the supplier. Under the contracting scheme we model, the buyer (principal) delegates the majority of product specification and production activity to the supplier (agent), but retains the flexibility to commit a given, observable amount of an internally available, limited resource (e.g., engineering hours) to help the supplier. The supplier, in turn, allocates his resource (e.g., engineering hours) to produce the finished product. As in the motivating scenario, both the supplier's resource allocation and capability are assumed to be hidden from the buyer. Hence, the principal's problem is to determine a menu of (resource-commitment, transfer-price) contracts to minimize her total expected cost. Our analysis demonstrates that if buyer resource and supplier capability are substitutes, then the buyer's second-best involvement in the supplier's production process will be greater than first-best. The opposite is true if they are complements. Further, when the opportunity cost for the buyer's resource is zero, then in the substitutes case the buyer will commit all of its resource, while in the complements case the buyer may withhold some resources to screen the supplier type. We describe two applications of the model-one in inventory management and one in pharmaceutical drug discovery-to illustrate its applicability and versatility. Finally, we use insights from the model to suggest hypotheses for empirical study.</description><author>Iyer, A. V.; Schwarz, L. B.; Zenios, S. A.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Formation of alliances in Internet-based supply exchanges</title><link>http://www.example.com/articles/1</link><description>Granot, D.; Sosic, G.
In different industries, such as automobiles, chemicals, or retailing, competitors are joining forces in establishing electronic marketplaces to reduce inefficiencies in the purchasing process and cut costs by combining their buying power. Joining such an alliance leads to reduced costs, including those of possible rivals, because members share the development and operating costs. A company that joins an alliance agrees to share its suppliers with others, which may lead to more intense competition among the increased number of suppliers, and it may further benefit an alliance member at the expense of companies left outside the alliance. Natural questions that could arise, then, are when would a firm prefer to take part in an electronic marketplace joint venture; when would it prefer that other firms, possibly rivals, join the venture; and what are the financial consequences of either joining an alliance or remaining independent? In an attempt to gain a better understanding of the issues, we have developed a model of three retailers whose products may have a certain degree of substitutability. We provide some conditions, in terms of product substitutability and compatibility of retailers, that would lead to the formation of a three-member alliance, or a two-member alliance, or no alliance at all. We also study the effect of alliance structure and compatibility of retailers on the profit of a company.</description><author>Granot, D.; Sosic, G.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A smart market for industrial procurement with capacity constraints</title><link>http://www.example.com/articles/1</link><description>Gallien, J.; Wein, L. M.
We address the problem of designing multi-item procurement auctions for a monopsonistic buyer in capacity-constrained environments. Using insights from classical auction theory, we construct an optimization-based auction mechanism ("smart market") relying on the dynamic resolution of a linear program minimizing the buyer's cost under the suppliers' capacity constraints. Suppliers can modify their offers in response to the optimal allocation corresponding to each set of bids, giving rise to a dynamic competitive bidding process. To assist suppliers, we also develop a bidding-suggestion device based on a myopic best-response (MBR) calculation that solves an associated optimization problem. Assuming linear costs for the suppliers, we study within a game-theoretic framework the sequence of bids arising in this smart market. Under a weak behavioral assumption and some symmetry requirements, an explicit upper bound for the winning bids is established. We then formulate a complete behavioral model and solution methodology based on the MBR rationale and show that the bounds derived earlier continue to hold. We analytically derive some structural and convergence properties of the MBR dynamics in the simplest nontrivial market environment, which suggests further possible design improvements, and investigate bidding dynamics and incentive compatibility issues via numerical simulations.</description><author>Gallien, J.; Wein, L. M.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Salesforce incentives, market information, and production/inventory planning</title><link>http://www.example.com/articles/1</link><description>Chen, F. R.
Salespeople are the eyes and ears of the firms they serve. They possess market knowledge that is critical for a wide range of decisions. A key question is how a firm can provide incentives to its salesforce so that it is in their interest to truthfully disclose their information about the market and to work hard. Many people have considered this question and provided solutions. Perhaps the most well-known solution is due to Gonik (1978), who proposed and implemented a clever scheme designed to elicit market information and encourage hard work. The purpose of this paper is to study Gonik's scheme and compare it with a menu of linear contracts - a solution often used in the agency literature - in a model where the market information possessed by the salesforce is important for the firm's production and inventory-planning decisions.</description><author>Chen, F. R.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition in multiechelon assembly supply chains</title><link>http://www.example.com/articles/1</link><description>Carr, S. M.; Karmarkar, U. S.
In this paper, we study competition in multiechelon supply chains with an assembly structure. Firms in the supply chain are grouped into homogenous sectors (nodes) that contain identical firms with identical production capabilities that all produce exactly one undifferentiated product (that may itself be a "kit" of components). Each sector may use several inputs to produce its product, and these inputs are supplied by different sectors. The production process within any sector is taken to be pure assembly in fixed proportions. The number of firms in each sector is known. The demand curve for the final product is assumed to be linear, as are production costs in all sectors. Competition is modeled via a "coordinated successive Cournot" model in which firms choose production quantities for their downstream market so as to maximize profits, given prices for all inputs and all complementary products. Production quantities for sectors supplying the same successor are coordinated through pricing mechanisms, so that complementary products are produced in the right proportions. Under these assumptions, equilibrium prices for any multiechelon assembly network are characterized by a system of linear equations. We derive closed-form expressions for equilibrium quantities and prices in any two-stage system (i.e., a system with multiple input sectors and a single assembly sector). We show that any assembly structure can be converted to an equivalent (larger) structure in which no more than two components are assembled at any node. Finally, large structures can be solved either by direct solution of the characteristic linear equations or through an iterative reduction (compression) to smaller structures.</description><author>Carr, S. M.; Karmarkar, U. S.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain coordination with revenue-sharing contracts: Strengths and limitations</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.; Lariviere, M. A.
Under a revenue-sharing contract, a retailer pays a supplier a wholesale price for each unit purchased, plus a percentage of the revenue the retailer generates. Such contracts have become more prevalent in the videocassette rental industry relative to the more conventional wholesale price contract. This paper studies revenue-sharing contracts in a general supply chain model with revenues determined by each retailer's purchase quantity and price. Demand can be deterministic or stochastic and revenue is generated either from rentals or outright sales. Our model includes the case of a supplier selling to a classical fixed-price newsvendor or a price-setting newsvendor. We demonstrate that revenue sharing coordinates a supply chain with a single retailer (i.e., the retailer chooses optimal price and quantity) and arbitrarily allocates the supply chain's profit. We compare revenue sharing to a number of other supply chain contracts (e.g., buy-back contracts, price-discount contracts, quantity-flexibility contracts, sales-rebate contracts, franchise contracts, and quantity discounts). We find that revenue sharing is equivalent to buybacks in the newsvendor case and equivalent to price discounts in the price-setting newsvendor case. Revenue sharing also coordinates a supply chain with retailers competing in quantities, e.g., Cournot competitors or competing newsvendors with fixed prices. Despite its numerous merits, we identify several limitations of revenue sharing to (at least partially) explain why it is not prevalent in all industries. In particular, we characterize cases in which revenue sharing provides only a small improvement over the administratively cheaper wholesale price contract. Additionally, revenue sharing does not coordinate a supply chain with demand that depends on costly retail effort. We develop a variation on revenue sharing for this setting.</description><author>Cachon, G. P.; Lariviere, M. A.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incentives between firms (and within)</title><link>http://www.example.com/articles/1</link><description>Gibbons, R.
This paper reviews the significant progress in "agency theory" (i.e., the economic theory of incentives) during the 1990s, with an eye toward applications to supply transactions. I emphasize six recent models, in three pairs: (1) new foundations for the theory of incentive contracts, (2) new directions in incentive theory, and (3) new applications to supply transactions. By reviewing these six models, I hope to establish three things. First, the theory of incentive contracts needed and received new foundations. Second, new directions in incentive theory teach us that incentive contracts are not the only source of incentives. Finally (and especially relevant to supply transactions), the integration decision is an instrument in the incentive problem.</description><author>Gibbons, R.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on incentives and coordination in operations management</title><link>http://www.example.com/articles/1</link><description>Chen, F. R.; Zenios, S. A.
nan</description><author>Chen, F. R.; Zenios, S. A.</author><pubDate>Sat, 01 Jan 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An econometric analysis of inventory turnover performance in retail services</title><link>http://www.example.com/articles/1</link><description>Gaur, V.; Fisher, M. L.; Raman, A.
Inventory turnover varies widely across retailers and over time. This variation undermines the usefulness of inventory turnover in performance analysis, benchmarking, and working capital management. We develop an empirical model using financial data for 311 publicly listed retail firms for the years 1987-2000 to investigate the correlation of inventory turnover with gross margin, capital intensity and sales surprise (the ratio of actual sales to expected sales for the year). The model explains 66.7% of the within-firm variation and 97.2% of the total variation (across and within firms) in inventory turnover. It yields an alternative metric of inventory productivity, adjusted inventory turnover, which empirically adjusts inventory turnover for changes in gross margin, capital intensity, and sales surprise, and can be applied in performance analysis and managerial decision making. We also compute time trends in inventory turnover and adjusted inventory turnover, and find that both have declined in retailing during the 1987-2000 period.</description><author>Gaur, V.; Fisher, M. L.; Raman, A.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing flexible capacity in a make-to-order environment</title><link>http://www.example.com/articles/1</link><description>Bish, E. K.; Muriel, A.; Biller, S.
Flexible capacity has been shown to be very effective to hedge against forecast errors at the investment stage. In a make-to-order environment, this flexibility can also be used to hedge against variability in customer orders in the short term. For that purpose, production levels must be adjusted each period to match current demands, to give priority to the higher margin product, or to satisfy the closest customer. However, this will result in swings in production, inducing larger order variability at upstream suppliers and significantly higher component inventory levels at the manufacturer. Through a stylized two-plant, two-product capacitated manufacturing setting, we show that the performance of the system depends heavily on the allocation mechanism used to assign products to the available capacity. Although managers would be inclined to give priority to higher-margin products or to satisfy customers from their closest production site, these practices lead to greater swings in production, result in higher operational costs, and may reduce profits.</description><author>Bish, E. K.; Muriel, A.; Biller, S.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Structural flexibility: A new perspective on the design of manufacturing and service operations</title><link>http://www.example.com/articles/1</link><description>Iravani, S. M.; Van Oyen, M. P.; Sims, K. T.
In this paper, we present a new perspective on flexibility in manufacturing and service operations by exploring a type of operational flexibility that we term "structural flexibility (SF)." We focus on strategic-level issues of how flexibility can be created by using multipurpose resources such as cross-trained labor, flexible machines, or flexible factories. The proposed structural flexibility method uses the structure of the capability pattern to generate indices that quantify the ability of a system to respond to variability in its environment. Simulations of serial and parallel queueing networks provide evidence that this index is useful in predicting the performance rank of alternative designs for implementing multifunctionality in the face of variability. The proposed methodology supports managerial insight into structural design of manufacturing and service systems at the strategic level.</description><author>Iravani, S. M.; Van Oyen, M. P.; Sims, K. T.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Low-revenue equilibria in simultaneous ascending-bid auctions</title><link>http://www.example.com/articles/1</link><description>Engelbrecht-Wiggans, R.; Kahn, C. M.
Low-revenue equilibria allow participants in an auction to obtain goods at prices lower than would prevail in a competitive market. These outcomes are generated as perfect equilibria of ascending-bid, multiunit auctions. We show that these equilibria are possible under mild conditions-conditions that are likely to prevail in many situations where auction formats have recently been adopted We argue that these equilibria could explain the low revenues of some recent auctions, and discuss potential remedies to eliminate low-revenue equilibria.</description><author>Engelbrecht-Wiggans, R.; Kahn, C. M.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Search and collusion in electronic markets</title><link>http://www.example.com/articles/1</link><description>Campbell, C.; Ray, G.; Muhanna, W. A.
We examine the impact of reduced search costs on prices of commodity products in electronic marketplaces. Conventionally, reduced consumer search costs may be expected to engender stronger price competition between firms, resulting in lower prices and improved consumer welfare. This notion was formalized in Stahl (1989, "Oligopolistic pricin with sequential consumer search," American Economic Review, Vol. 79, No. 4, pp. 700-712) in a model Of static firm competition. In this paper, we show that these standard welfare conclusions may be neutralized or reversed in a dynamic environment. We focus on self-enforcing collusion by firms and characterize the conditions under which collusive equilibria exist. We show that less costly consumer search can facilitate firms' abilities to collude, resulting in higher prices and reduced consumer welfare, even with imperfect or no monitoring by sellers of each other's prices. If the same technology that eases consumer search also allows,firms to monitor each other's prices more easily, then firms can more easily detect cheating on a collusive price arrangement, allowing an even greater scope for collusion. This raises antitrust concerns with respect to the electronic marketplace and suggests, that at least some of the anticipated competitive gains from electronic market systems may be difficult to realize.</description><author>Campbell, C.; Ray, G.; Muhanna, W. A.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competing across technology-differentiated channels: The impact of network externalities and switching costs</title><link>http://www.example.com/articles/1</link><description>Viswanathan, S.
Them from traditional channels. The interaction between firms operating across these differentiated channels involves interesting competitive dynamics that cannot be captured by isolated models of electronic markets. This paper develops a stylized spatial differentiation model to examine the impact of differences in channel flexibility, network externalities, and switching costs on competition between online, traditional, and hybrid firms. A basic model highlighting the moderating influence of the hybrid firm on both channels is extended to account for differential network externalities. and switching costs across the two channels. Our analysis indicates that while network effects as well as switching costs lead to the tipping of markets, such tipping occurs primarily due to the moderating effects of the competing channel. More importantly, with network effects an increased market share does not translate into higher profits. Contradictory to conventional wisdom, our results indicate that in a static market, consumers father than firms, benefit from increasing network externalities,, with competitive effects outweighing the surplus-extraction abilities of firms. Our results also highlight the importance of alternative revenue streams and provide insights for firms grappling with issues of channel choice as well as integration and divestiture.</description><author>Viswanathan, S.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient auction mechanisms for supply chain procurement</title><link>http://www.example.com/articles/1</link><description>Chen, R. R.; Roundy, R. O.; Zhang, R. Q.; Janakiraman, G. E.
We consider multiunit Vickrey auctions for procurement supply chain settings. This is the first paper that incorporates transportation costs into auctions in a complex supply network. We first introduce an auction mechanism that makes simultaneous production and transportation decisions so that. the total supply chain cost is minimized and induces truth telling from the suppliers. Numerical study shows that considerable supply chain cost savings can be achieved if production and transportation costs are considered simultaneously. However, the,buyer's payments in such-auctions can be high. We then develop a new Vickrey-type auction that incorporates the buyer's reservation price, function into quantity allocation and payment decision. As a result, the buyer has some control over his payments at the expense of introducing uncertainty in the quantity acquired in the, auction.</description><author>Chen, R. R.; Roundy, R. O.; Zhang, R. Q.; Janakiraman, G. E.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competitive options, supply contracting. and electronic markets</title><link>http://www.example.com/articles/1</link><description>Wu, D. J.; Kleindorfer, P. R.
This paper develops a framework for analyzing business-to-business (B2B) transactions and supply chain management based on integrating contract procurement markets with spot markets using capacity options and forwards. The framework is motivated by the emergence of B2B exchanges in several industrial sectors to facilitate such integrated contract and spot procurement. In the framework developed, a buyer and multiple sellers may either contract for delivery in advance (the "contracting" option) or they may buy and sell some or all of their input/output in a spot market. Contract pricing involves both a reservation fee per unit of capacity and an execution fee per unit of output if capacity is called. The key question addressed is the structure of the optimal portfolios of contracting and spot market transactions for the buyer and these sellers, and the pricing thereof in market equilibrium. Existence and structure of market equilibria are characterized for the associated competitive game between sellers with heterogeneous technologies, under the assumption that they know the buyer's demand function. This allows an explicit characterization of the price of capacity options and the value of managerial flexibility, as well as providing conditions under which B2B exchanges are efficient and sustainable.</description><author>Wu, D. J.; Kleindorfer, P. R.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Models for iterative multiattribute procurement auctions</title><link>http://www.example.com/articles/1</link><description>Parkes, D. C.; Kalagnanam, J.
Multiattribute auctions extend traditional auction settings to allow negotiation over nonprice attributes such as weight, color, and terms of delivery, in addition to price and promise to improve market efficiency in markets with configurable goods. This paper provides an iterative auction design for an important special case of the multiattribute allocation problem with special (preferential independent) additive structure on the buyer value and seller costs. Auction ADDITIVE&amp;DISCRETE provides a refined design for a price-based auction in. which the price feedback decomposes to an additive part with a price for each attribute and an aggregate part that appears as a price discount for each supplier. In addition, this design also has excellent information revelation properties that are validated through computational experiments. The auction terminates with an outcome of a modified Vickrey-Clarke-Groves mechanism. This paper also develops Auction NONLINEAR&amp;DISCRETE for the more general nonlinear case-a particularly simple design, that solves the general multiattribute allocation problem, but requires that the auctioneer maintains prices on bundles of attribute levels.</description><author>Parkes, D. C.; Kalagnanam, J.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A new and improved design for multiobject iterative auctions</title><link>http://www.example.com/articles/1</link><description>Kwasnica, A. M.; Ledyard, J. O.; Porter, D.; DeMartini, C.
In this paper we present a new improved design for multiobject auctions and report on the results of experimental test's of that design. We merge the better features of two extant but very different auction processes, the Simultaneous Multiple Round (SMR) design used by the FCC to auction the electromagnetic spectrum and the Adaptive User Selection Mechanism (AUSM) of Banks et al. (1989, "Allocating uncertain and unresponsive resources: An experimental approach," RAND Journal of Economics, Vol. 20, No. 1, pp. 1-25). Then, by adding one crucial new feature, we are able to create a new design, the Resource Allocation Design (RAD) auction process, which performs better than both. Our experiments demonstrate that the RAD auction achieves higher efficiencies, lower bidder losses, higher net revenues, and faster times to completion without increasing the complexity of a bidder's problem.</description><author>Kwasnica, A. M.; Ledyard, J. O.; Porter, D.; DeMartini, C.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Iterative combinatorial auctions with bidder-determined combinations</title><link>http://www.example.com/articles/1</link><description>Kwon, R. H.; Anandalingam, G.; Ungar, L. H.
In combinatorial auctions, multiple distinct items are sold simultaneously and a bidder may place a single bid on a set (package) of distinct items. The determination of packages for bidding is a nontrivial task, and existing efficient formats require that bidders know the set of packages and/or their valuations. In this paper, we extend an efficient ascending combinatorial auction mechanism to use approximate single-item pricing. The single-item prices in each round are derived from a linear program that is constructed to reflect the current allocation of packages. Introduction of approximate single-item prices allows for endogenous bid determination where bidders can discover packages that were not included in the original bid set. Due to nonconvexities, single-item prices may not exist that are exact marginal values. We show that the use of approximate single-item prices with endogenous bidding always produces allocations that are at least as efficient as those from bidding with a fixed set of packages based on package pricing. A network resource allocation example is given that illustrates the benefits of our endogenous bidding mechanism.</description><author>Kwon, R. H.; Anandalingam, G.; Ungar, L. H.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Assessing the resource base of Japanese and US auto producers: A Stochastic frontier production function approach</title><link>http://www.example.com/articles/1</link><description>Lieberman, M. B.; Dhawan, R.
T he "resource-based view of the firm" has become an important conceptual framework in strategic management but has been widely criticized for lack of an empirical base. To address this deficit, we utilize a new method for identifying interfirm differences in efficiency within the context of stochastic frontier production functions. Using data on Japanese and U.S. automobile manufacturers, we develop measures of resources and capabilities and test for linkages with firm performance. The results show the influence of manufacturing proficiency and scale economies at the firm and plant level. We apply the parameter estimates to account for Toyota's superior efficiency relative to other producers.</description><author>Lieberman, M. B.; Dhawan, R.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient production-distribution system design</title><link>http://www.example.com/articles/1</link><description>Elhedhli, S.; Goffin, J. L.
The production-distribution system design is an integral part of the general supply chain design. This paper proposes a novel solution methodology for this problem that is based on Lagrangean relaxation, interior-point methods, and branch and bound. Unlike classical approaches, Lagrangean relaxation is applied in a two-level hierarchy, branch and bound is based on a Lagrangean lower bound and column generation (branch and price), while interior-point methods are used within a cutting-plane context (analytic centre cuttingplane method-ACCPM). Numerical results demonstrate that the two-level approach outperforms the classical approach and provides a very sharp lower bound that is the (proven) optimal in most cases.</description><author>Elhedhli, S.; Goffin, J. L.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and allocation for quality-differentiated online services</title><link>http://www.example.com/articles/1</link><description>Bapna, R.; Goes, P.; Gupta, A.
We explore the problem of pricing and allocation of unique, one-time digital products in the form of data streams. We look at the short-term problem where the firm has a capacitated shared resource and multiple products or service levels. We formulate the allocatively efficient Generalized Vickrey Auction (GVA) for our setting and point out the computational challenges in determining the individual discriminatory transfer payments. We propose an alternative uniform-price, computationally efficient, revenue-maximizing knapsack formulation called the Multiple Vickrey Auction.(MVA). While not incentive compatible, the MVA mechanism achieves bounded posterior regret and can be solved in real time. It has the added benefit of realizing imputed commodity prices for the various services, a feature lacking in the discriminatory GVA approach. For service providers that are concerned about the incentive compatibility but want imputed service prices, we suggest a maximal MVA (mMVA) uniform-pricing scheme that trades off revenue maximization for allocative efficiency. For sake of completeness we discuss the properties of a first-price pay-your-bid scheme. While NP-hard and not incentive compatible, this formulation has the perceived benefit of cognitive simplicity on the parts of sellers and bidders.</description><author>Bapna, R.; Goes, P.; Gupta, A.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note - Price discrimination after the purchase: Rebates as state-dependent discounts</title><link>http://www.example.com/articles/1</link><description>Chen, Y. X.; Moorthy, S.; Zhang, Z. J.
Promotional tools such as rebates and coupons are usually seen as different ways of price discriminating among consumers. We focus on a different property of rebates: their ability to price discriminate within a consumer among her postpurchase states. Unlike price discrimination between consumers, this property is unique to rebates because, by design, they are redeemed after the purchase. (Coupons, by contrast, are redeemed with the purchase.) The consumer redeems the rebate only in postpurchase states in which her marginal utility of income is high. This selective redemption behavior provides an opportunity for the seller to "utility arbitrage," directing discounts to when they matter most, resulting in an increase in the consumer's up-front willingness to pay. In turn, this enables an increase in the regular price. Of course, rebates can still price discriminate among consumers. Indeed, their ability to deliver state-dependent discounts may enhance their overall price discrimination ability, as we show in an example comparing them to coupons.</description><author>Chen, Y. X.; Moorthy, S.; Zhang, Z. J.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Personalized pricing and quality differentiation</title><link>http://www.example.com/articles/1</link><description>Choudhary, V.; Ghose, A.; Mukhopadhyay, T.; Rajan, U.
We develop an analytical framework to investigate the competitive implications of personalized pricing (PP), whereby firms charge different prices to different consumers based on their willingness to pay We embed PP in a model of vertical product differentiation and show how it affects firms' choices over quality. We show that firms' optimal pricing strategies with PP may be non-monotonic in consumer valuations. When the PP firm has high quality; both firms raise their qualities relative to the uniform pricing case. Conversely, when the PP firm has low quality, both firms lower their qualities. Although many firms are trying to implement such pricing policies, we find that a higher-quality firm can actually be worse off with PP. While it is optimal for the firm adopting PP to increase product differentiation, the non-PP firm seeks to reduce differentiation by moving in closer in the quality space. While PP results in a wider market coverage, it also leads to aggravated price competition between firms. Because this entails a change in equilibrium qualities, the nature of the cost function determines whether firms gain or lose by implementing such PP policies. Despite the threat of first-degree price discrimination, we find that PP with competing firms can lead to an overall increase in consumer welfare.</description><author>Choudhary, V.; Ghose, A.; Mukhopadhyay, T.; Rajan, U.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Global production planning under exchange-rate uncertainty</title><link>http://www.example.com/articles/1</link><description>Kazaz, B.; Dada, M.; Moskowitz, H.
Motivated by an aggregate production-planning problem in an actual global manufacturing network, we, examine the impact of exchange-rate uncertainty on the choice of optimal production policies when the allocation decision can be deferred until the realization of exchange rates. This leads to the formulation of the problem as a two-stage recourse program whose optimal policy structure features two forms of flexibility denoted as operational hedging: (1) production hedging, where the firm deliberately produces less than the total demand; and (2) allocation hedging, where due to unfavorable exchange rates, some markets are not served despite having unused production. Our characterization of the optimal policy structure leads to an economic valuation of production and allocation hedging. We show that the prevalence of production hedging is moderated by the degree of correlation between exchange rates. A comprehensive examination under the following four generalized settings provides the depth, scope, and relevancy that our proposed operational hedges play to facilitate aggregate planning: (1) multiple periods, (2) demand uncertainty, (3) price setting or monopolistic pricing, and (4) price setting under demand uncertainty. We show that production and allocation hedging are robust for these generalizations and should be integrated into the overall aggregate planning strategy of a global manufacturing firm.</description><author>Kazaz, B.; Dada, M.; Moskowitz, H.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing diagnostic information</title><link>http://www.example.com/articles/1</link><description>Arora, A.; Fosfuri, A.
Diagnostic information allows an agent to predict the state of nature about the success of an investment project better than the prior. We analyze the optimal pricing scheme for selling diagnostic information to buyers with different, privately known, ex ante success probability Investment costs and returns of successful projects are assumed to be the same for all buyers. The value of diagnostic information is the difference in expected payoffs with and without it, and we show that the willingness to pay for diagnostic information is nonmonotonic in the ex ante success probability. When the information seller can offer only one quality level, and negative payments are not allowed, we find that the optimal menu of (linear) contracts is remarkably simple. A pure royalty is offered to buyers with low ex ante success probability and a pure fixed fee is offered to buyers with high ex ante success probability.</description><author>Arora, A.; Fosfuri, A.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Soybean inventory and forward curve dynamics</title><link>http://www.example.com/articles/1</link><description>Geman, H.; Nguyen, V. N.
We present two results concerning soybean prices. First, we exhibit a simple relationship between stocks and price volatility The observation of an increasing price volatility with decreasing inventory is often mentioned in the literature, but has so far been documented using a proxy for inventory (see Fama and French 1987, 1988; Litzenberger and Rabinowitz 1995). Instead, we reconstruct a yearly, quarterly, and monthly database of worldwide soybean inventories using aggregate data from the United States, Brazil, and Argentina. We show that under all time scales, price volatility is an increasing linear function of inverse inventory, which we term "scarcity." Second, we show how the addition of the factor scarcity in a state-variable approach to the dynamics of the term structure of soybean forward prices improves the quality of the fit. We document this property on a 25-year database of CBOT futures contracts and show that the superior accuracy also affects long-maturity futures contracts, an important property for the valuation of long-term origination contracts between producing countries and the agrifood industry.</description><author>Geman, H.; Nguyen, V. N.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The financial impact of ISO 9000 certification in the United States: An empirical analysis</title><link>http://www.example.com/articles/1</link><description>Corbett, C. J.; Montes-Sancho, M. J.; Kirsch, D. A.
The ISO 9000 series of quality management systems standards, introduced in 1986, has been adopted at over 560,000 locations worldwide. Anecdotal evidence suggests that firms can achieve internal benefits such as quality or productivity improvements or that certification can help firms maintain or increase their market share, or both. Others argue that the standard is too generic to cause performance improvement but can be seen as a signal of good management. In this paper, we track financial performance from 1987 to 1997 of all publicly traded ISO 9000 certified manufacturing firms in the United States with SIC codes 2000-3999, and test whether ISO 9000 certification leads to productivity improvements, market benefits, and improved financial performance. We employ event-study methods, matching each certified firm to a control group of one or more noncertified firms in the same industry with similar precertification size and/or return on assets. We find that firms' decision to seek their first ISO 9000 certification was indeed followed by significant abnormal improvements in financial performance, though the exact timing and magnitude of this effect depend on the specification of the control group. Three years after certification, the certified firms do display strongly significant abnormal performance under all control-group specifications. The degree to which the precise results vary across control-group specifications indicates that event studies should always include extensive sensitivity analysis, for instance matching by size and performance separately and jointly, using both single firms and portfolios as controls.</description><author>Corbett, C. J.; Montes-Sancho, M. J.; Kirsch, D. A.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incentives for efficient inventory management: The role of historical cost</title><link>http://www.example.com/articles/1</link><description>Baldenius, T.; Reichelstein, S.
This paper examines inventory management from an incentive perspective. We show that when a manager has private information about future attainable revenues, the residual income performance measure based on historical cost can achieve optimal (second-best) incentives with regard to managerial effort as well as production and sales decisions. The LIFO (last-in-first-out) inventory flow rule is shown to be preferable to the FIFO (first-in-first-out) rule for the purpose of aligning incentives. Our analysis also finds support for the lower-of-cost-or-market inventory-valuation rule in situations where the manager receives new information after the initial contracting stage.</description><author>Baldenius, T.; Reichelstein, S.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>What actually happened to the inventories of American companies between 1981 and 2000?</title><link>http://www.example.com/articles/1</link><description>Chen, H.; Frank, M. Z.; Wu, O. Q.
This paper examines the inventories of publicly traded American manufacturing companies between 1981 and 2000. The median of inventory holding periods were reduced from 96 days to 81 days. The average rate of inventory reduction is about 2% per year. The greatest reduction was found for work-in-process inventory, which declined by about 6% per year. Finished-goods inventories did not decline. Firms with abnormally high inventories have abnormally poor long-term stock returns. Firms with slightly lower than average inventories have good stock returns, but firms with the lowest inventories have only ordinary returns.</description><author>Chen, H.; Frank, M. Z.; Wu, O. Q.</author><pubDate>Fri, 01 Jul 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Entry by spinoffs</title><link>http://www.example.com/articles/1</link><description>Klepper, S.; Sleeper, S.
Entry by spinoffs from incumbent firms is investigated for the laser industry. A model in which spinoffs exploit knowledge from their parents is constructed to explain the market conditions conducive to spinoffs, the types of firms that spawn spinoffs, and the relationship of spinoffs to their parents. The model is tested using detailed data on all laser entrants from the start of the industry through 1994. Our findings support the basic premise of the model that spinoffs inherit knowledge from their parents that shapes their nature at birth. Implications of our findings for organizational behavior, business strategy, entry and industry evolution, and technological change are discussed.</description><author>Klepper, S.; Sleeper, S.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selling and leasing strategies for durable goods with complementary products</title><link>http://www.example.com/articles/1</link><description>Bhaskaran, S. R.; Gilbert, S. M.
It has been recognized that when a durable goods manufacturer sells its output, it has an incentive to produce at a rate that will drive down the market price of the product over time. Because anticipation of declining prices makes consumers less willing to invest in owning the durable good, selling can be self-defeating for the manufacturer. If the manufacturer instead leases the product, it can eliminate its own incentive to decrease the price over time, which allows it to extract larger rents from consumers. In this paper, we investigate how a durable goods manufacturer's choice between leasing and selling is affected by a complementary product that is produced by an independent firm. We show that a durable goods manufacturer that leases its product has an incentive to increase prices (by limiting the availability of the product) in response to the availability of a complement. Because this potential for opportunistic behavior discourages output of the complement, leasing can also be problematic. As a result, the durable goods manufacturer faces a trade-off between leasing, which commits the manufacturer to not overproduce, and selling, which commits it to not underproduce. Our contribution is to identify this trade-off and show how a durable goods manufacturer can use a combination of leasing and selling to balance its strategic commitment across both its own market as well as the complementary market.</description><author>Bhaskaran, S. R.; Gilbert, S. M.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quality implications of warranties in a supply chain</title><link>http://www.example.com/articles/1</link><description>Balachandran, K. R.; Radhakrishnan, S.
We examine a supply chain in which the final product consists of components made by a buyer and a supplier. In the single moral-hazard case, the buyer's quality is observable, whereas in the double moral-hazard case, the buyer's quality is not observable. The supplier's quality is not observable in both the single and double moral-hazard cases. In each case, we examine a warranty/penalty contract between the buyer and the supplier based on information from incoming inspection and external failures. When the warranty contract is based on information from external failures in the single moral-hazard case, the first-best quality is achieved, whereas in the double moral-hazard case, the first-best quality is achieved if the supplier is not held responsible for the buyer's defects. When the warranty contract is based on information from incoming inspection, the first-best is achieved in both the single and double moral-hazard cases, even when the incoming inspection does not identify all of the supplier's defectives. An analysis of whether the penalty on the supplier in each case meets a fairness criterion-that is, the penalty does not exceed the manufacturer's external failure cost-indicates that the fairness criterion is met by the warranty contract based on information from incoming inspection when the first-best incoming inspection is sufficiently high. However, if the first-best incoming inspection is low and the precision of pinpointing the supplier's responsibility for external failure is sufficiently high, the warranty contract based on external failures could satisfy the fairness criterion.</description><author>Balachandran, K. R.; Radhakrishnan, S.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory management for an assembly system with product or component returns</title><link>http://www.example.com/articles/1</link><description>DeCroix, G. A.; Zipkin, P. H.
This paper considers an inventory system with an assembly structure. In addition to uncertain customer demands, the system experiences uncertain returns from customers. Some of the components in the returned products can be recovered and reused, and these units are returned to inventory. Returns complicate the structure of the system, so that the standard approach (based on reduction to an equivalent series system) no longer applies in general. We identify conditions on the item-recovery pattern and restrictions on the inventory policy under which an equivalent series system does exist. For the special case where only the end product (or all items used to assemble the end product) is recovered, we show that the system is equivalent to a series system with no policy restrictions. For the general case, we explain how and why the system becomes more problematic and propose two heuristic policies, The heuristics are easy to compute and practical to implement, and they perform well in numerical trials. Based on these numerical trials, we obtain insights into the impact of various factors on system performance. For example, we find that holding and backorder costs tend to increase when the average return rate, the variability of returns, or the number of components recovered increases, However, neither the product architecture nor the specific set of components being recovered seems to have a significant impact on these costs. Whether product recovery reduces total system costs depends on the magnitude of the additional holding and backorder costs relative to potential procurement cost savings.</description><author>DeCroix, G. A.; Zipkin, P. H.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Allocating spending between advertising and information technology in electronic retailing</title><link>http://www.example.com/articles/1</link><description>Tan, Y.; Mookerjee, V. S.
This study examines coordination issues that occur in allocating spending between advertising and information technology (IT) in electronic retailing. Electronic retailers run the risk of overspending on advertising to attract customers but underspending on IT, thus resulting in inadequate processing capacity at the firm's website. In this paper, we present a centralized, joint marketing-IT model to optimally allocate spending between advertising and IT, and we discuss an uncoordinated case where marketing and IT make suboptimal advertising and capacity decisions. We show how these decisions can be coordinated either by reducing the value of a customer session or by designing an optimal processing contract between marketing and IT, Both the coordination methods can be implemented with only local knowledge of the IT function, yet they generate a solution that almost matches the quality of the centralized solution. We extend our basic model to consider demand uncertainty, lagged advertising effects, and uncertainties in the lead time to acquire IT capacity. With demand uncertainty, electronic retailers should reduce spending on advertising and increase IT capacity if there is potential for a demand upswing and the cost of IT capacity is relatively low. The value of a customer session should be further reduced when uncertainties exist. This is required to share the risk of excess or inadequate IT capacity.</description><author>Tan, Y.; Mookerjee, V. S.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theoretical approach to web design in E-commerce: A belief reinforcement model</title><link>http://www.example.com/articles/1</link><description>Song, J.; Zahedi, F.
Effective website design plays a critical role in attracting and maintaining customers' interest. Despite the importance of websites as the major and, at times, sole channel of communication in e-business, little theoretical knowledge is available about how websites may influence online shoppers' attitudes and behavior. In this paper, we develop a conceptual framework for measuring the impact of Web-design elements on the beliefs and behavior of Web customers. In developing the theoretical model (called the belief reinforcement model, or BRM), we synthesize the theory of planned behavior with theories in social psychology, consumer behavior, and management to categorize Web-design elements and conceptualize the salient aspects of Web shoppers' behavior. The empirical examination of BRM indicates that various categories of Web-design elements reinforce Web customers' beliefs, which in turn positively impact attitudinal constructs that lead to changes in their purchase intentions. BRM and its results provide an initial guideline for a rigorous approach to designing websites for e-business and testing their effectiveness before their full deployment.</description><author>Song, J.; Zahedi, F.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the value of flexibility in R&amp;D projects</title><link>http://www.example.com/articles/1</link><description>Santiago, L. P.; Vakili, P.
In this paper, we consider whether an increase in uncertainty increases the value of a research and development (R&amp;D) project, We also consider the related question of the impact of increased project uncertainty on the value of management flexibility, defined as the difference in value when the project is managed "actively" versus when it is under "passive" management. These questions have already been formulated in an insightful paper in the literature, where different sources of variability and uncertainty in R&amp;D projects are identified and abandonment and improvement at interim stages are considered as options that provide management flexibility. We follow the same formulation. We derive a set of negative results that are contrary to the results of the above-mentioned paper and a set of positive results that are different from those presented. Our negative results indicate that when the source of variability is development uncertainty or market requirement uncertainty, one cannot make a general statement about the impact of increased uncertainty. In some cases, the value of flexibility (and project value) increases and in others it decreases. On the other hand, if the source of variability is market payoff, we show that increased variability increases either the overall project value or the project option value. If the increased variability of market payoff increases the "passive" value of the project, the overall project value also increases; and if it decreases the "passive" value, the value of flexibility, i.e., the project option value, increases.</description><author>Santiago, L. P.; Vakili, P.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Market segmentation and product technology selection for remanufacturable products</title><link>http://www.example.com/articles/1</link><description>Debo, L. G.; Toktay, L. B.; Van Wassenhove, L. N.
Remanufacturing is a production strategy whose goal is to recover the residual value of used products. Used products can be remanufactured at a lower cost than the initial production cost, but consumers value remanufactured products less than new products. The choice of production technology influences the value that can be recovered from a used product. In this paper, we solve the joint pricing and production technology selection problem faced by a manufacturer that considers introducing a remanufacturable product in a market that consists of heterogeneous consumers. Our analysis discusses the market and technology drivers of product remanufacturability and identifies some phenomena of managerial importance that are typical of a remanufacturing environment.</description><author>Debo, L. G.; Toktay, L. B.; Van Wassenhove, L. N.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The making of a "hot product": A signaling explanation of marketers' scarcity strategy</title><link>http://www.example.com/articles/1</link><description>Stock, A.; Balachander, S.
Every marketer's dream is to create a "hot product" that customers would absolutely want to have, thus generating considerable profit to the marketer. According to one school of thought, marketers should make products hard to get in order to create really hot products. In this paper, using a game-theoretic model, we investigate if such scarcity strategies can indeed be optimal. While a scarcity strategy may appear to be a viable approach for making a firm's product successful, further analysis raises some puzzling issues. In particular, it is not clear why a firm would not increase its price to get demand and supply in sync and increase its profit in the process. We therefore offer a signaling explanation for the optimality of such strategies and show that a high-quality seller may optimally choose to make the product scarce in order to credibly signal the quality of its product to uninformed customers. Our analysis indicates that a high-quality seller optimally employs scarcity as a signaling device in product markets that are characterized, ceteris paribus, by a small difference in marginal cost between high- and low-quality products, a low reservation price for a low-quality product, a greater heterogeneity in reservation prices for a high-quality product, and a moderate number of informed consumers. Our results provide a rationale for the fact that scarcity strategies are usually observed for discretionary or specialty products, but not for commodity products, staple products, or new-to-the-world products.</description><author>Stock, A.; Balachander, S.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimizing the supply chain configuration for new products</title><link>http://www.example.com/articles/1</link><description>Graves, S. C.; Willems, S. P.
We address how to configure the supply chain for a new product for which the design has already been decided. The central question is to determine what suppliers, parts, processes, and transportation modes to select at each stage in the supply chain. There might be multiple options to supply a raw material, to manufacture or assemble the product, and to transport the product to the customer. Each of these options is differentiated by its lead time and direct cost added. Given these various choices along the supply chain, the configuration problem is to select the options that minimize the total supply chain cost. We develop a dynamic program with two state variables to solve the supply chain configuration problem for supply chains that are modeled as spanning trees. We illustrate the problem and its solution with an industrial example. We use the example to show the benefit from optimization relative to heuristics and to form hypotheses concerning the structure of optimal supply chain configurations. We conduct a computational experiment to test these hypotheses.</description><author>Graves, S. C.; Willems, S. P.</author><pubDate>Mon, 01 Aug 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note: Customer intimacy and cross-selling strategy</title><link>http://www.example.com/articles/1</link><description>Akcura, M. T.; Srinivasan, K.
Better targeting opportunities and the increasing role of information-intensive environments have created new challenges for firms in obtaining customer information. Such information can help firms increase their profits through cross-selling opportunities. However, revealing personal preferences and contact information can raise the risks for customers when dealing with a firm. Consequently, some customers trade off the benefit and risks of revealing information. As the opportunity to obtain a higher level of information increases, customers incur a higher level of risk when dealing with a firm. This increases the firm's incentive to commit on a cross-selling level. By such a commitment, a firm can obtain customer intimacy and benefit from detailed customer information. As a result, profits increase while prices decrease. Thus, legal regulations that explicitly require firms to spell out the extent of cross-selling may actually improve the profits of the firm.</description><author>Akcura, M. T.; Srinivasan, K.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management games: Horizontal and vertical competition</title><link>http://www.example.com/articles/1</link><description>Netessine, S.; Shumsky, R. A.
A well-studied problem in the literature on airline revenue (or yield) management is the optimal allocation of seat inventory among fare classes, given a demand distribution for each class. In practice, the seat allocation decisions of one airline affect the passenger demands for seats on other airlines. In this paper, we examine the seat inventory control problem under both horizontal competition (two airlines compete for passengers on the same flight leg) and vertical competition (different airlines fly different legs on a multileg itinerary). Such vertical competition can be the outcome of a code-sharing agreement between airlines, because each airline sells seats on the partner airlines' flights but the airlines are unwilling, or unable, to coordinate yield management decisions. We provide a general sufficient condition under which a pure-strategy Nash equilibrium exists in these revenue management games, and we also compare the total number of seats available in each fare class with, and without, competition. Analytical results as well as numerical examples demonstrate that more seats are protected for higher-fare passengers under horizontal competition than when a single airline acts as a monopoly. Under vertical competition the booking limit may be higher or lower, however, than the monopoly level, depending on the demand for connecting flights in each fare class. Finally, we discuss revenue-sharing contracts that coordinate the actions of both airlines.</description><author>Netessine, S.; Shumsky, R. A.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Beyond the endogeneity bias: The effect of unmeasured brand characteristics on household-level brand choice models</title><link>http://www.example.com/articles/1</link><description>Chintagunta, P.; Dube, J. P.; Goh, Y.
We investigate the role of potential weekly brand-specific characteristics that influence consumer choices, but are unobserved or unmeasurable by the researcher. We use an empirical approach, based on the estimation methods used for standard random coefficients logit models, to account for the presence of such unobserved attributes. Using household scanner panel data, we find evidence that ignoring such time-varying latent (to the researcher) characteristics can lead to two types of problems. First, consistent with previous literature, we find that these unobserved characteristics may lead to biased estimates of the mean price response parameters. This argument is based on a form of price endogeneity. If marketing managers set prices based on consumer willingness to pay, then the observed prices will likely be correlated with the latent (to the researcher) brand characteristics. We resolve this problem by using an instrumental variables procedure. Our findings suggest that simply ignoring these attributes may also lead to larger estimates of the variance in the heterogeneity distribution of preferences and price sensitivities across households. This could overstate the benefits from marketing activities such as household-level targeting. We resolve the problem by using weekly brand intercepts, embedded in a random coefficients brand choice model, to control for weekly brand-specific characteristics, while accounting for household heterogeneity. Overall, our results extend the finding on the endogeneity bias from the mean of the heterogeneity distribution (i.e., the price effect) to include the variance of that distribution.</description><author>Chintagunta, P.; Dube, J. P.; Goh, Y.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the timing of CEO stock option awards</title><link>http://www.example.com/articles/1</link><description>Lie, E.
This study documents that the abnormal stock returns are negative before unscheduled executive option awards and positive afterward. The return pattern has intensified over time, suggesting that executives have gradually become more effective at timing awards to their advantage, and possibly explaining why the results in this study differ from those in past studies. Moreover, I document that the predicted returns are abnormally low before the awards and abnormally high afterward. Unless executives possess an extraordinary ability to forecast the future marketwide movements that drive these predicted returns, the results suggest that at least some of the awards are timed retroactively.</description><author>Lie, E.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conservative accounting choices</title><link>http://www.example.com/articles/1</link><description>Bagnoli, M.; Watts, S. G.
Managers have sufficient discretion under generally accepted accounting principles (GAAP) to adopt more or less conservative financial reporting policies. In this paper, we develop a signaling model to provide insight into managers' decisions to be conservative in their accounting. We provide conditions under which the market can use the manager's exercise of discretion to infer her private information about the future prospects of the firm and thus firm value. Under these conditions, we also show that there are meaningful differences between earnings response coefficients for firms whose managers choose a conservative reporting policy and those whose managers do not. Finally, we use our theoretical model to provide intuition for some established empirical results on earnings response coefficients.</description><author>Bagnoli, M.; Watts, S. G.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Evolution of R&amp;D capabilities: The role of knowledge networks within a firm</title><link>http://www.example.com/articles/1</link><description>Nerkar, A.; Paruchuri, S.
In this paper, we suggest that the characteristics of individual positions in an intraorganizational network of inventors or intrafirm knowledge network predict the likelihood with which knowledge created by an inventor is used in the firm's research and development (R&amp;D) activities. Such choices lead to path dependence and subsequent specialization. We provide empirical evidence that a firm's R&amp;D is concentrated in those areas where it chooses to recombine knowledge, offering support for the path-dependent evolution of capabilities. We test this theory by analyzing the R&amp;D networks in DuPont, a highly regarded Fortune 500 chemical company. Cox Proportional Regression models of intrafirm citations on network characteristics offer strong empirical support for our theory.</description><author>Nerkar, A.; Paruchuri, S.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Collaborative networks as determinants of knowledge diffusion patterns</title><link>http://www.example.com/articles/1</link><description>Singh, J.
This paper examines whether interpersonal networks help explain two widely documented patterns of knowledge diffusion: (1) geographic localization of knowledge flows, and (2) concentration of knowledge flows within firm boundaries. I measure knowledge flows using patent citation data, and employ a novel regression framework based on choice-based sampling to estimate the probability of knowledge flow between inventors of any two patents. As expected, intraregional and intrafirm knowledge flows are found to be stronger than those across regional or firm boundaries. I explore whether these patterns can be explained by direct and indirect network ties among inventors, as inferred from past collaborations among them. The existence of a tie is found to be associated with a greater probability of knowledge flow, with the probability decreasing as the path length (geodesic) increases. Furthermore, the effect of regional or firm boundaries on knowledge flow decreases once interpersonal ties have been accounted for. In fact, being in the same region or firm is found to have little additional effect on the probability of knowledge flow among inventors who already have close network ties. The overall evidence is consistent with a view that interpersonal networks are important in determining observed patterns of knowledge diffusion.</description><author>Singh, J.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analyzing bioterror response logistics: The case of anthrax</title><link>http://www.example.com/articles/1</link><description>Craft, D. L.; Wein, L. M.; Wilkins, A. H.
To aid in understanding how best to respond to a bioterror anthrax attack, we analyze a system of differential equations that includes an atmospheric release model, a spatial array of biosensors, a dose-response model, a disease progression model, and a set of spatially distributed tandem queues for distributing antibiotics and providing hospital care. We derive approximate closed-form expressions for the number of deaths as a function of key parameters and management levers, including the size of the attack, the time at which the attack is detected via symptomatic patients, the number of days to distribute antibiotics, the efficacy (both for treatment and prevention) of antibiotics, the prophylactic antibiotic distribution strategy, the prioritization of the antibiotics queue, and the detection limit, deployment density and delay time of biosensors.</description><author>Craft, D. L.; Wein, L. M.; Wilkins, A. H.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Association between supply chain glitches and operating performance</title><link>http://www.example.com/articles/1</link><description>Hendricks, K. B.; Singhal, V. R.
This paper empirically documents the association between supply chain glitches and operating performance. The results are based on a sample of 885 glitches announced by publicly traded firms. Changes in various operating performance metrics for the sample firms are compared against a sample of control firms of similar size and from similar industries. In the year leading up to the announcement, the control-adjusted mean percent changes in operating income, return on sales, and return on assets for the sample firms are -107%, -114%, and -92%, respectively. During this same period, the control-adjusted changes in the level of return on sales and return on assets are -13.78% and -2.32%, respectively Relative to controls, firms that experience glitches report on average 6.92% lower sales growth, 10.66% higher growth in cost, and 13.88% higher growth in inventories. More importantly, firms do not quickly recover from the negative economic consequences of glitches. During the two-year time period after the glitch announcement, operating income, sales, total costs, and inventories do not improve. We also find that it does not matter who caused the glitch, what the reason was for the glitch, or what industry a firm belongs to-glitches are associated with negative operating performance across the board.</description><author>Hendricks, K. B.; Singhal, V. R.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Generating volatility forecasts from value at risk estimates</title><link>http://www.example.com/articles/1</link><description>Taylor, J. W.
Statistical volatility models rely on the assumption that the shape of the conditional distribution is fixed over time and that it is only the volatility that varies. The recently proposed conditional autoregressive value at risk (CAViaR) models require no such assumption, and allow quantiles to be modeled directly in an autoregressive framework. Although useful for risk management, CAViaR models do not provide volatility forecasts. Such forecasts are needed for several other important applications, such as option pricing and portfolio management. It has been found that, for a variety of probability distributions, there is a surprising constancy of the ratio of the standard deviation to the interval between symmetric quantiles in the tails of the distribution, such as the 0.025 and 0.975 quantiles. This result has been used in decision and risk analysis to provide an approximation of the standard deviation in terms of quantile estimates provided by experts. Drawing on the same result, we construct financial volatility forecasts as simple functions of the interval between CAViaR forecasts of symmetric quantiles. Forecast comparison, using five stock indices and 20 individual stocks, shows that the method is able to outperform generalized autoregressive conditional heteroskedasticity (GARCH) models and moving average methods.</description><author>Taylor, J. W.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Market for software vulnerabilities? Think again</title><link>http://www.example.com/articles/1</link><description>Kannan, K.; Telang, R.
Software vulnerability disclosure has become a critical area of concern for policyrnakers. Traditionally, a Computer Emergency Response Team (CERT) acts as an infomediary between benign identifiers (who voluntarily report vulnerability information) and software users. After verifying a reported vulnerability CERT sends out a public advisory so that users can safeguard their systems against potential exploits. Lately, firms such as Defense have been implementing a new market-based approach for vulnerability information. The market-based infomediary provides monetary rewards to identifiers for each vulnerability reported. The infomediary then shares this information with its client base. Using this information, clients protect themselves against potential attacks that exploit those specific vulnerabilities. The key question addressed in our paper is whether movement toward such a market-based mechanism for vulnerability disclosure leads to a better social outcome. Our analysis demonstrates that an active unregulated market-based mechanism for vulnerabilities almost always underperforms a passive CERT-type mechanism. This counterintuitive result is attributed to the market-based infomediary's incentive to leak the vulnerability information inappropriately. If a profit-maximizing firm is not allowed to (or chooses not to) leak vulnerability information, we find that social welfare improves. Even a regulated market-based mechanism performs better than a CERT-type one, but only under certain conditions. Finally, we extend our analysis and show that a proposed mechanism-federally funded social planner-always performs better than a market-based mechanism.</description><author>Kannan, K.; Telang, R.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A longitudinal model of continued IS use: An integrative view of four mechanisms underlying postadoption phenomena</title><link>http://www.example.com/articles/1</link><description>Kim, S. S.; Malhotra, N. K.
Although initial use is an important indicator of information system (IS) success, it does not necessarily lead to the desired managerial outcome unless the use continues. However, compared with the great amount of work done on IS adoption, little systematic effort has gone into providing insight into continued IS use over time. The objective of this study is to develop a longitudinal model of how users' evaluations and behavior evolve as they gain experience with the information technology application. The proposed model is a unified framework that sheds light on four different mechanisms underlying postadoption phenomena: (1) the processes suggested by the technology acceptance model; (2) sequential updating mechanisms; (3) feedback mechanisms; and (4) repeated behavioral patterns. The proposed model was empirically tested in the context of Web-based IS use in a nonexperimental setting. Our findings suggest that, as hypothesized, each of the four theoretical viewpoints is essential for a deeper understanding of continued IS use. We discuss important findings that emerged from this longitudinal study and suggest directions for additional research.</description><author>Kim, S. S.; Malhotra, N. K.</author><pubDate>Sun, 01 May 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Probabilistic inversion of expert judgments in the quantification of model uncertainty</title><link>http://www.example.com/articles/1</link><description>Kraan, B.; Bedford, T.
Expert judgment is, frequently used to assess parameter values of quantitative management science models, particularly in decision-making contexts. Experts can, however, only be expected to assess observable quantities, not abstract model parameters. This means that we need a method for translating expert assessed uncertainties on model outputs into uncertainties on model parameter values. This process is called probabilistic inversion. The probability distribution on model parameters obtained in this way can be used in a variety of ways, but in particular in an uncertainty analysis or as a Bayes prior. This paper discusses computational algorithms that have proven successful in various projects and gives examples from environmental modelling and banking. Those algorithms are given a theoretical basis by adopting a minimum information approach to modelling, partial information. The role of minimum information is two-fold: It enables us to resolve the problem, of nonuniqueness of distributions given the information we have, and it provides numerical stability to the algorithm by guaranteeing convergence properties.</description><author>Kraan, B.; Bedford, T.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Individual experience and experience working together: Predicting learning rates from knowing who knows what and knowing how to work together</title><link>http://www.example.com/articles/1</link><description>Reagans, R.; Argote, L.; Brooks, D.
Learning by doing represents an important mechanism through which organizations prosper. Some firms, however, learn from their experience at a dramatic rate, while other firms exhibit very little learning at all. Three factors have been identified that affect the rate at which firms learn: (a) the proficiency of individual workers, (b) the ability of firm members to leverage knowledge accumulated by others, and (c) the capacity for coordinated activity inside the organization. Each factor varies with a particular kind of experience. An increase in cumulative individual experience increases individual proficiency An increase in cumulative organizational experience provides individuals with the opportunity to benefit from knowledge accumulated by others. An increase in cumulative experience working together promotes more effective coordination and teamwork. To gain insight into factors responsible for the learning curve, we examine the contribution of each kind of experience to performance, while controlling for the impact of the other two. The study context is a teaching hospital. The task is a total joint replacement procedure, and the performance metric is procedure completion time. We find that each kind of experience makes a distinct contribution to team performance. We discuss the implications of our findings for the learning-by-doing framework in general, and learning in the team context in particular.</description><author>Reagans, R.; Argote, L.; Brooks, D.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Treatment errors in healthcare: A safety climate approach</title><link>http://www.example.com/articles/1</link><description>Naveh, E.; Katz-Navon, T.; Stern, Z.
Recent reports on patient safety in healthcare point to the high frequency of treatment errors. This study suggests a new theory of safety climate and brings empirical evidence that helps explain the occurrence of treatment errors. Four safety climate dimensions have been identified. They include employee perceptions of the suitability of the organization's safety procedures for their daily work, employee perceptions of the frequency and the clarity of the safety information distributed by the organization; the way employees interpret their managers' safety practices, and the perceived priority given to safety within the organization. The study was conducted in 21 medical units in a general hospital and the results were cross-validated in 15 units in another hospital. Results demonstrated that perceived suitable safety procedures and frequent and clear information flow reduced treatment errors only when managers practiced safety and through their influence on the level of priority given to safety within the unit. Implications for safety climate theory and for reducing the occurrence of treatment errors by safety interventions are discussed.</description><author>Naveh, E.; Katz-Navon, T.; Stern, Z.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note: A dynamic programming approach to customer relationship pricing</title><link>http://www.example.com/articles/1</link><description>Lewis, M.
The practice of offering discounts to prospective customers represents a rudimentary form of using transaction history measures to customize the marketing mix. Furthermore, the proliferation of powerful, customer relationship management (CRM) systems is providing the data and the communications channels necessary to extend this type of pricing strategy into true dynamic marketing policies that adjust pricing as customer relationships evolve. In this paper, we describe a dynamic programming-based approach to creating optimal relationship pricing policies. The methodology has two main components. The first component is a latent class logit model that is used to model customer buying behavior. The second component is a dynamic optimization procedure that computes profit-maximizing price paths. The methodology is illustrated using subscriber data provided by a large metropolitan newspaper. The empirical results provide support for the common managerial practice of offering discounts to new customers. However, in contrast to current practice, the results suggest the use of a series of decreasing discounts based on the length of customer tenure rather than a single steep discount for first-time purchasers. The dynamic programming (DP) methodology also represents an important approach to calculating customer value (CV). Specifically, the DP framework allows the calculation of CV to be an explicit function,of marketing policies and customer status. As such, this method for calculating CV accounts for the value of managerial flexibility and improves upon existing methods that do not model revenue and attrition rates as functions of marketing variables.</description><author>Lewis, M.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Order quantity and timing flexibility in supply chains: The role of demand characteristics</title><link>http://www.example.com/articles/1</link><description>Milner, J. M.; Kouvelis, P.
We study how differences in product demand characteristics affect the strategic value of different types of supply chain flexibility for accurate response. We propose a single-period inventory modelling framework, with two ordering opportunities. The second order reflects updated demand information and potentially capitalizes on supply chain flexibility. We consider two complementary forms of flexibility: quantity flexibility in production and timing flexibility in scheduling. In this framework, we analyze the total inventory cost of a firm for alternate demand types. We model functional products through the standard assumption of independent demand over the period,. fashion-driven innovative products through a Bayesian model, and innovative products with evolving demand through a Martingale process. The three demand processes exhibit very different behavior with respect to the value of the alternate forms of flexibility. We observe that quantity flexibility is of moderate value for functional goods and of high value for fashion-driven products for all lead times. Quantity flexibility is of low value for goods with evolving demand with long lead times but of high value for short lead times. Alternately, we observe timing flexibility is of highest value for functional goods, especially for cases of high holding cost, and is of lesser value for fashion-driven goods. It is of least value for goods with evolving demand. Both quantity and timing flexibility capabilities are required to significantly reduce the relevant supply chain costs for evolving-demand innovative goods when the lead times are long.</description><author>Milner, J. M.; Kouvelis, P.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information sharing in a supply chain under ARMA demand</title><link>http://www.example.com/articles/1</link><description>Gaur, V.; Giloni, A.; Seshadri, S.
In this paper we study how the time-series structure of the demand process affects the value of information sharing in A supply chain. We consider a two-stage supply chain model in which a retailer serves auto-regressive moving-average (ARMA) demand and a manufacturer fills the retailer's orders. We characterize three types of situations based on the parameters of the demand process: (i) the manufacturer benefits from inferring demand. information from the retailer's orders; (ii) the manufacturer cannot infer demand, but benefits from sharing demand information; and (iii) the manufacturer is better off neither inferring nor sharing, but instead uses only the most recent orders in its production planning. Using the example of ARMA(l,l) demand, we find that sharing or inferring retail demand leads to a 16.0% average reduction in the manufacturer's safety-stock requirement in cases (i) and (ii), but leads to an increase in the manufacturer's safety-stock requirement in (iii). Our results apply not only to two-stage but also to multistage supply chains.</description><author>Gaur, V.; Giloni, A.; Seshadri, S.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Global village or cyber-balkans? Modeling and measuring the integration of electronic communities</title><link>http://www.example.com/articles/1</link><description>Van Alstyne, M.; Brynjolfsson, E.
Information technology can link geographically separated people and help them locate interesting or useful resources. These attributes have the potential to bridge gaps and unite communities. Paradoxically, they also have the potential to fragment interaction and divide groups. Advances in technology can make it easier for people to spend more time on special interests and to screen out unwanted contact. Geographic boundaries can thus be supplanted by boundaries on other dimensions. This paper formally defines a precise set of measures of information integration and develops a model of individual knowledge profiles and community affiliation. These factors suggest specific conditions under which improved access, search, and screening can either integrate or fragment interaction on various dimensions. As IT capabilities continue to improve, preferences-not geography or technology-become the key determinants of community boundaries.</description><author>Van Alstyne, M.; Brynjolfsson, E.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Things change: Dynamic resource constraints and system-dependent selection in the evolution of organizational populations</title><link>http://www.example.com/articles/1</link><description>Lomi, A.; Larsen, E. R.; Freeman, J. H.
An extensive empirical literature has demonstrated the existence of. density-dependent selection in organizational vital rates. This research has also shown that historical trajectories followed by organizational populations only partly conform to the predictions of the original model. Inconsistencies with the model's predictions prompt a series of questions: Why do organizational populations suddenly, collapse after reaching a peak? Why do organizational populations oscillate after collapsing? What causes extinction of organizational forms? To address these questions, scholars have proposed a variety of modifications to the original model of density dependence. All have merit, but none is completely satisfying. The main objective of this study is to narrow the gap between theories, models, and observed historical trajectories by identifying a unitary analytical framework that can account for the variety of empirical trajectories typically followed by mature organizational populations. The model that we present is based on the hypothesis of system-dependent selection, according to which patterns of resource availability are produced by processes that are partly endogenous to organizational populations. The main analytical insight of the study is that under conditions of dynamic resource constraints introduced by system-dependent selection, the,presence of population-level inertia leads to a rich variety of historical trajectories during population maturity. We show that this result holds in the absence of any particular assumption about the microstructure of organizational populations. Possible trajectories include sustained oscillations, resurgence, and extinction.</description><author>Lomi, A.; Larsen, E. R.; Freeman, J. H.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring customer relationships: The case of the retail banking industry</title><link>http://www.example.com/articles/1</link><description>Nagar, V.; Rajan, M. V.
Arguing that GAAP is ill suited for estimating the future profitability of intangibles, the accounting literature (e.g., Kaplan and Norton 1996, Lev 2001) has recently proposed alternative measurement models. These models view intangibles as composed of a set of fundamental business activities and use multiple financial and nonfinancial metrics causally interlinked to profits to represent this view. Using a unique and proprietary cross-sectional data set of the retail banking industry, we provide some of the first tests on the empirical validity of such measurement models. We characterize the core deposit intangible, an important retail banking intangible representing a bank's relationships with its customers, using financial and nonfinancial metrics on price, service, customer usage, and customer satisfaction. We find that the metrics do not individually predict future earnings, but gain individual significance in a collective setting, increasing the predictive power substantially. We argue that this result occurs because the activities underlying the measures are causally interlinked to profits and explicitly illustrate these linkages with a structural path model. Our measurement model also predicts significant interactive effects in the way our measures are informative about future profits, and we document such effects, not just among the individual measures, but also across the measures and environmental factors such as the bank's strategy. In sum, our measurement model illustrates the key drivers, measures, and interactions in retail banking customer relationships.</description><author>Nagar, V.; Rajan, M. V.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Corporate disclosure and operational strategy: Financial vs. operational success</title><link>http://www.example.com/articles/1</link><description>Ozbilgin, M.; Penno, M.
We introduce a simple game between two rival firms-a leader and a follower, where the leader moves first and makes an operational choice under uncertainty. The leader's disclosure of its resulting financial success or failure may in turn give the follower a competitive advantage by informing its operational choice. When this occurs, the leader reacts by sometimes making an operational choice that it knows to be less likely to produce operational success than the alternative. This makes the financial report less useful to the follower and expected financial success more likely for the leader. Alternatively, when the financial report does not provide useful information to the follower (e.g., the financial report aggregates many activities in addition to the activity the follower is interested in), it may be the follower rather than the leader who makes the choice less likely to be operationally successful. We document that when trading off operational success for financial success, the leader's aim is operational unpredictability, while the follower's aim is coordination. As such, this paper highlights the intricate interplay between internal operational decisions, public inferences concerning those decisions, and different forms of success under intense competition.</description><author>Ozbilgin, M.; Penno, M.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Detecting regime shifts: The causes of under- and overreaction</title><link>http://www.example.com/articles/1</link><description>Massey, C.; Wu, G.
Many decision makers operate in dynamic environments in which markets, competitors, and technology change regularly. The ability to detect and respond to these regime shifts is critical for economic success. We conduct three experiments to test how effective individuals are at detecting such regime shifts. Specifically, we investigate when individuals are most likely to underreact to change and when they are most likely to overreact to it. We develop a system-neglect hypothesis: Individuals react primarily to the signals they observe and secondarily to the environmental system that produced the signal. The experiments, two involving probability estimation and one involving prediction, reveal a behavioral pattern consistent with our system-neglect hypothesis: Underreaction is most common in unstable environments with precise signals, and overreaction is most common in stable environments with noisy signals. We test this pattern formally in a statistical comparison of the Bayesian model with a parametric specification of the system-neglect model.</description><author>Massey, C.; Wu, G.</author><pubDate>Wed, 01 Jun 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Resource allocation based on efficiency analysis</title><link>http://www.example.com/articles/1</link><description>Korhonen, P.; Syrjanen, M.
The purpose of this paper is to develop an approach to a resource-allocation problem that typically appears in organizations with a centralized decision-making environment, for example, supermarket chains, banks, and universities. The central unit is assumed to be interested in maximizing the total amount of outputs produced by the individual units by allocating available resources to them. We will develop an interactive formal approach based on data envelopment analysis (DEA) and multiple-objective linear programming (MOLP) to find the most preferred allocation plan. The units are assumed to be able to modify their production in the current production possibility set within certain assumptions. Various assumptions are considered concerning returns to scale and the ability of each unit to modify its production plan. Numerical examples are used to illustrate the approach.</description><author>Korhonen, P.; Syrjanen, M.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Metaheuristics with local search techniques for retail shelf-space optimization</title><link>http://www.example.com/articles/1</link><description>Lim, A.; Rodrigues, B.; Zhang, X. W.
Efficient shelf-space allocation can provide retailers with a competitive edge. While there has been little study on this subject, there is great interest in improving product allocation in the retail industry. This paper examines a practicable linear allocation model for optimizing shelf-space allocation. It extends the model to address other requirements such as product groupings and nonlinear profit functions. Besides providing a network flow solution, we put forward a strategy that combines a strong local search with a metaheuristic approach to space allocation. This, strategy is flexible and efficient, as it can address both linear and nonlinear problems of realistic size while achieving, near-optimal solutions through easily implemented algorithms in reasonable timescales. It offers, retailers opportunities for more efficient and,profitable shelf management, as well as higher-quality planograms.</description><author>Lim, A.; Rodrigues, B.; Zhang, X. W.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A note on "the latest arrival hub location problem"</title><link>http://www.example.com/articles/1</link><description>Wagner, B. N.
Kara and Tansel (Management Science, Volume 47, 2001, 1408-1420) introduce the latest arrival hub location problem as a more realistic way of modeling cargo delivery. The key feature of the new model is that each link in the network is served by a single vehicle that makes exactly bone trip. As a result, the traffic requiring a particular link may be,forced to wait for all other traffic requiring the same link to arrive. Therefore, the travel time between a particular pair of nodes depends not just on travel times of the links on the path taken, but also on these forced "waiting times," which are called transient times in Kara and Tansel (2001). We show, however, that if the objective function depends only on the maximum travel time, this "new" model is essentially the same as the classical model that ignores the transient times. Our result implies that two of the three versions of the latest arrival hub location problem proposed by Kara and Tansel, including the one to, which they devote most of their paper, are equivalent to their classical counterparts.</description><author>Wagner, B. N.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note: Additional learning and implications on the role of informative advertising</title><link>http://www.example.com/articles/1</link><description>Soberman, D. A.
Observers argue that evidence for the persuasive role of advertising comes from competitive categories where increases in advertising lead to higher average prices. Conversely, others claim that advertising serves a purely informational role. Here, higher levels of advertising lead to better-informed consumers and this should increase competition and stimulate lower prices. The objective of this study is to neither confirm nor refute either of these perspectives. It is rather to show that increases in informative advertising alone can lead to both higher or lower prices. I further show that the direction of this relationship depends on the level of differentiation between competing firms. Similar to Grossman and Shapiro (1984), I examine conditions where the differences between competing products are small, but I also examine conditions where the differences are significant. The role of advertising is to inform consumers about individual products and higher advertising for a product means that more of the potential market knows about it. Higher levels of advertising increase the relative importance of fully informed consumers compared to partially informed consumers. This dynamic is the basis for explaining why informative advertising can push prices either up or down in a uniformly distributed spatial market.</description><author>Soberman, D. A.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing inventory and supply performance in assembly systems with random supply capacity and demand</title><link>http://www.example.com/articles/1</link><description>Bollapragada, R.; Rao, U. S.; Zhang, J.
We consider stock positioning in a pure assembly system controlled using installation base-stock policies. When component suppliers have random capacity and end-product demand is uncertain, we characterize the system's inventory dynamics. We show that components and, the, end product. play convex complementary roles in providing customer service. We propose a decomposition approach that uses an internal service level to independently determine near-optimal stock levels for each component. Compared with the optimal, the average error of the decomposition approach is 0.66% across the tested instances. Compared with current practice, this approach has the potential to reduce the safety-stock cost by as much as 30%. Our computational analysis on two-echelon systems also illustrates several managerial insights: We observe that the cost reduction from improving supply performance is high when demand variability or the number of components or target customer service is high, or when the end product is more expensive relative to components. On average, (i) reducing the lead time of the more expensive component yielded higher-benefit than reducing the lead time for the less expensive component, and (ii) the benefit of improving one of the supply parameters (service level or lead time) was higher when the value of the other parameter was already more favorable (lower lead time or higher service level, respectively). Finally, we analytically show how a multi-echelon pure assembly system may be converted into an equivalent two-echelon assembly system to which all our results apply.</description><author>Bollapragada, R.; Rao, U. S.; Zhang, J.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Franchising, ownership, and experience: A study of pizza restaurant survival</title><link>http://www.example.com/articles/1</link><description>Kalnins, A.; Mayer, K. J.
We hypothesize that retail and service business units will enjoy reduced failure rates if affiliated with experienced multiunit owners and franchisors. Experience of individual owners and franchisees should result in knowledge that is tacit-and idiosyncratic and thus primarily of value locally Because franchisors typically codify knowledge gained from experience, we argue that units should benefit from both local and distant experience of their franchisor. Using Texan pizza restaurant failure data, we found that the units of all multiunit owners, franchised or not, benefited from their owner's local congenital experience, but not from distantly gained experience. Further, the franchisor's local experience reduced failure rates. Contrary to one hypothesis, franchisors' distant experience did not prove beneficial. In addition, a complementary effect was found for owner and franchisor congenital experience. These results highlight the continued importance of local experience, even among the most codified and standardized business organizations.</description><author>Kalnins, A.; Mayer, K. J.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Are physicians "easy are physicians marks"? Quantifying the effects of detailing and sampling on new prescriptions</title><link>http://www.example.com/articles/1</link><description>Mizik, N.; Jacobson, R.
uch public attention and considerable controversy surround pharmaceutical marketing practices and their impact on physicians. However, views on the matter have largely been shaped by anecdotal evidence or results from analyses with insufficient controls. Making use of a dynamic fixed-effects distributed lag regression model, we empirically assess the role that two central components of pharmaceutical marketing practices (namely, detailing and sampling) have on physician prescribing behavior. Key differentiating features of our model include its ability to (i) capture persistence in the prescribing process and decompose it into own-growth and competitive-stealing effects, (ii) estimate an unrestricted decay structure of the promotional effects over time, and (iii) control for physician-specific effects that, if not taken into account, induce biased coefficient estimates of detailing and sampling,effects. Based on pooled time series cross-sectional data involving three drugs, 24 monthly observations, and 74,07 individual physicians (more than 2 million observations in total), we find that detailing and free drug samples have positive and statistically significant effects on the number of new prescriptions issued by a physician. However, we find that the magnitudes of the effects are modest.</description><author>Mizik, N.; Jacobson, R.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Simulation of the new product development process for performance improvement</title><link>http://www.example.com/articles/1</link><description>Bhuiyan, N.; Gerwin, D.; Thomson, V.
This paper explores the linkages between key features of the new product development (NPD) process and NPD performance and suggests ways of designing the process to improve performance. Using a stochastic computer model, we examine, under varying uncertainty conditions, how the key features of overlapping and functional interaction affect the performance measures of development time and effort (total person-days for a project). Findings indicate that, first and foremost, whether or not overlapping occurs, increasing functional interaction eventually leads to a trade-off between development time and effort. Second, an "early-start-in-the-dark" approach of increasing overlapping with no functional interaction is inferior even to an "over-the-wall" approach. Third, increasing overlapping when some functional interaction exists is beneficial in low uncertainty and harmful in high uncertainty Fourth, concurrent engineering (CE) is appropriate under low uncertainty, while a type of sequential engineering (SE), different than the "over-the-wall" approach, should be used under high uncertainty, and last, dedicated teams are suitable under high, and not low, uncertainty. We developed the model with the aid of a company and validated it against a published account of five case studies.</description><author>Bhuiyan, N.; Gerwin, D.; Thomson, V.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The misalignment of product architecture and organizational structure in complex product development</title><link>http://www.example.com/articles/1</link><description>Sosa, M. E.; Eppinger, S. D.; Rowles, C. M.
Product architecture knowledge is typically embedded in the communication patterns of established development organizations. While this enables the development of products using the existing architecture, it hinders the organization's ability to implement novel architectures, especially for complex products. Structured methods addressing this issue are lacking, as previous research has studied complex product development from two separate perspectives: product architecture and organizational structure. Our research integrates these viewpoints with a structured approach to study how design interfaces in them product architecture map onto communication patterns within the development organization. We investigate how organizational and system boundaries, design interface strength; indirect interactions, and system modularity impact the alignment of design interfaces and team interactions. We hypothesize and test how these factors explain the existence of the following cases: (1) known design interfaces not addressed by team intractions, and (2) observed team interactions not predicted by design interfaces. Our results offer important insights to managers dealing with interdependences across organizational and functional boundaries. In particular, we show how boundary effects moderate the impact of design interface strength and indirect team interactions and are contingent on system modularity. The research uses data collected from a large commercial aircraft engine development process.</description><author>Sosa, M. E.; Eppinger, S. D.; Rowles, C. M.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Nonlinear pricing of information goods</title><link>http://www.example.com/articles/1</link><description>Sundararajan, A.
This paper analyzes optimal pricing for information goods under incomplete information, when both unlimited-usage (fixed-fee) pricing and usage-based pricing are feasible and administering usage-based pricing may involve transaction costs. It is shown that offering fixed-fee pricing in addition to a nonlinear usage-based pricing scheme is always profit improving in the presence of nonzero transaction costs, and there may be markets in which a pure fixed-fee is optimal. This implies that the optimal pricing strategy for information goods is almost never fully revealing. Moreover, it is proved that the optimal usage-based pricing schedule is independent of the value of the fixed fee, a result that simplifies the simultaneous design of pricing schedules considerably and provides a simple procedure for determining the optimal combination of fixed-fee and nonlinear usage-based pricing. The introduction of fixed-fee pricing is shown to increase both consumer surplus and total surplus. The differential effects of setup costs, fixed transaction costs, and variable transaction costs on pricing policy are described. These results suggest a number of managerial guidelines for designing pricing schedules. For instance, in nascent information markets, firms may profit from low fixed-fee penetration pricing, but as these markets mature, the optimal pricing mix should expand to include a wider range of usage-based pricing options. Minimum fees, quantity discounts, and adoption levels across the different pricing schemes are characterized, strategic pricing responses to changes in market characteristics are described, and the implications of the paper's results for bundling and vertical differentiation of information goods are discussed.</description><author>Sundararajan, A.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reduced quality and an unlevel playing field could make consumers happier</title><link>http://www.example.com/articles/1</link><description>Melumad, N. D.; Ziv, A.
We study a model of imperfect competition and limited production capacity in which a key feature is the trade-off between quality and quantity. In particular, lowering product quality enables firms to increase total production. We illustrate that, in the presence of limited capacity, the choice of lower quality often results in increased social welfare. We also explore the relation between the extent of competition and the choice of quality. We find that, in some cases, reduced competition leads to increased production, decreased average quality, increased total welfare, and makes consumers better off. Finally, we consider the possibility of regulator-mandated quality standards. Imposing high-quality standards never improves welfare in our model. On the other hand, mandating an upper bound on quality could either increase or decrease welfare in either a monopoly or a duopoly market.</description><author>Melumad, N. D.; Ziv, A.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information revelation, incentives, and the value of a real option</title><link>http://www.example.com/articles/1</link><description>Mittendorf, B.
The real options approach to capital budgeting focuses on valuing benefits of project flexibility This paper presents an incentive consideration in such valuation. Operating flexibility not only allows a firm to change course in response to new information, but also allows interested observers to make inferences based on the change in course (or lack thereof). Such information conveyance through refined operating choices can alter observers' incentives. As a result, an option to delay may prove valuable because it allows a firm to prolong informational advantages.</description><author>Mittendorf, B.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategy selection and performance measurement choice when profit drivers are uncertain</title><link>http://www.example.com/articles/1</link><description>Dye, R. A.
This paper studies a manager's attempt to maximize his firm's discounted expected profits by choosing what strategic actions to select and what performance measurement system to employ in a setting where the manager is uncertain about what variables "drive" the firm's profits, the firm's profit drivers remain stationary over time, and strategic actions differ in the amount of information they produce about the firm's profit drivers. For each available performance measurement system, this paper identifies necessary and sufficient conditions for experimentation-that is, deviating from the firm's short-run expected profit-maximizing action-to be optimal. In addition, the paper determines what factors influence a firm's preferred performance measurement system, and it explains why the preferred performance measurement system is likely to change over time.</description><author>Dye, R. A.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A perspective on "asymmetric information, incentives and intrafirm resource allocation"</title><link>http://www.example.com/articles/1</link><description>Rajan, M. V.; Reichelstein, S.
The paper "Asymmetric Information, Incentives and Intrafirm Resource Allocation," by Harris, Kriebel, and Raviv, was published in the June 1982 issue of Management Science. In this article, written as part of this journal's 50-year anniversary celebration, we highlight the significance of the Harris et al. paper for research in managerial accounting. We first formulate and solve a continuous version of the Harris et al. model to illustrate the key assumptions and findings of their paper. We then review several strands of the resource-allocation literature in managerial accounting that have taken their inspiration, either directly or indirectly, from the work of Harris et al.</description><author>Rajan, M. V.; Reichelstein, S.</author><pubDate>Wed, 01 Dec 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational behavior, strategy, performance, and design in Management Science</title><link>http://www.example.com/articles/1</link><description>Boudreau, J. W.
This article provides a personal perspective on the themes and topics that have emerged in research published by the department of organizational behavior, in the first 50 years of Management Science. A review of articles accepted by the department suggests several themes that reflect broad objectives, such as "improving management science models," or distinguishing points of view or assumptions, such as "treating organizations as decision-making entities." The research is summarized through topics and subtopics that classify the subjects and findings of the research, identifying the relative popularity of the topics, and their contribution to each of the themes. The pattern suggests that as a "behavioral" department in a journal characterized by managerial practice, optimization, and the context of real work organizations, research is uniquely grounded in applications, solutions, and work consequences in ways that are less prevalent in typical outlets for behavioral research. The article concludes by suggesting that there are ample signs of convergence across departments in the journal. Scholars in organizational behavior increasingly recognize that the context provided by other management disciplines provides essential insights. Likewise, scholars in other management disciplines increasingly recognize the value of integrating behavioral theories and findings into their frameworks and models.</description><author>Boudreau, J. W.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The strength of weak ties you can trust: The mediating role of trust in effective knowledge transfer</title><link>http://www.example.com/articles/1</link><description>Levin, D. Z.; Cross, R.
Research has demonstrated that relationships are critical to knowledge creation and transfer, yet findings have been mixed regarding the importance of relational and structural characteristics of social capital for the receipt of tacit and explicit knowledge. We propose and test a model of two-party (dyadic) knowledge exchange, with strong support in each of the three companies surveyed. First, the link between strong ties and receipt of useful knowledge (as reported by the knowledge seeker) was mediated by competence- and benevolence-based trust. Second, once we controlled for these two trustworthiness dimensions, the structural benefit of weak ties emerged. This finding is consistent with prior research suggesting that weak ties provide access to non-redundant information. Third, competence-based trust was especially important for the receipt of tacit knowledge. We discuss implications for theory and practice.</description><author>Levin, D. Z.; Cross, R.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Remembrance of things past? The dynamics of organizational forgetting</title><link>http://www.example.com/articles/1</link><description>de Holan, P. M.; Phillips, N.
How organizations create, transfer, and retain knowledge has been the focus of intensive investigation by management researchers. However, one aspect of the dynamics of knowledge-organizational forgetting-has received comparatively little attention. In this paper, we draw on an exploratory, multiple-case study of learning in international strategic alliances to explore how and why organizations forget. Based on our case study, we develop a theory of organizational forgetting, discuss the role of forgetting in the dynamics of organizational knowledge, and present a typology of types of organizational forgetting.</description><author>de Holan, P. M.; Phillips, N.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How effective are electronic reputation mechanisms? An experimental investigation</title><link>http://www.example.com/articles/1</link><description>Bolton, G. E.; Katok, E.; Ockenfels, A.
Electronic reputation or "feedback" mechanisms aim to mitigate the moral hazard problems associated with exchange among strangers by providing the type of information available in more traditional close-knit groups, where members are frequently involved in one another's dealings. In this paper, we compare trading in a market with online feedback (as implemented by many Internet markets) to a market without feedback, as well as to a market in which the same people interact with one another repeatedly (partners market). We find that while the feedback mechanism induces quite a substantial improvement in transaction efficiency, it also exhibits a kind of public goods problem in that, unlike in the partners market, the benefits of trust and trustworthy behavior go to the whole community and are not completely internalized. We discuss the implications of this perspective for improving feedback systems.</description><author>Bolton, G. E.; Katok, E.; Ockenfels, A.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Innovation and attention to detail in the quality improvement paradigm</title><link>http://www.example.com/articles/1</link><description>Naveh, E.; Erez, M.
This study asserted that quality improvement (QI) requires the coexistence of two cultural values of innovation and attention to detail and proposed that their coexistence depends on the implementation of multiple QI practices. A longitudinal QI intervention, with five phases, consisting of multiple QI practices-ISO 9000, QI teams, quality goals, and coaching and communication by top management-was implemented. Participants were 425 employees working in 18 departments of four manufacturing plants. The QI practices were implemented in a different order in each one of the plants. Measures were assessed five times, at the end of each implementation phase. We used hierarchical linear models (HLM) to account for the nested structure of departments within the plants and the five repeated measures. Findings demonstrated that the above-mentioned QI practices had differential effects on innovation and attention to detail: ISO 9000 positively affected attention to detail but negatively affected innovation. Both QI teams and quality goals positively affected innovation. Thus, the multiple QI initiative enabled the coexistence of the two aforementioned cultural values. Both cultural values had a positive impact on performance quality and productivity and partially mediated the effects of ISO 9000 on productivity.</description><author>Naveh, E.; Erez, M.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Focusing firm evolution: The impact of information infrastructure on market entry by US telecommunications companies, 1984-1998</title><link>http://www.example.com/articles/1</link><description>Williams, C.; Mitchell, W.
Organization structure acts as a lens on the environment, gathering information and shaping its flow through a firm to inform managers' choices. This shaping of information flow happens through an organization's operating units, which selectively process information from the environment, and through the links between them, which pass information between units. We explore the relationship between this "information infrastructure" and firm strategy using structure and service information from the eight largest telephone service providers in the United States from 1984 to 1998. We find that firms with more units that scan areas of opportunity are more likely to enter a market, while firms with more units that scan nonfocal areas are less likely to enter the market. We also find that personnel links between units and the corporate level of the firm often constrain entry to new markets by dampening a unit's appetite for risk. Personnel links between operating units, on the other hand, can make a firm more likely to enter new markets, particularly when the cooperating units combine different sets of information. Thus, a firm's information infrastructure plays a dual role in shaping firm evolution, leading toward some paths and away from others.</description><author>Williams, C.; Mitchell, W.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The role of volition in organizational learning: The case of automotive product recalls</title><link>http://www.example.com/articles/1</link><description>Haunschild, P. R.; Rhee, M.
What is the role of volition in organizational learning? Do firms learn better in response to internal procedures or external mandates? Existing literature provides conflicting answers to this question, with some theories suggesting that volition is important for learning because autonomy increases commitment and problem analyses, whereas external mandates tend to produce defensive reactions that are not coupled to the organization in any useful way. Yet, other theories suggest that mandate is important for learning because external pressures act as jolts that help overcome organizational inertia, resulting in deep exploration of problems to prevent future surprises. We investigate this issue in the context of automakers learning from voluntary versus involuntary product recalls. Using data on all recalls experienced by automakers that sold passenger cars in the United States during the 1966-1999 period, we follow the learning-curve tradition in investigating the effects of voluntary and involuntary recalls on subsequent recall rates. We find that voluntary recalls result in more learning than mandated recalls when learning is measured as a reduction in subsequent involuntary recalls. This effect is at least partly because of shallower learning processes that result from involuntary recalls. The effect of volition, however, is different for generalist and specialist automakers. The results of this study suggest an important, yet understudied, determinant of the rate and effectiveness of learning-volition. The results also add to our knowledge of the different learning processes of generalist and specialist organizations.</description><author>Haunschild, P. R.; Rhee, M.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effects of adaptive behaviors and shared mental models on control crew performance</title><link>http://www.example.com/articles/1</link><description>Waller, M. J.; Gupta, N.; Giambatista, R. C.
Control crews are highly trained teams responsible for monitoring complex systems, performing routine procedures, and quickly responding to nonroutine situations. Previous literature suggests that higher-performing control crews engage in adaptive behavior during high-workload or crisis situations. Other work suggests that higher-performing crews use periods of lower workloads to prepare for future problems. To understand which behaviors performed during which situations better differentiate lower- from higher-performing crews, we conducted a study of 14 nuclear power plant control room crews and examined adaptive behaviors and shared mental model development in the crews as they faced monitoring, routine, and nonroutine situations. Our results suggest that few differences in adaptive behaviors exist between higher- and lower-performing crews during monitoring or routine situations, but that information collection and shared mental model development activities, and intracrew processes used during model development, differ significantly between lower- and higher-performing control crews during nonroutine situations.</description><author>Waller, M. J.; Gupta, N.; Giambatista, R. C.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge and performance in knowledge-worker teams: A longitudinal study of transactive memory systems</title><link>http://www.example.com/articles/1</link><description>Lewis, K.
This study examined how transactive memory systems (TMSs) emerge and develop to affect the performance of knowledge-worker teams. Sixty-four MBA consulting teams (261 members) participated in the study. I proposed that the role and function of TMSs change to meet different task and knowledge demands during a project. Hypotheses predicting that TMSs emerge during a project-planning phase as a function of a team's initial conditions, and later develop and mature as a function of the nature and frequency of communication were generally supported, as were hypothesized relationships between TMSs and team performance and viability. Findings suggest that teams with initially distributed expertise and familiar members are more likely to develop a TMS. Frequent face-to-face communication also led to TMS emergence, but communication via other means had no effect. Teams with more established TMSs later benefited from face-to-face communication, but they were less helped by frequent communication via other means, suggesting that transactive retrieval processes may have been triggered during face-to-face communication and suppressed during other types of communication. TMSs were positively related to team viability and team performance, suggesting that developing a TMS is critical to the effectiveness of knowledge-worker teams.</description><author>Lewis, K.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing knowledge-based resource capabilities under uncertainty</title><link>http://www.example.com/articles/1</link><description>Carrillo, J. E.; Gaimon, C.
A firm's ability to manage its knowledge-based resource capabilities has become increasingly important as a result of performance threats triggered by technology change and intense competition. At the manufacturing plant level, we focus on three repositories of knowledge that drive performance. First, the physical production or information systems represent knowledge embedded in the plant's technical systems. Second, the plant's workforce has knowledge, including diverse scientific information and skills, to effectively operate the technical systems. Third, the firm's managerial systems embody knowledge in the form of goals, reward systems, and control and coordination systems. Taken together, we consider the technical systems, workforce knowledge, and the managerial systems as the plant's knowledge-based resource capability. Two normative models are introduced offering insight on how plant performance is impacted by investments in workforce knowledge (training) or the technical systems (process change). The models explicitly recognize that the outcome of investments in knowledge-based change is uncertain due to factors including technical problems, worker resistance, and limited financial resources. Also, we recognize that workforce knowledge may be deployed to mitigate the outcome uncertainty encountered with process change. Investments in knowledge-based change cannot be fully understood in isolation of the managerial systems. In one model, the plant manager is motivated by an incentive system that rewards the realization of a threshold goal, whereas in the other model the incentive system emphasizes the realization of meeting a particular target goal. We also investigate the impact of the manager's view of uncertainty (her willingness to absorb risk), which is influenced by the managerial systems. Results show that different characterizations of the managerial systems have a profound effect on managerial behavior and plant-level performance.</description><author>Carrillo, J. E.; Gaimon, C.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing knowledge in the dark: An empirical study of the reliability of capability evaluations</title><link>http://www.example.com/articles/1</link><description>Denrell, J.; Arvidsson, N.; Zander, U.
If knowledge is to be managed and transferred, it is essential that members of organizations know and agree on where capabilities reside. Few studies, however, have examined the difficulties of evaluating capabilities in large firms. This paper reports an in-depth empirical study of capabilities central to knowledge management efforts in large leading multinational companies. The results show that evaluation of these capabilities is a complex task. The median interrater correlation for capabilities designated as strategic by top management is only 0.28. Analysis of the determinants of reliability show that the difference in evaluations is largest for subsidiaries managers know less about, for younger subsidiaries, and for subsidiaries in less important markets. The results of our empirical study have important implications for creating, retaining, and transferring knowledge in organizations.</description><author>Denrell, J.; Arvidsson, N.; Zander, U.</author><pubDate>Mon, 01 Nov 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Primal-dual simulation algorithm for pricing multidimensional American options</title><link>http://www.example.com/articles/1</link><description>Andersen, L.; Broadie, M.
This paper describes a practical algorithm based on Monte Carlo simulation for the pricing of multidimensional American (i.e., continuously exercisable) and Bermudan (i.e., discretely exercisable) options. The method generates both lower and upper bounds for the Bermudan option price and hence gives valid confidence intervals for the true value. Lower bounds can be generated using any number of primal algorithms. Upper bounds are generated using a new Monte Carlo a ialgorithm based on the duality representation of the Bermudan value function suggested independently in Haugh and Kogan (2004) and Rogers (2002). Our proposed algorithm can handle virtually any type of process dynamics, factor structure, and payout specification. Computational results for a variety of multifactor equity and interest-rate options demonstrate the simplicity and efficiency of the proposed algorithm. In particular, we use the proposed method to examine and verify the tightness of frequently used exercise rules in Bermudan swaption markets.</description><author>Andersen, L.; Broadie, M.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decentralized pricing and capacity decisions in a multitier system with modular assembly</title><link>http://www.example.com/articles/1</link><description>Bernstein, F.; DeCroix, G. A.
We model a modular assembly system in which a final assembler outsources some of the assembly task to first-tier suppliers (subassemblers), who produce modules made up of multiple components. The assembler sets module prices it will pay to the subassemblers, the subassemblers set component prices they will pay to suppliers, and then all players choose how much capacity to install, with the minimum capacity choice determining system capacity. Finally, stochastic end-product demand is observed and all players produce (and are paid for) the same number of units-the minimum of demand and system capacity. We characterize equilibrium price and capacity choices, and then use that characterization to derive results regarding higher-level structural choices by the assembler-such as how to group components into modules and which suppliers to choose as subassemblers. We also compare performance of the system to a traditional assembly system with an assembler and suppliers but without subassemblers.</description><author>Bernstein, F.; DeCroix, G. A.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory management with asset-based financing</title><link>http://www.example.com/articles/1</link><description>Buzacott, J. A.; Zhang, R. Q.
Most of the traditional models in production and inventory control ignore the financial states of an organization and can lead to infeasible practices in real systems. This paper is the first attempt to incorporate asset-based financing into production decisions. Instead of setting a known, exogenously determined budgetary constraint as most existing models suggest, we model the available cash in each period as a function of assets and liabilities that may be updated periodically according to the dynamics of the production activities. Furthermore, our models allow different interest rates on cash balance and outstanding loans, which is an enhancement over most traditional models in that inventory financed by a loan may be more expensive than that by out-of-pocket cash. We demonstrate the importance of joint consideration of production and financing decisions in a start-up setting in which the ability to grow the firm is mainly constrained by its limited capital and dependence on bank financing. We then explain the motivation for asset-based financing by examining the decision making at a bank and a set of retailers in a newsvendor setting.</description><author>Buzacott, J. A.; Zhang, R. Q.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comparison of VaR and CVaR constraints on portfolio selection with the mean-variance model</title><link>http://www.example.com/articles/1</link><description>Alexander, G. J.; Baptista, A. M.
In this paper, we analyze the portfolio selection implications arising from imposing a value-at-risk (VaR) constraint on the mean-variance model, and compare them with those arising from the imposition of a conditional value-at-risk (CVaR) constraint. We show that for a given confidence level, a CVaR constraint is tighter than a VaR constraint if the CVaR and VaR bounds coincide. Consequently, a CVaR constraint is more effective than a VaR constraint as a tool to control slightly risk-averse agents, but in the absence of a risk-free security, has a perverse effect in that it is more likely to force highly risk-averse agents to select portfolios with larger standard deviations. However, when the CVaR bound is appropriately larger than the VaR bound or when a risk-free security is present, a CVaR constraint "dominates" a VaR constraint as a risk management tool.</description><author>Alexander, G. J.; Baptista, A. M.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dividends and debt with managerial agency and lender holdup</title><link>http://www.example.com/articles/1</link><description>Kanatas, G.; Qi, J. P.
A well-known view in the literature is that if management is more concerned with the firm's survival than with profitability, it is efficient to use a levered capital structure and thereby transfer the liquidation decision to lenders. Our paper extends this idea to a setting where lenders behave opportunistically when they control the liquidation decision. We show that in this situation, an optimal mix of debt and dividends, can mitigate the twin moral hazard problems of the manager and the lender. Given an otherwise optimal capital structure, initiating a dividend policy increases firm value, lowers debt payments, but raises total cash disbursements-interest and dividends-to investors. Numerous other empirical implications of the model are also discussed.</description><author>Kanatas, G.; Qi, J. P.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing path-dependent securities by the extended tree method</title><link>http://www.example.com/articles/1</link><description>Kishimoto, N.
This paper presents a discrete-time method (ET method) for pricing path-dependent securities by the supplementary variable technique and examines the ET method from the point of view of Arrow-Debreu event tree. In particular, this paper identifies sufficient conditions on supplementary variables under which the ET method yields the same price for a path-dependent security as a valuation method based on a comparable Arrow-Debreu event tree. Two examples are provided to illustrate the ET method. The first example is a valuation of collateralized mortgage obligations (CMOs), where the collateral of a CMO is modeled as a pool of mortgage loans with heterogeneous prepayment costs. The second example is a valuation of American average options where the average is computed over a moving period with a fixed length. In addition, this paper presents a measure for the computational size of the ET method and illustrates numerical advantages of the ET method with examples.</description><author>Kishimoto, N.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Which GARCH model for option valuation?</title><link>http://www.example.com/articles/1</link><description>Christoffersen, P.; Jacobs, K.
Characterizing asset return dynamics using volatility models is an important part of empirical finance. The existing literature on GARCH models favors some rather complex volatility specifications whose relative performance is usually assessed through their likelihood based on a time series of asset returns. This paper compares a range of GARCH models along a different dimension, using option prices and returns under the risk-neutral as well as the physical probability measure. We judge the relative performance of various models by evaluating an objective function based on option prices. In contrast with returns-based inference, we find that our option-based objective function favors a relatively parsimonious model. Specifically, when evaluated out-of-sample, our analysis favors a model that, besides volatility clustering, only allows for a standard leverage effect.</description><author>Christoffersen, P.; Jacobs, K.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Convergence of the least squares Monte Carlo approach to American option valuation</title><link>http://www.example.com/articles/1</link><description>Stentoft, L.
In a recent paper, Longstaff and Schwartz (2001) suggest a method to American option valuation based on simulation. The method is termed the Least Squares Monte Carlo (LSM) method, and although it has become widely used, not much is known about the properties of the estimator. This paper corrects this shortcoming using theory from the literature on seminonparametric series estimators. A central part of the LSM method is the approximation of a set of conditional expectation functions. We show that the approximations converge to the true expectation functions under general assumptions in a multiperiod, multidimensional setting. We obtain convergence rates in the two-period, multidimensional case, and we discuss the relation between the optimal rate of convergence and the properties of the conditional expectation. Furthermore, we show that the actual price estimates converge to the true price. This provides the mathematical foundation for the use of the LSM method in derivatives research.</description><author>Stentoft, L.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Option pricing under a double exponential jump diffusion model</title><link>http://www.example.com/articles/1</link><description>Kou, S. G.; Wang, H.
Analytical tractability is one of the challenges faced by many alternative models that try to generalize the Black-Scholes option pricing model to incorporate more empirical features. The aim of this paper is to extend the analytical tractability of the Black-Scholes model to alternative models with jumps. We demonstrate that a double exponential jump diffusion model can lead to an analytic approximation for finite-horizon American options (by extending the Barone-Adesi and Whaley method) and analytical solutions for popular path-dependent options (such as lookback, barrier, and perpetual American options). Numerical examples indicate that the formulae are easy to implement, and are accurate.</description><author>Kou, S. G.; Wang, H.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Option pricing: Valuation models and applications</title><link>http://www.example.com/articles/1</link><description>Broadie, M.; Detemple, J. B.
This paper surveys the literature on option pricing from its origins to the present. An extensive review of valuation methods for European- and American-style claims is provided. Applications to complex securities and numerical methods are surveyed. Emphasis is placed on recent trends and developments in methodology and modeling.</description><author>Broadie, M.; Detemple, J. B.</author><pubDate>Wed, 01 Sep 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The performance implications of media richness in a business-to-business service environment: Direct versus indirect effects</title><link>http://www.example.com/articles/1</link><description>Vickery, S. K.; Droge, C.; Stank, T. P.; Goldsby, T. J.; Markland, R. E.
This research examines media richness by modeling face-to-face, telephone, and electronic media as one construct and testing its performance implications. The context is the third-party logistics industry, in which a customer firm allows a service provider to assume responsibility for all or part of a critical business process. This business-to-business service environment is characterized by high levels of complexity (uncertainty, variability, equivocality) and network interdependence, key contextual attributes that enhance media richness' impact. We found a direct effect of media richness on relational performance and through it, indirect effects on satisfaction and loyalty. Furthermore, we found a direct effect of media richness on loyalty, which suggests that service firms in networked relationships provide loyalty-inducing benefits the genesis of which is not in the satisfaction created by the service itself. While past studies have examined the relationship of richness-related constructs and performance, no significant link was found. Our study is the first to demonstrate that media richness can affect firm performance when businesses interact in a complex environment.</description><author>Vickery, S. K.; Droge, C.; Stank, T. P.; Goldsby, T. J.; Markland, R. E.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Throughput in serial lines with state-dependent behavior</title><link>http://www.example.com/articles/1</link><description>Powell, S. G.; Schultz, K. L.
Experimental evidence suggests that production-line workers adjust their work rates in certain situations to prevent idle time. We refer to this as state-dependent behavior, in contrast to the state-independent behavior of machines. In this paper, we develop several models for the state-dependent behavior of production workers. We then use these models to analyze the relation of line length to throughput in these systems. We find that state-dependent behavior makes serial lines more efficient and reduces the detrimental effects that longer line lengths have on throughput. In some cases, line efficiency can actually increase with length. This is a result of a higher percentage of workers having two buffers to provide feedback on the state of the line. Further, we show that workers who both speed up when they are likely to cause idle time for others and slow down when they are likely to become idle themselves improve the overall efficiency of the line.</description><author>Powell, S. G.; Schultz, K. L.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling the impact of merging capacity in production-inventory systems</title><link>http://www.example.com/articles/1</link><description>Iyer, A. V.; Jain, A.
W e model two separate, decentralized systems, each consisting of a warehouse and a production capacity. The demand processes experienced by the systems have different variabilities. The two decentralized systems consider an agreement to pool their production capacities. We examine the impact of the pooling of capacity on inventory costs under two operating rules: (i) the orders from the two warehouses are treated in a first-come-first-served manner, and (ii) the orders from the lower-variability warehouse are given nonpreemptive priority. We examine this issue using an analytical model that integrates base-stock inventory models with queuing models for the production capacity. The higher-variability demand is modeled as a hyperexponential renewal process, and the lower-variability demand is modeled as a Poisson process. In case of pooled capacity, the arrival process at the production queue is the superposition of the two processes. We prove conditions under which the first-come-first-served operating rule will fail to achieve a Pareto improvement over the separate systems because it would increase inventory cost at the lower-variability warehouse. We then show cases under which the high-variability warehouse will see a reduction in inventory cost over the split system even if it accepts lower priority.</description><author>Iyer, A. V.; Jain, A.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Are supply and plant inspections complements or substitutes? A strategic and operational assessment of inspection practices in biotechnology</title><link>http://www.example.com/articles/1</link><description>Mayer, K. J.; Nickerson, J. A.; Owan, H.
T his paper theoretically and empirically examines the conventional wisdom in procurement management that often portrays supply inspections and supplier plant inspections as substitutes. We develop a theoretical model that focuses on potential internal spillover costs of the buyer receiving low-quality inputs and external spillover costs should low-quality inputs go undetected. Key to our analysis is the condition of whether a buyer can commit to the intensity of supply inspection. If a buyer cannot commit, supply inspections and plant inspections are substitutes, as widely believed. The two types of inspections, however, may become complements when a buyer is able to commit to the intensity of supply inspection. Complementarity is especially likely when (a) external spillovers are smaller than expected internal spillovers, which depends on the level of buffer inventory, (b) when knowledge sharing between buyer and supplier becomes more effective as the supplier allocates more resources to learning for quality improvement, or (c) when hiding aspects of the production processes is easier for suppliers. We empirically evaluate our model with a new data set drawn from a large biotechnology manufacturer. Empirical results provide broad support for theory, which, we argue, might help to explain variation in inspection practices across industries. Our theory and empirical analysis contribute to the literatures on strategic management, organizational economics, and procurement management by highlighting the organizational and strategic use of inspection practices.</description><author>Mayer, K. J.; Nickerson, J. A.; Owan, H.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Auctions of homogeneous goods with increasing returns: Experimental comparison of alternative "Dutch" auctions</title><link>http://www.example.com/articles/1</link><description>Katok, E.; Roth, A. E.
Most business-to-business (B2B) auctions are used to transact large quantities of homogeneous goods, and therefore use multiunit mechanisms. In the B2B context, bidders often have increasing returns to scale, or synergies. We compare two commonly used auction formats for selling multiple homogeneous objects, both sometimes called "Dutch" auctions, in a set of value environments that include synergies and potentially subject bidders to the "exposure" and "free-riding" problems. We find that the descending-price auction, best known for its use in the Dutch flower auctions, is robust and performs well in a variety of environments, although there are some situations in which the ascending uniform-price auction similar to the one used by eBay better avoids the free-riding problem. We discuss the factors that influence each mechanism's performance in terms of the overall efficiency, the informational requirements, the seller's revenue, and the buyer's profit.</description><author>Katok, E.; Roth, A. E.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Designing marine reserves for fishery management</title><link>http://www.example.com/articles/1</link><description>Meester, G. A.; Mehrotra, A.; Ault, J. S.; Baker, E. K.
Recent reports have raised serious concerns about the rapid declines of historically productive marine fishery resources and the degradation of essential fish habitats. This global crisis has spurred development of innovative management strategies to rebuild depleted fisheries and marine ecosystems. One highly touted strategy involves the design and creation of marine reserves (areas off limits to extractive uses) to rebuild fisheries and conserve marine biodiversity. In this paper, we propose an integrated sequence of methodologies that provides an objective, quantitative framework for the design of marine reserves in spatially heterogeneous coastal ocean environments. The marine reserve designs proposed here satisfy the multiple, often-conflicting criteria of disparate resource user groups. This research is the first attempt to explicitly explore the trade-off between the conservation goals of fishery management and coral reef protection and the consumptive interests of commercial and recreational fishing fleets. The spatial distribution and size abundance of reef fish stocks throughout the Florida Keys coral reef ecosystem were estimated from a database consisting of more than 18,000 visual samples taken from 1979 to 2002. These distributions of multispecies abundance and biomass, in conjunction with a geographic database of coral reef habitats, are used to demonstrate an integer goal programming methodology for the design of networks of marine reserves, called plans. Once multiple plans are proposed, a simulation model is used to assess the effects of reserve size and shape on select Florida Keys reef fish populations under dynamic spatial and temporal conditions.</description><author>Meester, G. A.; Mehrotra, A.; Ault, J. S.; Baker, E. K.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Converting technology to mitigate environmental damage</title><link>http://www.example.com/articles/1</link><description>Levi, M. D.; Nault, B. R.
There are many situations where policy makers would like to induce firms to make a major discrete conversion in production technology to help the environment. This paper examines how heterogeneity in the operating condition of firms' plant and equipment, which cannot be observed by policy makers, can affect the choice between incentives to encourage conversion to a cleaner technology. By relating different conditions of firms' plant and equipment to production costs, extent of environmental damage, and cost of conversion to a cleaner technology, we show when a perfectly discriminating incentive to encourage conversion is not feasible. In addition, we show that firms with plant and equipment in better condition will convert their technology to mitigate their environmental damage, and firms with plant and equipment in poorer condition will not. This and a series of additional results lead to conditions under which an administratively simple uniform lump-sum incentive to switch to cleaner technology is preferable to one based on output. These results and conditions extend to cases where there are network externalities in conversion, and where there is strategic timing in firms' choice of when to convert.</description><author>Levi, M. D.; Nault, B. R.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Improving emergency responsiveness with management science</title><link>http://www.example.com/articles/1</link><description>Green, L. V.; Kolesar, P. J.
While the goal of OR/MS is to aid decision makers, implementation of published models occurs less frequently than one might hope. However, one area that has been significantly impacted by management science is emergency response systems. Dozens of papers on emergency service management appeared in the OR/MS literature in the 1970s alone, many of which were published in Management Science. Three of these papers won major prizes. More importantly, many of these papers led to the implementation of substantially new policies and practices, particularly in policing and firefighting. Much of this work originated in New York City, though many other cities subsequently adopted the resulting models and strategies. In this paper, we look at the context, content, and nature of the research and the factors that led to these early implementation successes. We then track the extent to which these original models are still affecting decision making in emergency response systems. We also examine the pace of development of new OR/MS models and applications in the area. Finally, we look at issues in emergency responsiveness that have emerged recently as a result of the national focus on terrorism and discuss the potential for future OR/MS modeling and application.</description><author>Green, L. V.; Kolesar, P. J.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient diversification according to Stochastic dominance criteria</title><link>http://www.example.com/articles/1</link><description>Kuosmanen, T.
This paper develops the first operational tests of portfolio efficiency based on the general stochastic dominance (SD) criteria that account for an infinite set of diversification strategies. The main insight is to preserve the cross-sectional dependence of asset returns when forming portfolios by reexpressing the SD criteria in T-dimensional Euclidean space, with elements representing rates of return in T different states of nature. We characterize subsets of this state-space that dominate a given evaluated return vector by first- and second-order SD. This allows us to derive simple SD efficiency measures and test statistics, computable by standard mathematical programming algorithms. The SD tests and efficiency measures are illustrated by an empirical application that analyzes industrial diversification of the market portfolio.</description><author>Kuosmanen, T.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficiency-driven heavy-traffic approximations for many-server queues with abandonments</title><link>http://www.example.com/articles/1</link><description>Whitt, W.
To provide useful practical insight into the performance of service-oriented (non-revenue-generating) call centers, which often provide low-to-moderate quality of service, this paper investigates the efficiency-driven (ED), many-server heavy-traffic limiting regime for queues with abandonments. Attention is focused on the M/M/s/r + M model, having a Poisson arrival process, exponential service times, s servers, r extra waiting spaces, exponential abandon times (the final +M), and the first-come-first-served service discipline. Both the number of servers and the arrival rate are allowed to increase, while the individual service and abandonment rates are held fixed. The key is how the two limits are related: In the now common quality-and-efficiency-driven (QED) or Halfin-Whitt limiting regime, the probability of initially being delayed approaches a limit strictly between 0 and 1, while the probability of eventually being served (not abandoning) approaches 1. In contrast, in the ED limiting regime, the probability of eventually being served approaches a limit strictly between 0 and 1, while the probability of initially being delayed approaches 1. To obtain the ED regime, it suffices to let the arrival rate and the number of servers increase with the traffic intensity rho held fixed with p &gt; 1 (so that the arrival rate exceeds the maximum possible service rate). The ED regime can be realistic because with the abandonments, the delays need not be extraordinarily large. When the ED appropriations are appropriate, they are appealing because they are remarkably simple.</description><author>Whitt, W.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Demand allocation in multiple-product, multiple-facility, make-to-stock systems</title><link>http://www.example.com/articles/1</link><description>Benjaafar, S.; Elhafsi, M.; de Vericourt, F.
We consider the problem of allocating demand arising from multiple products to multiple production facilities with finite capacity and load-dependent lead times. Production facilities can choose to manufacture items either to stock or to order. Products vary in their demand rates, holding and backordering costs, and service-level requirements. We develop models and solution procedures to determine the optimal allocation of demand to facilities and the optimal inventory level for products at each facility. We consider two types of demand allocation, one in which we allow the demand for a product to be split among multiple facilities and the other in which demand from each product must be entirely satisfied by a single facility. We also consider two forms of inventory warehousing, one in which inventory locations are factory based and one in which they are centralized. For each case, we offer a solution procedure to obtain optimal demand allocations and optimal inventory base-stock levels. For systems with multiple customer classes, we also determine optimal inventory rationing levels for each class for each product. We use the models to characterize analytically several properties of the optimal solution. In particular, we highlight eight principles that relate the effects of cost, congestion, inventory pooling, multiple sourcing, customer segmentation, inventory rationing, and process and demand variability.</description><author>Benjaafar, S.; Elhafsi, M.; de Vericourt, F.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The optimal timing of living-donor liver transplantation</title><link>http://www.example.com/articles/1</link><description>Alagoz, O.; Maillart, L. M.; Schaefer, A. J.; Roberts, M. S.
Living donors are a significant and increasing source of livers for transplantation, mainly because of the insufficient supply of cadaveric organs. We consider the problem of optimally timing a living-donor liver transplant to maximize the patient's total reward, such as quality-adjusted life expectancy. We formulate a Markov decision process (MDP) model in which the state of the process is described by patient health. We derive structural properties of the MDP model, including a set of intuitive conditions that ensure the existence of a control-limit optimal policy We use clinical data in our computational experiments, which show that the optimal policy is typically of control-limit type.</description><author>Alagoz, O.; Maillart, L. M.; Schaefer, A. J.; Roberts, M. S.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Jump bidding strategies in Internet auctions</title><link>http://www.example.com/articles/1</link><description>Easley, R. F.; Tenorio, R.
A bidding strategy commonly observed in Internet auctions is that of "jump bidding," or entering a bid larger than what is necessary to be a currently winning bidder. In this paper, we argue that the cost associated with entering online bids and the uncertainty about future entry-both of which distinguish Internet from live auctions-can explain this behavior. We present a simple theoretical model that includes the preceding characteristics, and derive the conditions under which jump bidding arises in a format commonly used for online trading, the ascending-price auction. We also present evidence, recorded from hundreds of Internet auctions, that is consistent with some of the basic predictions from our model. We find that jump bidding is more likely earlier in an auction, when jumping has a larger strategic value, and that the incentives to jump bid increase as competition increases. Our results also indicate that jump bidding is effective: Jump bidders place fewer bids overall, and increased early jump bidding deters entry later in the auction. We also discuss possible means of reducing bidding costs and evidence that Internet auctioneers are pursuing this goal.</description><author>Easley, R. F.; Tenorio, R.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Redesigning teams and incentives in a merger: An experiment with managers and students</title><link>http://www.example.com/articles/1</link><description>Montmarquette, C.; Rulliere, J. L.; Villeval, M. C.; Zeiliger, R.
After a merger, company officials face the challenge of making compensation schemes uniform and of redesigning teams with managers from companies with different incentives, work habits, and recruiting methods. In this paper, we investigate the relationship between executive pay and performance after a merger by dissociating the respective influence of shifts, which occur in both compensation incentives and team composition. The results of a real task experiment conducted with managers within a large pharmaceutical company not only show that changes in compensation incentives affect performance, but also suggest that the sorting effect of incentives in the previous companies impact cooperation and efficiency after the merger. Replicating this experiment with students showed differences in strategy rather than in substance between the two groups of subjects with managers appearing performance driven, while students are more cost driven.</description><author>Montmarquette, C.; Rulliere, J. L.; Villeval, M. C.; Zeiliger, R.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>From T-Mazes to labyrinths: Learning from model-based feedback</title><link>http://www.example.com/articles/1</link><description>Denrell, J.; Fang, C.; Levinthal, D. A.
Many organizational actions need not have any immediate or direct payoff consequence but set the stage for subsequent actions that bring the organization toward some actual payoff. Learning in such settings poses the challenge of credit assignment (Minsky 1961), that is, how to assign credit for the overall outcome of a sequence of actions to each of the antecedent actions. To explore the process of learning in such contexts, we create a formal model in which the actors develop a mental model of the value of stage-setting actions as a complex problem-solving task is repeated. Partial knowledge, either of particular states in the problem space or inefficient and circuitous routines through the space, is shown to be quite valuable. Because of the interdependence of intelligent action when a sequence of actions must be identified, however, organizational knowledge is relatively fragile. As a consequence, while turnover may stimulate search and have largely benign implications in less interdependent task settings, it is very destructive of the organization's near-term performance when the learning problem requires a complementarity among the actors' knowledge.</description><author>Denrell, J.; Fang, C.; Levinthal, D. A.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Two faces: Effects of business groups on innovation in emerging economies</title><link>http://www.example.com/articles/1</link><description>Mahmood, I. P.; Mitchell, W.
This paper shows that business groups in emerging economies exert dual effects on innovation. While groups facilitate innovation by providing institutional infrastructure, groups also discourage innovation by creating entry barriers for nongroup firms and thereby inhibit the proliferation of new ideas. This pattern reflects an evolutionary process in which the interplay of the availability of innovation infrastructure and variety of ideas influences the level of innovation in an industry. We show that group market share has an inverted-U impact on innovation in industrial sectors of both Korea and Taiwan during the 1981-1995 period. Institutional differences between Korea and Taiwan in terms of market structure and industrial policies lead to different innovation thresholds, the point at which the marginal costs of increasing group share begin to dominate the marginal benefits in the two countries.</description><author>Mahmood, I. P.; Mitchell, W.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selectionism and learning in projects with complexity and unforeseeable uncertainty</title><link>http://www.example.com/articles/1</link><description>Sommer, S. C.; Loch, C. H.
Companies innovating in dynamic environments face the combined challenge of unforeseeable uncertainty (the inability to recognize the relevant influence variables and their functional relationships; thus, events and actions cannot be planned ahead of time) and high complexity (large number of variables and interactions; this leads to difficulty in assessing optimal actions beforehand). There are two fundamental strategies to manage innovation with unforeseeable uncertainty and complexity: trial and error learning and selectionism. Trial and error learning involves a flexible (unplanned) adjustment of the considered actions and targets to new information about the relevant environment as it emerges. Selectionism involves pursuing several approaches independently of one another and picking the best one ex post. Neither strategy nor project management literatures have compared the relative advantages of the two approaches in the presence of unforeseeable uncertainty and complexity. We build a model of a complex project with unforeseeable uncertainty, simulating problem solving as a local search on a rugged landscape. We compare the project payoff performance under trial and error learning and selectionism, based on a priori identifiable project characteristics: whether unforeseeable uncertainty is present, how high the complexity is, and how much trial and error learning and parallel trials cost. We find that if unforeseeable uncertainty is present and the team cannot run trials in a realistic user environment (indicating the project's true market performance), trial and error learning is preferred over selectionism. Moreover, the presence of unforeseeable uncertainty can reverse an established result from computational optimization: Without unforeseeable uncertainty, the optimal number of parallel trials increases in complexity. But with unforeseeable uncertainty, the optimal number of trials might decrease because the unforeseeable factors make the trials less and less informative as complexity grows.</description><author>Sommer, S. C.; Loch, C. H.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How do value creation and competition determine whether a firm appropriates value?</title><link>http://www.example.com/articles/1</link><description>MacDonald, G.; Ryall, M. D.
How does competition among economic actors determine the value that each is able to appropriate? We provide a formal, general framework within which this question can be posed and answered, and then provide several results. Chief among them is a condition that is both required for, and guarantees, value appropriation. We apply our methodology to (i) assess the familiar notion that uniqueness, inimitability, and competition imply value appropriation, and (ii) determine the value appropriation possibilities for an innovator whose unique discovery is of use to several others who can compete for the right to use it.</description><author>MacDonald, G.; Ryall, M. D.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The strategy field from the perspective of Management science: Divergent strands and possible integration</title><link>http://www.example.com/articles/1</link><description>Gavetti, G.; Levinthal, D. A.
We reflect on the evolution of the strategy field as seen through the window of Management Science. Reflecting the diverse disciplinary roots of strategy research, we identify a broad-ranging body of work that varies with respect to the assumptions made regarding individual rationality and the level of analysis at which the research is carried out. We argue that recent developments begin to delineate a potentially unifying conceptual framework for treating the field's defining questions-the conceptual apparatus of evolutionary economics. We conclude by laying out important challenges for evolutionary economics if it is to serve as a foundation for both the positive and the normative research agendas of the strategy field.</description><author>Gavetti, G.; Levinthal, D. A.</author><pubDate>Fri, 01 Oct 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Joint inventory replenishment and component allocation optimization in an assemble-to-order system</title><link>http://www.example.com/articles/1</link><description>Akcay, Y.; Xu, S. H.
This paper considers a multicomponent, multiproduct periodic-review assemble-to-order (ATO) system that uses an independent base-stock policy for inventory replenishment. Product demands in each period are integer-valued correlated random variables, with each product being assembled from multiple units of a subset of components. The system quotes a prespecified response time window for each product and receives a reward if the demand for that product is filled within its response time window. We formulate a two-stage stochastic integer program with recourse to determine the optimal base-stock policy and the optimal component allocation policy for the ATO system. We show that the component allocation problem is a general multidimensional knapsack problem (MDKP) and is NP-hard. We propose a simple, order-based component allocation rule and show that it can be solved in either polynomial or pseudopolynomial time. We also use the sample average approximation method to determine the optimal base-stock levels and compare it with two variations of the equal fractile heuristic. Intensive testing indicates that our solution method for each stage of the stochastic program is robust, effective, and that it significantly outperforms existing methods. Finally, we discuss several managerial implications of our findings.</description><author>Akcay, Y.; Xu, S. H.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The evolution of research on information systems: A fiftieth-year survey of the literature in Management Science</title><link>http://www.example.com/articles/1</link><description>Banker, R. D.; Kauffman, R. J.
The development of the information systems (IS) literature in Management Science during the past 50 years reflects the inception, growth, and maturation of several different research streams. The five research streams we identify incorporate different definitions of the managerial problems that relate to IS, the alternate theoretical perspectives and different methodological paradigms to study them, and the levels of the organization at which their primary results impact managerial practice. The decision support and design science research stream studies the application of computers in decision support, control, and managerial decision making. The value of information research stream reflects relationships established based on economic analysis of information as a commodity in the management of the firm. The human-computer systems design research stream emphasizes the cognitive basis for effective systems design. The IS organization and strategy research stream focuses the level of analysis on the locus of value of the IS investment instead of on the perceptions of a system or its user. The economics of information systems and technology research stream emphasizes the application of theoretical perspectives and methods from analytical and empirical economics to managerial problems involving IS and information technologies (IT). Based on a discussion of these streams, we evaluate the IS literature's core contributions to theoretical and managerial knowledge, and make some predictions about the road that lies ahead for IS researchers.</description><author>Banker, R. D.; Kauffman, R. J.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the depth and dynamics of online search behavior</title><link>http://www.example.com/articles/1</link><description>Johnson, E. J.; Moe, W. W.; Fader, P. S.; Bellman, S.; Lohse, G. L.
This paper examines search across competing e-commerce sites. By analyzing panel data from over 10 000 Internet households and three commodity-like products (books, compact discs (CDs), and air travel services), we show that the amount of online search is actually quite limited. On average, households visit only 1.2 book sites, 1.3 CD sites, and 1.8 travel sites during a typical active month in each category. Using probabilistic models, we characterize search behavior at the individual level in terms of (1) depth of search, (2) dynamics of search, and (3) activity of search. We model an individual's tendency to search as a logarithmic process, finding that shoppers search across very few sites in a given shopping month. We extend the logarithmic model of search to allow for time-varying dynamics that may cause the consumer to evolve and, perhaps, learn to search over time. We find that for two of the three product categories studied, search propensity does not change from month to month. However, in the third product category we find mild evidence of time-varying dynamics, where search decreases over time from already low levels. Finally, we model the level of a household's shopping activity and integrate it into our model of search. The results suggest that more-active online shoppers tend also to search across more sites. This consumer characteristic largely drives the dynamics of search that can easily be mistaken as increases from experience at the individual level.</description><author>Johnson, E. J.; Moe, W. W.; Fader, P. S.; Bellman, S.; Lohse, G. L.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Long-distance access network design</title><link>http://www.example.com/articles/1</link><description>Berger, R. T.; Raghavan, S.
Long-distance telephone companies in the United States pay access fees to local telephone companies to transport calls that originate and terminate on their networks. These charges form the largest portion of the cost of providing long-distance service. Recent changes in the structure of access rates, which were mandated by the Federal Communications Commission (FCC), have created opportunities for long-distance companies to better manage access costs. In this paper, we develop an optimization-based approach to the economic design of access networks. Our novel solution approach combines stochastic aspects of the problem with a challenging discrete facility location problem in a three-phase algorithm. Computational results indicate a potential cost savings of hundreds of millions of dollars annually for long-distance companies.</description><author>Berger, R. T.; Raghavan, S.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic conversion behavior at e-commerce site's</title><link>http://www.example.com/articles/1</link><description>Moe, W. W.; Fader, P. S.
T his paper develops a model of conversion behavior (i.e., converting store visits into purchases) that predicts each customer's probability of purchasing based on an observed history of visits and purchases. We offer an individual-level probability model that allows for different forms of customer heterogeneity in a very flexible manner. Specifically, we decompose an individual's conversion behavior into two components: one for accumulating visit effects and another for purchasing threshold effects. Each component is allowed to vary across households as well as over time. Visit effects capture the notion that store visits can play different roles in the purchasing process. For example, some visits are motivated by planned purchases, while others are associated with hedonic browsing (akin to window shopping); our model is able to accommodate these (and several other) types of visit-purchase relationships in a logical, parsimonious manner. The purchasing threshold captures the psychological resistance to online purchasing that may grow or shrink as a customer gains more experience with the purchasing process at a given website. We test different versions of the model that vary in the complexity of these two key components and also compare our general framework with popular alternatives such as logistic regression. We find that the proposed model offers excellent statistical properties, including its performance in a holdout validation sample, and also provides useful managerial diagnostics about the patterns underlying online buyer behavior.</description><author>Moe, W. W.; Fader, P. S.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Suckers are born but markets are made: Individual rationality, arbitrage, and market efficiency on an electronic futures market</title><link>http://www.example.com/articles/1</link><description>Oliven, K.; Rietz, T. A.
The Iowa Electronic Markets are specially designed futures markets that appear to aggregate information efficiently to predict events such as election outcomes. Yet, in theory, perfect information aggregation is impossible. Further, the markets are populated by a nonrepresentative sample of mistake-prone and biased traders. That is, traders are prone to the behavioral anomalies predicted by behavioral finance. How can this be reconciled with market efficiency? Here, we take a first step by analyzing the behavior of two self-selected types of traders. Dramatic differences in mistake rates across traders can help us answer the question. Market-making traders who set prices are less mistake prone and appear to be more rational than price-taking traders. This highlights an important feature of markets: marginal (in this case, market making), not average, traders set prices. This can drive the efficiency of market prices in spite of large numbers of traders who display patently suboptimal behavior.</description><author>Oliven, K.; Rietz, T. A.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Work groups, structural diversity, and knowledge sharing in a global organization</title><link>http://www.example.com/articles/1</link><description>Cummings, J. N.
Effective work groups engage in external knowledge sharing-the exchange of information, know-how, and feedback with customers, organizational experts, and others outside of the group. This paper argues that the value of external knowledge sharing increases when work groups are more structurally diverse. A structurally diverse work group is one in which the members, by virtue of their different organizational affiliations, roles, or positions, can expose the group to unique sources of knowledge. It is hypothesized that if members of structurally diverse work groups engage in external knowledge sharing, their performance will improve because of this active exchange of knowledge through unique external sources. A field study of 182 work groups in a Fortune 500 telecommunications firm operationalizes structural diversity as member differences in geographic locations, functional assignments, reporting managers, and business units, as indicated by corporate database records. External knowledge sharing was measured with group member surveys and performance was assessed using senior executive ratings. Ordered logit analyses showed that external knowledge sharing was more strongly associated with performance when work groups were more structurally diverse. Implications for theory and practice around the integration of work groups and social networks are addressed.</description><author>Cummings, J. N.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis and optimization of a multistage inventory-queue system</title><link>http://www.example.com/articles/1</link><description>Liu, L. M.; Liu, X. M.; Yao, D. D.
An important issue in the management of supply chains and manufacturing systems is to control inventory costs at different locations throughout the system while satisfying an end-customer service-level requirement. The challenge involved is to solve a nonlinear constrained optimization problem that captures the key dynamics of a complex production-inventory system. In this paper, we first develop a multistage inventory-queue model and a job-queue decomposition approach that evaluates the performance of serial manufacturing and supply systems with inventory control at every stage. We then present an efficient procedure to minimize the overall inventory in the system while meeting the required service level. Our technique is relatively simple and delivers accurate performance estimates. Furthermore, numerical studies generate certain managerial insights into related design and control issues.</description><author>Liu, L. M.; Liu, X. M.; Yao, D. D.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A polyhedral approach for the staff rostering problem</title><link>http://www.example.com/articles/1</link><description>Felici, G.; Gentile, C.
In this paper we formulate and efficiently solve staff scheduling problems for large organizations that provide continuous services to customers. We describe an integer programming approach for a class of such problems, where solutions have to obey a number of constraints related to workload balancing, shift compatibility and distribution of days off. The formulation of the constraints is general and can be extended to different personnel management problems where staff members must cover shifts, and management must assign a fixed number of days off per week. The model maximizes staff satisfaction, expressed by positive weights for pairs of shifts in consecutive days. We consider the associated polytope and study its structure, determining some classes of inequalities that are facet inducing for special subproblems and other valid classes. We also identify a particular subproblem whose solution can be used to determine strong cuts for the complete problem. In addition, we design special branching rules that break the symmetries that arise in the solution space and have a large impact in the efficiency of the method. The validity of this approach has been ascertained by extensive computational tests; moreover, the operations research (OR) department of an airline has implemented the method to solve ground staff management problems.</description><author>Felici, G.; Gentile, C.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Productivity effects of organizational change: Microeconometric evidence</title><link>http://www.example.com/articles/1</link><description>Bertschek, I.; Kaiser, U.
This paper analyzes the relationship between investment in information and communication technologies (ICT), non-ICT investment, labor productivity, and workplace reorganization. Firms are assumed to reorganize workplaces if the productivity gains arising from workplace reorganization exceed the associated reorganization costs. Two different types of organizational change are considered: enhancement group work and flattening of hierarchies. Empirical evidence is provided for a sample of 411 firms from the German business-related services sector. We develop and estimate a model for labor productivity and firms' decision to reorganize workplaces that allows workplace reorganization to affect any parameter of the labor productivity equation. Our general and flexible methodology allows us to properly take account of strategic complementarities between the input factors and workplace reorganization. The estimation results show that changes in human resources practices do not significantly affect firms' output elasticities with respect to ICT, non-ICT capital, and labor, although most of the point estimates of the individual output elasticities and of the control variables for observable firm heterogeneity are larger if workplace reorganization is realized. We therefore apply the Kernel density-estimation technique and demonstrate that for firms with organizational change, the entire labor productivity distribution shifts significantly out to the right if workplace reorganization takes place, indicating that workplace reorganization induces an increase in labor productivity that is attributable to complementarities between the various input factors and workplace reorganization. By contrast, firms without organizational change would not have realized significant productivity gains if they had reorganized workplaces.</description><author>Bertschek, I.; Kaiser, U.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multistage Monte Carlo method for solving influence diagrams using local computation</title><link>http://www.example.com/articles/1</link><description>Charnes, J. M.; Shenoy, P. P.
The main goal of this paper is to describe a new multistage Monte Carlo (MMC) simulation method for solving influence diagrams using local computation. Global methods have been proposed by others that sample from the joint probability distribution of all the variables in the influence diagram. However, for influence diagrams having many variables, the state space of all variables grows exponentially, and the sample sizes required for good estimates may be too large to be practical. In this paper, we develop a MMC method, which samples only a small set of chance variables for each decision node in the influence diagram. MMC is akin to methods developed for exact solution of influence diagrams in that we limit the number of chance variables sampled at any time. Because influence diagrams model each chance variable with a conditional probability distribution, the MMC method lends itself well to influence diagram representations.</description><author>Charnes, J. M.; Shenoy, P. P.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note - Commonality strategies: Value drivers and equivalence with flexible capacity and inventory substitution</title><link>http://www.example.com/articles/1</link><description>Van Mieghem, J. A.
Commonality strategies assemble different products from at least one common component and one other product-specific component. The distinguishing feature of commonality, i.e., the presence of dedicated components to be assembled with a common component, is shown to be mathematically inconsequential in the sense that the unified commonality problem for two products can be reduced to an equivalent substitution flexibility problem without those dedicated components. This significant simplification provides the first general, closed-form condition for commonality adoption and identifies its value drivers. Commonality is optimal even for perfectly correlated demands if products have sufficiently different margins. This introduces the "revenue-maximization option" of commonality as a second benefit that is independent of the traditional risk-pooling benefit. "Pure commonality" strategies are never optimal unless complexity costs are introduced. Dual sourcing, externalities, and operational hedging features of commonality are discussed.</description><author>Van Mieghem, J. A.</author><pubDate>Mon, 01 Mar 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on marketing and operations management interfaces and coordination</title><link>http://www.example.com/articles/1</link><description>Ho, T. H.; Tang, C. S.
This special issue, by addressing problems surrounding marketing and operations management, depicts state-of-the-art approaches, methodologies, and insights to improve a firm's or supply chain's overall performance. Top scholars in the field address many of the ways in which companies can synchronize their marketing and operations departments or their supply chain partners to improve competitiveness and profit. The information in this issue should be of interest both to academics and managers, and represents the current thoughts in an emerging area of marketing and operations interfaces.</description><author>Ho, T. H.; Tang, C. S.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Manufacturer benefits from information integration with retail customers</title><link>http://www.example.com/articles/1</link><description>Kulp, S. C.; Lee, H. L.; Ofek, E.
Information integration efforts between manufacturers and retailers, in the form of information sharing, synchronized replenishment, and collaborative product design and development, have been cited as major means to improve supply chain performance. This paper develops a conceptual framework that relates information-integration initiatives to manufacturer profitability. The framework allows such initiatives to impact inventory management and revenue-enhancing measures that, in turn, increase manufacturer profit margins, or affect profit margins directly. Through an extensive survey in the food and consumer packaged goods industry, we empirically examine this framework. The analysis reveals that the various integration techniques are differentially associated with manufacturer performance. Collaborative planning on replenishment, in the form of vendor-managed inventory (VMI), is directly and positively related to manufacturer margins, while collaboration on new products and services is positively related to intermediate performance measures. Specifically, this latter form of collaboration allows the manufacturer to charge higher wholesale prices and, interestingly, is associated with lower retailer, and consequently manufacturer, stockouts. In contrast, collaboration on the handling of excess and defective retailer inventory (i.e., reverse logistics) results in higher manufacturer stock-out levels, on average. Solely sharing information on either inventory levels or customer needs is associated with higher manufacturer performance measures up to a certain point; sharing this information is prevalent among manufacturers that achieve industry-average profitability relative to those that achieve below industry-average profitability. The paper explains these results in the context of the conceptual framework developed and discusses the managerial implications for effective coordination between supply chain partners.</description><author>Kulp, S. C.; Lee, H. L.; Ofek, E.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Retailer- vs. vendor-managed inventory and brand competition</title><link>http://www.example.com/articles/1</link><description>Mishra, B. K.; Raghunathan, S.
Vendor-managed inventory (VMI) is emerging as a significant development in the recent trend towards collaboration and information sharing in supply chain management. Transfer of inventory monitoring and other overhead costs to manufacturers and continuous replenishment of retailer inventory are commonly cited as potential benefits that VMI offers to retailers. We provide a new explanation in this paper for why retailers might be interested in VMI. We show that VMl intensifies the competition between manufacturers of competing brands and that the increased competition benefits a retailer that stocks these brands. Competition arises because of brand substitution; that is, some consumers may switch to another brand if their "preferred" brand is out of stock. The manufacturer whose brand is out of stock thus risks losing sales from those consumers who buy the competing brand. Consequently, each manufacturer has an incentive to keep a higher stock of its own brand, not only to satisfy the demand from its customers, but also the spillover demand that arises if a competing brand goes out of stock. When the retailer makes the stocking-level decisions, the competition is mitigated by the pooling of demands at the retailer. VMl restores the competition between the manufacturers and benefits the retailer.</description><author>Mishra, B. K.; Raghunathan, S.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain decision making: Will shorter cycle times and shared point-of-sale information necessarily help?</title><link>http://www.example.com/articles/1</link><description>Steckel, J. H.; Gupta, S.; Banerji, A.
Using a simulated supply chain experiment based on the well-known "beer game," we examine how changes in order and delivery cycles, availability of shared point-of-sale (POS) information, and the pattern of customer demand affect supply chain efficiency. We find that speeding up cycle time is beneficial, but the sharing of POS information is not necessarily so. Whether or not the sharing of POS information is beneficial depends on the nature of the demand pattern represented by the POS information. If the demand pattern conveys continual change in ultimate downstream customer demand (as does an S-shaped demand pattern), the POS information can distract the upstream decision maker from what is perhaps more immediately relevant information, orders placed by the proximate downstream agent and the supply line.</description><author>Steckel, J. H.; Gupta, S.; Banerji, A.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The benefits of advance booking discount programs: Model and analysis</title><link>http://www.example.com/articles/1</link><description>Tang, C. S.; Rajaram, K.; Alptekinoglu, A.; Ou, J. H.
Consider a retailer who sells perishable seasonal products with uncertain demand. Due to the short sales season and long replenishment lead times associated with such products, the retailer is unable to update demand forecasts by using actual sales data generated from the early part of the season and to respond by replenishing stocks during the season. To overcome this limitation, we examine the case in which the retailer develops a program called the "advance booking discount" (ABD) program that entices customers to commit to their orders at a discount price prior to the selling season. The time between placement and fulfillment of these precommitted orders provides an opportunity for the retailer to update demand forecasts by utilizing information generated from the precommitted orders and to respond by placing a cost-effective order at the beginning of the selling season. In this paper, we evaluate the benefits of the ABD program and characterize the optimal discount price that maximizes the retailer's expected profit.</description><author>Tang, C. S.; Rajaram, K.; Alptekinoglu, A.; Ou, J. H.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Setting customer expectation in service delivery: An integrated marketing-operations perspective</title><link>http://www.example.com/articles/1</link><description>Ho, T. H.; Zheng, Y. S.
Service firms have increasingly been competing for market share on the basis of delivery time. Many firms now choose to set customer expectation by announcing their maximal delivery time. Customers will be satisfied if their perceived delivery times are shorter than their expectations. This gap model of service quality is used in this paper to study how a firm might choose a delivery-time commitment to influence its customer expectation, and delivery quality in order to maximize its market share. A market share model is developed to capture (1) the impact of delivery-time commitment and delivery quality on the firm's market share and (2) the impact of the firm's market share and process variability on delivery quality when there is a congestion effect. We show that the choice of the delivery-time commitment requires a proper balance between the level of service capacity and customer sensitivities to delivery-time expectation and delivery quality. We prove the existence of Nash equilibria in a duopolistic competition, and show that this delivery-time commitment game is analogous to a Prisoners' Dilemma.</description><author>Ho, T. H.; Zheng, Y. S.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>When not all conflict is bad: Manufacturing-marketing conflict and strategic incentive design</title><link>http://www.example.com/articles/1</link><description>Balasubramanian, S.; Bhardwaj, P.
Researchers and managers broadly agree that coordination and harmony between manufacturing and marketing improve firm performance by eliminating suboptimal practices within the firm. In this paper, we present a contrasting view of the manufacturing-marketing interface. We model a duopoly in which the firms compete on price and quality dimensions. The manufacturing and marketing managers within each firm are presented with conflicting incentives focused on cost minimization and revenue maximization, respectively. These managers bargain with each other before arriving-at compromise decisions regarding price and quality. While frequently encountered in practice, this "conflicting-objectives puzzle" is surprising because one expects that centralized coordination by the owners of the firm towards profit maximization would lead to higher profits. In this paper, we resolve the conflicting-objectives puzzle and demonstrate that, surprisingly, the firm's resulting profits in this setting of conflict can be higher than those obtained when the decisions of the managers are perfectly coordinated. We also analyze the equilibrium in incentive plans when the owners can choose between compromise and perfect coordination. Our results offer a new interpretation of manufacturing-marketing conflict as a strategic tool that can enhance firm profits.</description><author>Balasubramanian, S.; Bhardwaj, P.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing capacity through reward programs</title><link>http://www.example.com/articles/1</link><description>Kim, B. D.; Shi, M. Z.; Srinivasan, K.
Rewarding customers with own products or services has become an increasingly popular practice across a spectrum of industries such as airlines, hotels, and telecommunication. In these service industries, firms face demand uncertainty and strict short-term capacity constraint. When the market demand is low, firms hold excess capacities that would lead to intense price competition. In this paper we study the adoption and design of reward programs in the context of capacity management. We demonstrate that it is optimal for firms to offer capacity rewards when the market demand varies from one period to the other. By offering the reward programs, firms can effectively reduce available capacities when the market demand is low, and hence credibly show their unwillingness to undersell. Such a commitment can encourage their competitors to set their prices high. When firms provide reward programs, if a firm sets a higher price than the other and sells less today, in the future the firm can benefit from the other firm's larger reduction in available capacity through rewards. Thus, reward programs also provide additional incentives for firms to set higher current prices. Finally, since reward programs can add flexibility in adjusting the available capacities to the market demand, firms increase the size of regular capacities with reward programs.</description><author>Kim, B. D.; Shi, M. Z.; Srinivasan, K.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Doing the right thing or doing the thing right: Allocating resources between marketing research and manufacturing</title><link>http://www.example.com/articles/1</link><description>Hess, J. D.; Lucas, M. T.
Matching production with sales potential is essential for survival in volatile markets. Manufacturing and marketing managers compete for staff, space, cash, and other assets as they struggle both to determine what and how many products ought to be produced, and to actually produce them. We develop an analytical framework to answer one simple question, "How much marketing research should a firm do when it takes resources away from manufacturing the goods that generate revenue?" To understand the costs and benefits of marketing research, we account for the lost opportunities to produce these goods. Some analytical findings are striking: firms without initial knowledge of their potential customers should allocate one-third of the firm's resources to marketing research. The model suggests a host of issues to be more deeply studied by management scientists.</description><author>Hess, J. D.; Lucas, M. T.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing NPD: Cost and schedule performance in design and manufacturing</title><link>http://www.example.com/articles/1</link><description>Bajaj, A.; Kekre, S.; Srinivasan, K.
In this field study, conducted at a leading avionics guidance systems manufacturer, we gathered primary data on time and cost performance of both the design and manufacturing phases of new product development (NPD). We modeled the impact of the management levers relating to oversight, the intensity of design specialization, and the level of interaction with the customer. The study highlights the necessity of leveraging the interdependencies between the design and manufacturing phases in NPD.</description><author>Bajaj, A.; Kekre, S.; Srinivasan, K.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>New-product strategy and industry clockspeed</title><link>http://www.example.com/articles/1</link><description>Souza, G. C.; Bayus, B. L.; Wagner, H. M.
We study how industry clockspeed, internal firm factors, such as product development, production, and inventory costs, and competitive factors determine a firm's optimal new-product introduction timing and product-quality decisions. We explicitly model market demand uncertainty, a firm's internal cost structure, and competition, using an infinite-horizon Markov decision process. Based on a large-scale numerical analysis, we find that more frequent new-product introductions are optimal under faster clockspeed conditions. In addition, we find that a firm's optimal product-quality decision is governed by a firm's relative costs of introducing new products with incremental versus more substantial improvements. We show that a time-pacing product introduction strategy results in a production policy with a simple base-stock form and performs well relative to the optimal policy Our results thus provide analytical support for the managerial belief that industry clockspeed and time to market are closely related.</description><author>Souza, G. C.; Bayus, B. L.; Wagner, H. M.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Designing supply contracts: Contract type and information asymmetry</title><link>http://www.example.com/articles/1</link><description>Corbett, C. J.; Zhou, D. M.; Tang, C. S.
T his paper studies the value to a supplier of obtaining better information about a buyer's cost structure, and of being able to offer more general contracts. We use the bilateral monopoly setting to analyze six scenarios: three increasingly general contracts (wholesale-pricing schemes, two-part linear schemes, and two-part nonlinear schemes), each under full and incomplete information about the buyer's cost structure. We allow both sides to refuse to trade by explicitly including reservation profit levels for both; for the supplier, this is implemented through a cutoff policy. We derive the supplier's optimal contracts and profits for all six scenarios and examine the value of information and of more general contracts. Our key findings are as follows: First, the value of information is higher under two-part contracts; second, the value of offering two-part contracts is higher under full information; and third, the proportion of buyers the supplier will choose to exclude can be substantial.</description><author>Corbett, C. J.; Zhou, D. M.; Tang, C. S.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multiperson tournaments: An experimental examination</title><link>http://www.example.com/articles/1</link><description>Orrison, A.; Schotter, A.; Weigelt, K.
Modern hierarchical organizations, like corporations, must motivate agents to work hard. Given their pyramid structure, it is not surprising that one commonly used motivator is the promotion tournament. In such tournaments, agents compete to advance to positions at higher organizational levels. Though these tournaments are common, little research has empirically looked at the interface of organizational structure and tournament design. This paper aims to take a step in filling this void by comparing the performance of various tournament designs using controlled laboratory techniques.</description><author>Orrison, A.; Schotter, A.; Weigelt, K.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fifty years of Management Science</title><link>http://www.example.com/articles/1</link><description>Hopp, W. J.
This issue marks the start of the 50th volume of Management Science. As is customary on such "round number" occasions, we will be taking time for a bit of retrospective review and soul-searching speculation. We begin here with an overall assessment of the journal's performance relative to its original mission. In subsequent articles, which will appear in issues throughout Volume 50, we will take more detailed disciplinary looks at the past, present, and future of Management Science and the management subfields it represents.</description><author>Hopp, W. J.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Five decades of operations management and the prospects ahead</title><link>http://www.example.com/articles/1</link><description>Chopra, S.; Lovejoy, W.; Yano, C.
Operations and Supply Chains is the current title for a department that has evolved through several different titles in recent years, reflecting its evolving mission from a focus on classical operations research at the time of ORSXs founding 50 years ago toward art embrace of a broader body of theory. Throughout this evolution, the focus on applied problems and the goal of improving practice through the development of suitable theory has remained constant. The Operations and Supply Chains Department promotes the theory underlying the practice of operations management, which encompasses the design and management of the transformation processes in manufacturing and service organizations that create value for society. Operations is the function that is uniquely associated with the design and management of these processes. The problem domains of concern to the department have been, and remain, the marshalling of inputs, the transformation itself, and the distribution of outputs in pursuit of this value-creating end. Over the past 50 years the department has had a variety of titles, reflecting an evolving understanding of the boundaries of the operations function. In this article we celebrate past accomplishments, identify current challenges, and anticipate a future that is as exciting and opportunity-rich as any our field has seen.</description><author>Chopra, S.; Lovejoy, W.; Yano, C.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management under a general discrete choice model of consumer behavior</title><link>http://www.example.com/articles/1</link><description>Talluri, K.; van Ryzin, G.
Customer choice behavior, such as buy-up and buy-down, is an important phenomenon in a wide range of revenue management contexts. Yet most revenue management methodologies ignore this phenomenon - or at best approximate it in a heuristic way. In this paper, we provide an exact and quite general analysis of this problem. Specifically, we analyze a single-leg reserve management problem in which the buyers' choice behavior is modeled explicitly. The choice model is very general, simply specifying the probability of purchase for each fare product as a function of the set of fare products offered. The control problem is to decide which subset of fare products to offer at each point in time. We show that the optimal policy for this-problem has a quite simple form. Namely, it consists of identifying an ordered family of "efficient" subsets S-1,...,S-m, and at each point in time,opening one of these sets S-k, where,the optimal. index k is increasing in the remaining capacity x and decreasing in the remaining time. That is, the more capacity (or less time) available, the further the optimal set is along this,sequence. We also show that the optimal policy is a nested allocation policy if and only if the sequence of efficient sets is nested, that is S-1 subset of or equal to S-2 subset of or equal to ... subset of or equal to S-m. Moreover, we give a characterization of when nesting by fare order is optimal. We also develop an estimation procedure for this setting based on the expectation-maximization (EM) method that jointly estimates arrival rates and choice model parameters when no-purchase outcomes are unobservable. Numerical results are given to illustrate both the model and estimation procedure.</description><author>Talluri, K.; van Ryzin, G.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Channel performance under consignment contract with revenue sharing</title><link>http://www.example.com/articles/1</link><description>Wang, Y. Z.; Jiang, L.; Shen, Z. J.
Under a consignment contract with revenue sharing, a supplier decides on the retail price and delivery quantity for his product, and retains ownership of the goods; for each item sold, the retailer deducts a percentage from the selling price and remits the balance to the supplier. In this paper we show that, under such a contract, both the overall channel performance and the performance of individual firms depend critically on demand price elasticity and on the retailer's share of channel cost. In particular, the (expected) channel profit loss, compared with that of a centralized system, increases with demand price elasticity and decreases with retailer's cost share, while the profit share extracted by the retailer decreases with price elasticity and increases with retailer's cost share. With an iso-price-elastic demand model, we show that the channel profit loss cannot exceed 26.4%, and that the retailer's profit share cannot be below 50%. When price elasticity is low, or when the retailer's cost share approaches 100%, or both, the retailer can extract nearly all the channel profit that is almost equal to the centralized channel profit.</description><author>Wang, Y. Z.; Jiang, L.; Shen, Z. J.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordinating contracts for decentralized supply chains with retailer promotional effort</title><link>http://www.example.com/articles/1</link><description>Krishnan, H.; Kapuscinski, R.; Butz, D. A.
In this paper, a risk-neutral manufacturer sells a single product to a risk-neutral retailer. The retailer chooses inventories ex ante and promotional effort ex post. If the wholesale price exceeds marginal production cost, the retailer orders fewer than the joint profit-maximizing inventories. If the manufacturer attempts to coordinate inventories by buying back unsold units, then the retailer's promotional incentives are dulled. Under very general assumptions on the form of the effort function, we show that buy-backs adversely affect supply chain profits, and higher buy-back prices imply lower profits. Also, while a buy-back alone cannot coordinate the channel, coupling buy-backs with promotional cost-sharing agreements (if effort cost is observable), offering unilateral markdown allowances ex post (if demand is observable but not verifiable), or placing additional constraints on the buy-back (if demand is observable and verifiable) does result in coordination. This problem is not limited to returns policies but is shown to hold for a much larger set of contracts. The results are quite robust (e.g., when the retailer chooses effort before observing demand), but coordinating contracts become more problematic if, for example, the retailer also stocks substitutes for the manufacturer's product. Other model extensions are also discussed.</description><author>Krishnan, H.; Kapuscinski, R.; Butz, D. A.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply contracts, profit sharing, switching, and reaction options</title><link>http://www.example.com/articles/1</link><description>Kamrad, B.; Siddique, A.
A common theme in the studies of flexible supply contracts has been the producer's profit-maximization problem without regard to the suppliers' reactions. However, suppliers do react and protect their downside against producer's operating policies by revising their strategies in a manner consistent with their profit-maximization objectives. This fact motivates our work. Using a real-options (contingent claims) approach, we analyze and value supply contracts in a setting characterized by exchange rate uncertainty, supplier-switching options, order-quantity flexibility, profit sharing, and supplier reaction options. We also use basic diversification concepts, from portfolio theory, to analyze risk reduction in a unique framework. Given this setup, we explicitly model how flexibility can be mutually beneficial to both the producer and the suppliers. Using this model, we concurrently solve and examine the dual optimization problem for the suppliers and the producer. Our approach also endogenizes the extent and degree of profit sharing through the resulting optimal policies. We also analyze what induces the producer and the suppliers to accept flexibility in their contracts.</description><author>Kamrad, B.; Siddique, A.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Benefits of skill chaining in serial production lines with cross-trained workers</title><link>http://www.example.com/articles/1</link><description>Hopp, W. J.; Tekin, E.; Van Oyen, M. P.
To gain insight into the potential logistical benefits of worker Cross-training and agile workforce policies, we study simple models of serial production systems with flexible servers operating under a constant work-in-process (CONWIP) release policy. Two important and interrelated issues are: (a) how to decide which skill(s) are strategically most desirable for workers to gain, and (b) how to coordinate these workers to respond dynamically to congestion. We address these by considering two cross-training strategies: a straightforward capacity-balancing approach, which, we call cherry picking (CP), and an innovative overlapping zone strategy that we call skill chaining. Our comparison shows that skill-chaining strategies have the potential to be robust and efficient methods for implementing workforce agility in serial production lines.</description><author>Hopp, W. J.; Tekin, E.; Van Oyen, M. P.</author><pubDate>Thu, 01 Jan 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stochastic simulation research in Management Science</title><link>http://www.example.com/articles/1</link><description>Nelson, B. L.
When the simulation department of Management Science was created in 1978 it ushered in an era of significant methodogolgical advances in stochastic simulation. However, the foundation for the field-not just the work that has been published in Management Science-was provided by two papers published long before simulation had its own department in the journal. We will review the seminal papers of Conway, Johnson, and Maxwell (1959) and Conway (1963), and then trace their impact through eight award-winning papers that appeared much later in Management Science.</description><author>Nelson, B. L.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Ordering and inventory policies for step changes in the unit item cost: A discounted cash flow approach</title><link>http://www.example.com/articles/1</link><description>Grubbstrom, R. W.; Kingsman, B. G.
This paper considers the problem of determining the optimal ordering quantities of a purchased item where there are step changes in price, either up or down. Other costs incurred include ordering costs associated with each replenishment and holding costs related to capital tied up in inventory and physical stock holding. The net present value (NPV) principle is applied. Explicit expressions for the development of the optimal order quantities over time are presented. It is shown that three cases may be distinguished: (i) when the price change is very small, (ii) when an essential price increase occurs, and (iii) when there is an essential price decrease. Although the optimal last-order quantity before a price increase is similar in magnitude to what has been presented in other articles applying average cost approaches, in certain respects, this paper offers novel results contradictory to those suggested by other authors. Analysis shows that the average-cost model solutions are first-order approximations in the discount rate. Numerical evaluations of a range of price increases and times to the price increase suggest that, with certain important caveats, the average-cost formulae are likely to be acceptable for most practical situations for the infinite horizon situation.</description><author>Grubbstrom, R. W.; Kingsman, B. G.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Closed-loop supply chain models with product remanufacturing</title><link>http://www.example.com/articles/1</link><description>Savaskan, R. C.; Bhattacharya, S.; Van Wassenhove, L. N.
The importance of remanufacturing used products into new ones has been widely recognized in the literature and in practice. In this paper, we address the problem of choosing the appropriate reverse channel structure for the collection of used products from customers. Specifically, we consider a manufacturer who has three options for collecting such products: (1) she can collect them herself directly from the customers, (2) she can provide suitable incentives to an existing retailer (who already has a distribution channel) to induce the collection, or (3) she can subcontract the collection activity to a third party. Based on our observations in the industry, we model the three options described above as decentralized decision-making systems with the manufacturer being the Stackelberg leader. When considering decentralized channels, we find that ceteris paribus, the agent, who is closer to the customer (i.e., the retailer), is the most effective undertaker of product collection activity for the manufacturer. In addition, we show that simple coordination mechanisms can be designed such that the collection effort of the retailer and the supply chain profits are attained at the same level as in a centrally coordinated system.</description><author>Savaskan, R. C.; Bhattacharya, S.; Van Wassenhove, L. N.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The allocation of inventory risk in a supply chain: Push, pull, and advance-purchase discount contracts</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.
While every firm in a supply chain bears supply risk (the cost of insufficient supply), some firms may, even with wholesale price contracts, completely avoid inventory risk (the cost of unsold inventory). With a push contract there is a single wholesale price and the retailer, by ordering his entire supply before the selling season, bears all of the supply chain's inventory risk. A pull contract also has a single wholesale price, but the supplier bears the supply chain's inventory risk because only the supplier holds inventory while the retailer replenishes as needed during the season. (Examples include Vendor Managed Inventory with consignment and drop shipping.) An advance-purchase discount has two wholesale prices: a discounted price for inventory purchased before the season, and a regular price for replenishments during the selling season. Advance-purchase discounts allow for intermediate allocations of inventory risk: The retailer bears the risk on inventory ordered before the season while the supplier bears the risk on any production in excess of that amount. This research studies how the allocation of inventory risk (via these three types of wholesale price contracts) impacts supply chain efficiency (the ratio of the supply chain's profit to its maximum profit). It is found that the efficiency of a single wholesale price contract is considerably higher than previously thought as long as firms consider both push and pull contracts. In other words, the literature has exaggerated the value of implementing coordinating contracts (i.e., contracts that achieve 100% efficiency, such as buy-backs or revenue sharing) because coordinating contracts are compared against an inappropriate benchmark (often just a push contract). Furthermore, if firms also consider advance-purchase discounts, which are also simple to administer, then the coordination of the supply chain and the arbitrary allocation of its profit is possible. Several limitations of advance-purchase discounts are discussed.</description><author>Cachon, G. P.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Industry risk and market integration</title><link>http://www.example.com/articles/1</link><description>Carrieri, F.; Errunza, V.; Sarkissian, S.
Traditionally, integration has been studied at the country level. With increasing economic integration, industrial reorganization, and blurring of national boundaries (e.g., European Union (EU)), it is important to investigate global integration at the industry level. We argue that country-level integration (segmentation) does not preclude industry-level segmentation (integration). Indeed, our results suggest that a country is integrated with (segmented from) the world capital markets only if most of her industries are integrated (segmented). We also show that although global industry risk is small, it can be priced for certain industries. Industries that are priced differently from either the world or domestic markets represent incremental opportunities for international diversification.</description><author>Carrieri, F.; Errunza, V.; Sarkissian, S.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Identifying innovators for the cross-selling of new products</title><link>http://www.example.com/articles/1</link><description>Kamakura, W. A.; Kossar, B. S.; Wedel, M.
With recent advances in information technology, most companies are amassing extensive customer databases. The wealth of information in these databases can be useful in identifying those customers most likely to purchase a new product and in predicting when this adoption may take place. This can assist database marketers in determining when individuals should be targeted for the promotion of a new product, which may increase the efficiency of manufacturing and distribution, and assure a faster return on investments. For this purpose, we propose a model that considers the timing of past purchases across multiple product categories and produces estimates of each customer's propensity of ever purchasing in a particular product category and of the timing of their purchases. The model is designed to help managers identify the best prospects for a new offer in one of multiple categories based on generalizations obtained from past offers. The proposed model also provides projections of aggregate penetration for new brands within the database, based on sample estimates.</description><author>Kamakura, W. A.; Kossar, B. S.; Wedel, M.</author><pubDate>Sun, 01 Aug 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technological innovation, product development, and entrepreneurship in Management Science</title><link>http://www.example.com/articles/1</link><description>Shane, S. A.; Ulrich, K. T.
This article is a review of work published in Management Science on the topics of technological innovation, product development, and entrepreneurship since the inception of the journal in 1954. We intend the article to serve two goals. First, we hope that it will be useful to doctoral students and researchers interested in understanding what questions have been addressed in Management Science in the area of innovation. Second, we hope that the article will be useful to sociologists of science who are interested in understanding how knowledge develops in a field. We organize the literature into 12 themes. We then describe some aggregate properties of the articles. In an online supplement, we present brief summaries of the 250 articles we have identified as falling in the domain of our department.</description><author>Shane, S. A.; Ulrich, K. T.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Collaborative prototyping and the pricing of custom-designed products</title><link>http://www.example.com/articles/1</link><description>Terwiesch, C.; Loch, C. H.
A major challenge in the creation of custom-designed products lies in the elicitation of customer needs. As customers are frequently unable to accurately articulate their needs, designers typically create one or several prototypes, which they then present to the customer. This process, which we call collaborative prototyping, allows both parties to anticipate the outcome of the design process. Prototypes have two advantages: They help the customer to evaluate the unknown customized product, and they guide both parties in the search for the ideal product specification. Collaborative prototyping involves two economic agents, with different information structures and different-and potentially conflicting-objective functions. This raises several interesting questions: how many prototypes should be built, who should pay for them, and how should they and the customized product be priced. We show that, depending on the design problem and the market characteristics, the designer should offer prototypes at a profit, at cost, or even for free.</description><author>Terwiesch, C.; Loch, C. H.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modularity and innovation in complex systems</title><link>http://www.example.com/articles/1</link><description>Ethiraj, S. K.; Levinthal, D.
The problem of designing, coordinating, and managing complex systems has been central to the management and organization literature. Recent writings have tended to offer modularity as at least a partial solution to this design problem. However, little attention has been paid to the problem of identifying what constitutes an appropriate modularization of a complex system. We develop a formal simulation model that allows us to carefully examine the dynamics of innovation and performance in complex systems. The model points to the trade-off between the destabilizing effects of overly refined modularization and the modest levels of search and a premature fixation on inferior designs that can result from excessive levels of integration. The analysis highlights an asymmetry in this trade-off, with excessively refined modules leading to cycling behavior and a lack of performance improvement. We discuss the implications of these arguments for product and organization design.</description><author>Ethiraj, S. K.; Levinthal, D.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge reuse for innovation</title><link>http://www.example.com/articles/1</link><description>Majchrzak, A.; Cooper, L. P.; Neece, O. E.
This study was conducted to better understand the knowledge reuse process when radical innovation (e.g., experiments to prepare for human exploration of Mars) is expected. The research involved detailing the knowledge reuse process in six case studies varying in degree of innovation. Across the six cases, a six-stage reuse-for-innovation process was identified consisting of three major actions: reconceptualize the problem and approach, including deciding to search for others' ideas to reuse; search-and-evaluate others' ideas to reuse; and develop the selected idea. Findings include (1) the need for an insurmountable gap in performance to stimulate the decision to reuse others' knowledge; (2) the critical importance of an adapter to bridge the idea source and recipient; (3) three layers of search-and-evaluate activities in which the first layer of scanning to find ideas to reuse and the last layer of detailed analysis of ideas are bridged by a layer of brief evaluations of ideas assessing the presence (or absence) of targeted information about each idea; and (4) the differential use of metaknowledge about each idea to facilitate proceeding through each search-and-evaluate layer. In addition, reusers in the more (versus less) innovative cases redefined problems at the outset in nontraditional ways using analogies and extensions, rather than accepting the preexisting problem definition; used a substantially broader search strategy with a greater variety of search methods; and worked more closely with adapters during the latter stages of the reuse process.</description><author>Majchrzak, A.; Cooper, L. P.; Neece, O. E.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Designing a better shopbot</title><link>http://www.example.com/articles/1</link><description>Montgomery, A. L.; Hosanagar, K.; Krishnan, R.; Clay, K. B.
A primary tool that consumers have for comparative shopping is the shopbot, which is short for shopping robot. These shopbots automatically search a large number of vendors for price and availability. Typically a shopbot searches a predefined set of vendors and reports all results, which can result in time-consuming searches that provide redundant or dominated alternatives. Our research demonstrates analytically how shopbot designs can be improved by developing a utility model of consumer purchasing behavior. This utility model considers the intrinsic value of the product and its attributes, the disutility from waiting, and the cognitive costs associated with evaluating the offers retrieved. We focus on the operational decisions made by the shopbot: which stores to search, how long to wait, and which offers to present to the user. To illustrate our model we calibrate the model to price and response time data collected at online bookstores over a six-month period. Using prior expectations about price and response time, we show how shopbots can substantially increase consumer utility by searching more intelligently and then selectively presenting offers.</description><author>Montgomery, A. L.; Hosanagar, K.; Krishnan, R.; Clay, K. B.</author><pubDate>Sun, 01 Feb 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The marketing department in Management Science: Its history, contributions, and the future</title><link>http://www.example.com/articles/1</link><description>Morrison, D. G.; Raju, J. S.
This year marks the 50th anniversary of Management Science. We take this opportunity to trace the history of the Marketing Department in Management Science, outline the role that the Marketing Department has played in supporting management science research in Marketing, its impact on the field, and its plans for the future.</description><author>Morrison, D. G.; Raju, J. S.</author><pubDate>Thu, 01 Apr 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Self-interested routing in queueing networks</title><link>http://www.example.com/articles/1</link><description>Parlakturk, A. K.; Kumar, S.
We study self-interested routing in stochastic networks, taking into account the discrete stochastic dynamics of such networks. We analyze a two-station multiclass queueing network in which the system manager chooses the scheduling rule and individual customers choose routes in a self-interested manner. We show that this network can be unstable in Nash equilibrium under some scheduling rules. We also design a nontrivial scheduling rule that negates the performance degradation resulting from self-interested routing and achieves a Nash equilibrium with performance comparable to the first-best solution.</description><author>Parlakturk, A. K.; Kumar, S.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capacity expansion for random exponential demand growth with lead times</title><link>http://www.example.com/articles/1</link><description>Ryan, S. M.
The combination of demand uncertainty and a lead time for adding capacity creates the risk of capacity shortage during the lead time. formulate a model of capacity expansion for uncertain exponential demand growth and deterministic expansion lead times when there is an obligation to provide a specified level of service. The service level, defined in terms of the ratio of expected lead-time shortage to installed capacity, is guaranteed by timing each expansion to begin when demand reaches a fixed proportion of the capacity position. Under this timing rule, the optimal facilities to install can be determined by solving an equivalent deterministic problem without lead times. Numerical results show the effects of the demand parameters and lead-time length on the expansion timing. The interaction of timing with expansion size is explored for the case when continuous facility sizes are available with economies of scale.</description><author>Ryan, S. M.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selection, provisioning, shared fixed costs, maximum closure, and implications on algorithmic methods today</title><link>http://www.example.com/articles/1</link><description>Hochbaum, D. S.
Motivated by applications in freight handling and open-pit mining, Rhys, Balinski, and Picard studied the problems of selection and closure in papers published in Management Science in 1970 and 1976. They identified efficient algorithms based on linear programming and maximum-flow/minimum-cut procedures to solve these problems. This research has had major impact well beyond the initial applications, reaching across three decades and inspiring work on numerous applications and extensions. The extensions are nontrivial optimization problems that are of theoretical interest. The applications ranged from evolving technologies, image segmentation, revealed preferences, pricing, adjusting utilities for consistencies, just-in-time production, solving certain integer programs in polynomial time, and providing efficient 2-approximation algorithms for a wide variety of hard problems. A recent generalization to a convex objective function has even produced novel solutions to prediction and Bayesian estimation problems. This paper surveys the streams of research stimulated by these papers as an example of the impact of Management Science on the optimization field and an illustration of the far-reaching implications of good original research.</description><author>Hochbaum, D. S.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and priority auctions in queueing systems with a generalized delay cost structure</title><link>http://www.example.com/articles/1</link><description>Afeche, P.; Mendelson, H.
This paper studies alternative price-service mechanisms for a provider that serves customers whose delay cost depends on their service valuations. We propose a generalized delay cost structure that augments the standard additive model with a multiplicative component, capturing the interdependence between delay cost and values. We derive and compare the revenue-maximizing and socially optimal equilibria under uniform pricing, preemptive, and nonpreemptive priority auctions with an admission price. We find that the delay cost structure has a paramount effect on system behavior. The classical result that the revenue-maximizing admission price is higher and the utilization lower than is socially optimal can be reversed under our generalized structure, and we identify the conditions driving this reversal under each mechanism. We show that the conditional bid equilibria are unique and induce the socially optimal allocations. The auctions yield gains in system net value and provider profit over uniform pricing, which are dramatically larger for the preemptive mechanism. Both auctions perform better under multiplicative compared to additive delay costs. The highest-value customers always gain under the preemptive, but may lose under the nonpreemptive auction. The lowest-value customers always gain in either auction.</description><author>Afeche, P.; Mendelson, H.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A fast cross-entropy method for estimating buffer overflows in queueing networks</title><link>http://www.example.com/articles/1</link><description>de Boer, P. T.; Kroese, D. P.; Rubinstein, R. Y.
In this paper, we propose a fast adaptive importance sampling method for the efficient simulation of buffer overflow probabilities in queueing networks. The method comprises three stages. First, we estimate the minimum cross-entropy tilting parameter for a small buffer level; next, we use this as a starting value for the estimation of the optimal tilting parameter for the actual (large) buffer level. Finally, the tilting parameter just found is used to estimate the overflow probability of interest. We study various properties of the method in more detail for the M/M/1 queue and conjecture that similar properties also hold for quite general queueing networks. Numerical results support this conjecture and demonstrate the high efficiency of the proposed algorithm.</description><author>de Boer, P. T.; Kroese, D. P.; Rubinstein, R. Y.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling daily arrivals to a telephone call center</title><link>http://www.example.com/articles/1</link><description>Avramidis, A. N.; Deslauriers, A.; L'Ecuyer, P.
We develop stochastic models of time-dependent arrivals, with focus on the application to call centers. Our models reproduce three essential features of call center arrivals observed in recent empirical studies: a variance larger than the mean for the number of arrivals in any given time interval, a time-varying arrival intensity over the course of a day, and nonzero correlation between the arrival counts in different periods within the same day. For each of the new models, we characterize the joint distribution of the vector of arrival counts, with particular focus on characterizing how the new models are more flexible than standard or previously proposed models. We report empirical results from a study on arrival data from a real-life call center, including the essential features of the arrival process, the goodness of fit of the estimated models, and the sensitivity of various simulated performance measures of the call center to the choice of arrival process model.</description><author>Avramidis, A. N.; Deslauriers, A.; L'Ecuyer, P.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Valuation of commodity-based swing options</title><link>http://www.example.com/articles/1</link><description>Jaillet, P.; Ronn, E. I.; Tompaidis, S.
In the energy markets, in particular the electricity and natural gas markets, many contracts incorporate flexibility-of-delivery options known as "swing" or "take-or-pay" options. Subject to daily as well as periodic constraints, these contracts permit the option holder to repeatedly exercise the right to receive greater or smaller amounts of energy We extract market information from forward prices and volatilities and build a pricing framework for swing options based on a one-factor mean-reverting stochastic process for energy prices that explicitly incorporates seasonal effects. We present a numerical scheme for the valuation of swing options calibrated for the case of natural gas.</description><author>Jaillet, P.; Ronn, E. I.; Tompaidis, S.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Random walks and sustained competitive advantage</title><link>http://www.example.com/articles/1</link><description>Denrell, J.
Strategy is concerned with sustained interfirm profitability differences. Observations of such sustained differences are often attributed to unobserved systematic a priori differences in firm characteristics. This paper shows that sustained interfirm profitability differences may be very likely even if there are no a priori differences among firms. As a result of the phenomenon of long leads in random walks, even a random resource accumulation process is likely to produce persistent resource heterogeneity and sustained interfirm profitability differences. A Cournot model in which costs follow a random walk shows that such a process could produce evidence of substantial persistence of profitability The results suggest that persistent profitability does not necessarily provide strong evidence for systematic a priori differences among firms. Nevertheless, since the phenomenon of long leads is highly unrepresentative of intuitive notions of random sequences, such evidence may still be persuasive.</description><author>Denrell, J.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pre-IPO operational and financial decisions</title><link>http://www.example.com/articles/1</link><description>Babich, V.; Sobel, M. J.
Many owners of growing privately held firms make operational and financial decisions in an effort to maximize the expected present value of the proceeds from an initial public offering (IPO). We ask: "What is the right time to make an IPO?" and "How should operational and financial decisions be coordinated to increase the likelihood of a successful IPO?" Financial and operational decisions in this problem are linked because adequate financial capital is crucial for operational decisions to be feasible and operational decisions affect the firm's access to financial resources. The IPO event is treated as a stopping time in an infinite-horizon discounted Markov decision process. Unlike traditional stopping-time models, at every stage the model includes other decisions such as production, sales, and loan size. The results include (1) characterization of an optimal capacity-expansion policy, (2) sufficient conditions for a monotone threshold rule to yield an optimal IPO decision, and (3) algorithmic implications of results in (1) and (2).</description><author>Babich, V.; Sobel, M. J.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Assessing data quality for information products: Impact of selection, projection, and Cartesian product</title><link>http://www.example.com/articles/1</link><description>Parssian, A.; Sarkar, S.; Jacob, V. S.
The cost associated with making decisions based on poor-quality data is quite high. Consequently, the management of data quality and the quality of associated data management processes has become critical for organizations. An important first step in managing data quality is the ability to measure the quality of information products (derived data) based on the quality of the source data and associated processes used to produce the information outputs. We present a methodology to determine two data quality characteristics-accuracy and completeness-that are of critical importance to decision makers. We examine how the quality metrics of source data affect the quality for information outputs produced using the relational algebra operations selection, projection, and Cartesian product. Our methodology is general, and can be used to determine how quality characteristics associated with diverse data sources affect the quality of the derived data.</description><author>Parssian, A.; Sarkar, S.; Jacob, V. S.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Eliminating public knowledge biases in information-aggregation mechanisms</title><link>http://www.example.com/articles/1</link><description>Chen, K. Y.; Fine, L. R.; Huberman, B. A.
We present a novel methodology for identifying public knowledge and eliminating the biases it creates when aggregating information in small group settings. A two-stage mechanism consisting of an information market and a coordination game is used to reveal and adjust for individuals' public information. A nonlinear aggregation of their decisions then allows for the calculation of the probability of the future outcome of an uncertain event, which can then be compared to both the objective probability of its occurrence and the performance of the market as a whole. Experiments show that this nonlinear aggregation mechanism outperforms both the imperfect market and the best of the participants.</description><author>Chen, K. Y.; Fine, L. R.; Huberman, B. A.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Markov perfect equilibrium advertising strategies of Lanchester duopoly model: A technical note</title><link>http://www.example.com/articles/1</link><description>Jarrar, R.; Martin-Herran, G.; Zaccour, G.
We propose a numerical approach to compute stationary Markov perfect Nash equilibrium advertising strategies of the Lanchester model. The algorithm can be implemented using a standard mathematical package, and, importantly, it does not require that the players discount their future earnings at a zero rate, an assumption that has been made in the literature.</description><author>Jarrar, R.; Martin-Herran, G.; Zaccour, G.</author><pubDate>Thu, 01 Jul 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Long-range reserve crew manpower planning</title><link>http://www.example.com/articles/1</link><description>Sohoni, M. G.; Johnson, E. L.; Bailey, T. G.
Airlines are continually faced with the challenge of efficient utilization of their cockpit crew resources. In addition to regular flying crews, airlines maintain significant reserve staffing levels to meet contractual obligations and provide smooth daily operations. Most airlines also depend on voluntary and involuntary flying by regular crews to cover trips that fall out of work schedules due to conflicts and disruptions (open time trips). These open time trips, combined with expected levels of voluntary and involuntary flying, affect reserve staffing levels; airlines must plan crew staffing in advance to meet resulting training and new-hire requirements. Inefficient operational reserve utilization can further affect long-range crew staffing, resulting in higher training and new-hire costs. The proposed optimization strategy to estimate long-range crew staffing combines operational reserve utilization and premium operational costs due to voluntary and involuntary flying with long-range business needs.</description><author>Sohoni, M. G.; Johnson, E. L.; Bailey, T. G.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A multi-exchange heuristic for the single-source capacitated facility location problem</title><link>http://www.example.com/articles/1</link><description>Ahuja, R. K.; Orlin, J. B.; Pallottino, S.; Scaparra, M. P.; Scutella, M. G.
We present a very large-scale neighborhood (VLSN) search algorithm for the capacitated facility location problem with single-source constraints. The neighborhood structures are induced by customer multiexchanges and by facility moves. We consider both traditional single-customer multi-exchanges, detected on a suitably defined customer improvement graph, and more innovative multicustomer multi-exchanges, detected on a facility improvement graph dynamically built through the use of a greedy scheme. Computational results for some benchmark instances are reported that demonstrate the effectiveness of the approach for solving large-scale problems. A further test on real data involving an Italian factory is also presented.</description><author>Ahuja, R. K.; Orlin, J. B.; Pallottino, S.; Scaparra, M. P.; Scutella, M. G.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Flow shop scheduling with partial resource flexibility</title><link>http://www.example.com/articles/1</link><description>Daniels, R. L.; Mazzola, J. B.; Shi, D. L.
Resource flexibility refers to the ability to dynamically reallocate units of resource from one stage of a production process to another in response to shifting bottlenecks. Recent research has demonstrated that substantial improvements in operational performance can be realized in both serial- and parallel-machine production environments through the effective utilization of resource flexibility. In these contexts the resource was assumed to exhibit complete flexibility This research explores the extent to which the operational benefits associated with resource flexibility can be achieved in a flow shop envirornment using a partially flexible resource. Focusing on labor flexibility, we propose corresponding metrics for partial flexibility and formulate a model for flow shop scheduling with partial resource flexibility. On the basis of computational experiments, we explore properties pertaining to the relative amounts as well as the allocation of partial resource flexibility as it is distributed across the workforce. The conclusions drawn from this research provide significant insight into the management of flow shops with a workforce that is crosstrained to achieve partial flexibility. Moreover, we extend the principles developed by Jordan and Graves (1995) for partially flexible manufacturing plants to the flow shop scheduling environment, and we link these principles in a novel way to recent research on self-buffering flow lines.</description><author>Daniels, R. L.; Mazzola, J. B.; Shi, D. L.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information transparency of business-to-business electronic markets: A game-theoretic analysis</title><link>http://www.example.com/articles/1</link><description>Zhu, K.
he abundance of transaction data available on the Internet tends to make information more transparent in electronic marketplaces. In such a transparent environment, it becomes easier for suppliers to obtain information that may allow them to infer their rivals' costs. Is this good news or bad news? In this study, we focus on the informational effects of business-to-business (B2B) exchanges, and explore firms' incentives to join a B2B exchange that provides an online platform for information transmission. We then study the equilibria by developing a game-theoretic model under asymmetric information. We examine whether the incentives to join a B2B exchange would be different under different competition modes (quantity and price), different information structures, and by varying the nature of the products (substitutes and complements). Our results challenge the "information transparency hypothesis" (i.e., open sharing of information in electronic markets is beneficial to all participating firms). In contrast to the popular belief, we show that information transparency could be a double-edged sword. The individual rationality of participation in the online exchange reflects the tradeoff between information transparency and data confidentiality. This may have important implications for the microstructure design (e.g., data access rules) of B2B electronic marketplaces.</description><author>Zhu, K.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product variety under brand influence: An empirical investigation of personal computer demand</title><link>http://www.example.com/articles/1</link><description>Hui, K. L.
Prior research suggests that brand may influence consumer preference for differentiated products. However, the extant literature does not measure how brand value affects product similarity and consumer choice. This paper examines demand response to the proliferation of personal computers (PCs). Using both the central processing unit (CPU) and brand as segmentation variables, I construct a two-level nested generalized extreme value (GEV) discrete choice model to estimate the brand values and product similarities of a set of PC vendors. With these estimates, I infer the relative efficacy of product variety for firms which possess different degrees of brand values. My results suggest that consumers treat PCs from the same firm as close substitutes, and the proximity of the PCs correlates positively with the firms' brand values. This finding suggests that there are decreasing demand returns to product variety for branded multiproduct firms. I discuss a few possible drivers of brand value and explore the significance of product line extension in building long-term brand reputation.</description><author>Hui, K. L.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Advance booking discount programs under retail competition</title><link>http://www.example.com/articles/1</link><description>McCardle, K.; Rajaram, K.; Tang, C. S.
We model a situation in which two retailers consider launching an "Advance Booking Discount" (ABD) program. In this program, customers are enticed to precommit their orders at a discount price prior to the regular selling season. However, these precommitted orders are filled during the selling season. While the ABD program enables the retailers to lock in a portion of the customer demand and use this demand information to develop more accurate forecasts and supply plans, the ABD price reduces profit margin. We analyze the four possible scenarios wherein each of the two firms offer an ABD program or not, and establish conditions under which the unique equilibrium calls for launching the ABD program at both retailers.</description><author>McCardle, K.; Rajaram, K.; Tang, C. S.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>"Stack them high, let 'em fly": Lot-sizing policies when inventories stimulate demand</title><link>http://www.example.com/articles/1</link><description>Balakrishnan, A.; Pangburn, M. S.; Stavrulaki, E.
In some retail contexts, stocking large quantities of inventory may not only improve service levels, but can also stimulate demand. For products having demand rates that increase with inventory levels, we analyze the effect of stocking decisions on firm profitability to develop managerial insights regarding the structure of the optimal inventory policy, and to understand how this policy differs from traditional approaches. When inventories stimulate demand, iteratively applying the standard economic order quantity (EOQ) model-by setting the demand rate parameter equal to the observed average demand rate in prior cycles-yields an equilibrium order quantity that is robust to demand parameter misestimation, but is suboptimal. The profit-maximizing policy orders larger quantities, and can replenish inventory even before on-hand stock fully depletes. Using an extension of a standard inventory-dependent demand model from the literature, we first provide a convenient characterization of products that require early replenishment. We demonstrate that the optimal cycle time is largely governed by the conventional trade-off between ordering and holding costs, whereas the reorder point relates to a promotions-oriented cost-benefit perspective. We show that the optimal policy yields significantly higher profits than cost-based inventory policies, underscoring the importance of profit-driven inventory management.</description><author>Balakrishnan, A.; Pangburn, M. S.; Stavrulaki, E.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pooling problem: Alternate formulations and solution methods</title><link>http://www.example.com/articles/1</link><description>Audet, C.; Brimberg, J.; Hansen, P.; Le Digabel, S.; Mladenovic, N.
The pooling problem, which is fundamental to the petroleum industry, describes a situation in which products possessing different attribute qualities are mixed in a series of pools in such a way that the attribute qualities of the blended products of the end pools must satisfy given requirements. It is well known that the pooling problem can be modeled through bilinear and nonconvex quadratic programming. In this paper, we investigate how best to apply a new branch-and-cut quadratic programming algorithm to solve the pooling problem. To this effect, we consider two standard models: One is based primarily on flow variables, and the other relies on the proportion. of flows entering pools. A hybrid of these two models is proposed for general pooling problems. Comparison of the computational properties of flow and proportion models is made on several problem instances taken from the literature. Moreover, a simple alternating procedure and a variable neighborhood search heuristic are developed to solve large instances and compared with the well-known method of successive linear programming. Solution of difficult test problems from the literature is substantially accelerated, and larger ones are solved exactly or approximately.</description><author>Audet, C.; Brimberg, J.; Hansen, P.; Le Digabel, S.; Mladenovic, N.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Who benefits from transshipment? Exogenous vs. endogenous wholesale prices</title><link>http://www.example.com/articles/1</link><description>Dong, L. X.; Rudi, N.
This paper studies how transshipments affect manufacturers and retailers, considering both exogenous and endogenous wholesale prices., For a distribution system where a single manufacturer sells to multiple identical-cost retailers, we consider both the manufacturer being a price taker and the manufacturer being a price setter in a single-period setup under multivariate normal demand distribution. In the case of the manufacturer being a price taker, we provide several analytical results regarding the effects of key parameters on order quantities and profits. In the case of the manufacturer being a price setter, we characterize the Stackelberg game that arises, and provide several insights into how the game dynamics are affected by transshipments. Specifically, we find that risk pooling makes retailers' order quantities less sensitive to the wholesale price set by the manufacturer; hence, in general, the manufacturer benefits from retailers' transshipments by charging a higher wholesale price, while retailers are often worse off. The paper captures the effect of demand correlation and the effect of the number of retailers throughout, and it illustrates the findings by a numerical example. We also provide an interactive Web page for numerical experiments.</description><author>Dong, L. X.; Rudi, N.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Prognosis using an isotonic prediction technique</title><link>http://www.example.com/articles/1</link><description>Ryu, Y. U.; Chandrasekaran, R.; Jacob, V.
Outcome prediction based on historical data has been of practical and theoretical interest in many disciplines. A common type of outcome prediction is binary or discrete outcome prediction, as found in medical diagnosis and firm bankruptcy prediction. The prediction problem studied in this paper is outcome time prediction, or prognosis. Prognosis in medicine refers to a prediction of probable outcome of a disease for a patient. Patient data used as the basis for disease prognosis are usually censored because some of the patients have not experienced the outcome of the disease at the time of prognosis. A mathematical-programming approach, called isotonic prediction, is developed for the purpose of such prognosis tasks. The proposed technique is different from well-known statistical survival analysis methods, such as Kaplan-Meier product-limit estimation and Cox's regression, in that it predicts individual patients' survival time frame. Two medical applications are presented to show the applicability of the proposed isotonic prediction technique.</description><author>Ryu, Y. U.; Chandrasekaran, R.; Jacob, V.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A branch-and-price algorithm for multistage stochastic integer programming with application to stochastic batch-sizing problems</title><link>http://www.example.com/articles/1</link><description>Lulli, G.; Sen, S.
In this paper, we present. a branch-and-price method to solve special structured multistage stochastic integer programming problems: We validate our method on two different versions of a multistage stochastic batchsizing problem (SBSP). One version adopts a recourse formulation, and the other is based on probabilistic constraints. Our algorithmic approach is applicable to both formulations. Our computational results suggest that both classes of problems can be solved using relatively few nodes of a branch-and-price tree. The success of our approach calls for extensions in methodology as well as applications.</description><author>Lulli, G.; Sen, S.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Tolerance sensitivity and optimality bounds in linear programming</title><link>http://www.example.com/articles/1</link><description>Wendell, R. E.
Traditional sensitivity analysis in linear programming usually focuses on variations of one coefficient or term at a time. The tolerance approach was proposed to provide a decision maker with an effective and easy-to-use method to summarize the effects of simultaneous and independent changes in selected parameters. In particular, for variations of the objective function coefficients, the approach gives a maximum-tolerance percentage within which selected coefficients may vary from their estimated values (within a priori limits) while still retaining the same optimal basic feasible solution. Although an optimal solution may cease being optimal for variations beyond the maximum-tolerance percentage, it may still be close to optimal. Herein we characterize the potential loss of optimality for variations beyond the maximum-tolerance percentage as a maximum-regret function. We consider theoretical properties of this function and propose a method to compute a relevant portion of it.</description><author>Wendell, R. E.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Do promotions benefit manufacturers, retailers, or both?</title><link>http://www.example.com/articles/1</link><description>Srinivasan, S.; Pauwels, K.; Hanssens, D. M.; Dekimpe, M. G.
Do price promotions generate additional revenue and for whom? Which brand, category, and market conditions influence promotional benefits and their allocation across manufacturers and retailers? To answer these questions, we conduct a large-scale econometric investigation of the effects of price promotions on manufacturer revenues, retailer revenues, and total profits (margins). A first major finding is that a price promotion typically does not have permanent monetary effects for either party. Second, price promotions have a predominantly positive impact on manufacturer revenues, but their effects on retailer revenues are mixed. Moreover, retailer category margins are typically reduced by price promotions. Even when accounting for cross-category and store-traffic effects, we still find evidence that price promotions are typically not beneficial to the retailer. Third, our results indicate that manufacturer revenue elasticities are higher for promotions of small-share brands, for frequently promoted brands and for national brands in impulse product categories with a low degree of brand proliferation and low private-label shares. Retailer revenue elasticities are higher for brands with frequent and shallow promotions, for impulse products, and in categories with a low degree of brand proliferation. Finally, retailer margin elasticities are higher for promotions of small-share brands and for brands with infrequent and shallow promotions. We discuss the managerial implications of our results for both manufacturers and retailers.</description><author>Srinivasan, S.; Pauwels, K.; Hanssens, D. M.; Dekimpe, M. G.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Don't fence me in: Fragmented markets for technology and the patent acquisition strategies of firms</title><link>http://www.example.com/articles/1</link><description>Ziedonis, R. H.
How do firms avoid being "fenced in" by owners of patented technologies used, perhaps unknowingly, in the design or manufacture of their products? This paper examines the conditions under which firms expand their own portfolios of patents in response to potential hold-up problems in markets for technology Combining insights from transactions cost theory with recent scholarship on intellectual property and its exchange, I predict firms will patent more aggressively than otherwise expected when markets for technology are highly fragmented (i.e., ownership rights to external technologies are widely distributed); this effect should be more pronounced for firms with large investments in technology-specific assets and under a strong legal-appropriability regime. Although these characteristics of firms and their external environments have been highlighted in the theoretical literature, prior research has not explored the extent to which such factors interact to shape the patenting behavior of firms. To empirically test these hypotheses, I develop a citations-based "fragmentation index" and estimate the determinants of patenting for 67 U.S. semiconductor firms between 1980 and 1994. Accumulating exclusionary rights of their own may enable firms to safeguard. their investments in new technologies while foregoing some of the costs and delays associated with ex ante contracting.</description><author>Ziedonis, R. H.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge sourcing effectiveness</title><link>http://www.example.com/articles/1</link><description>Gray, P. H.; Meister, D. B.
Much of the knowledge management (KM) literature focuses on ways to increase the volume of knowledge available to employees, ensure its quality, and improve its accessibility. Such supply-side arguments are limited to the extent that they do not address the demand for knowledge within,organizations. This paper takes a novel approach to understanding how access to others' knowledge produces benefits by studying the extent to which individuals intentionally access each other's expertise, experience, insights, and opinions, which we term knowledge sourcing. A general model of knowledge sourcing, including contextual and dispositional antecedents and learning outcomes, is proposed and validated using survey data from a global organization. Knowledge sourcing-explains a significant proportion of individuals' learning outcomes, but the strength of this effect is moderated both by the strength of individuals' learning orientations and the degree to which they find their jobs to be intellectually demanding. For researchers, this study extends existing knowledge by proposing, testing, and validating a new way to understand an important KM issue in organizations. Practitioners can use these findings to evaluate existing KM efforts and better target future KM interventions towards those individuals most likely to benefit.</description><author>Gray, P. H.; Meister, D. B.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Do scientists pay to be scientists?</title><link>http://www.example.com/articles/1</link><description>Stern, S.
his paper explores the relationship between wages and the scientific orientation of R&amp;D organizations. Firms that adopt a science-oriented research approach (i.e., "science") allow their researchers to pursue and publish an individual research agenda. The adoption of science may be associated with a "taste" for science on the part of researchers (a preference effect) and/or as a "ticket of admission" to gain earlier access to scientific discoveries with commercial application (a productivity effect). These two effects differ in their impact on wages. Whereas the preference effect contributes to a negative compensating differential, the productivity effect may result in rent sharing. However, because science may be adopted by firms employing higher-quality researchers, cross-sectional evaluations of wages and science may be biased by unobserved heterogeneity. To overcome this bias, this paper introduces a novel empirical approach. Specifically, prior to accepting a given job, many scientists receive multiple job offers, allowing for the calculation of the wage-science relationship and controlling for differences in salary levels offered to individual researchers. Using a dataset composed of multiple job offers to postdoctoral biologists, the results suggest a negative relationship between wages and science. These findings are robust to restricting the sample to nonacademic job offers, but the findings depend critically on the inclusion of researcher fixed effects. Conditional on perceived ability, scientists do indeed pay to be scientists.</description><author>Stern, S.</author><pubDate>Tue, 01 Jun 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A likelihood approach to estimating market equilibrium models</title><link>http://www.example.com/articles/1</link><description>Draganska, M.; Jain, D.
This paper develops a new likelihood-based method for the simultaneous estimation of structural demand-and-supply models for markets with differentiated products. We specify an individual-level discrete-choice model of demand and derive the supply side assuming manufacturers compete in prices. The proposed estimation method considers price endogeneity through simultaneous estimation of demand and supply, allows for consumer heterogeneity; and incorporates a pricing rule consistent with economic theory. The basic idea behind the proposed estimation procedure is to simulate prices and choice probabilities by solving for the market equilibrium. By repeating this many times, we obtain an empirical distribution of equilibrium prices and probabilities. The empirical distribution is then smoothed and used in a likelihood procedure to estimate the parameters of the model. The advantage of this method is that it avoids the need to perform a transformation of variables. If the tastes of consumers are independent across market periods, our approach yields maximum likelihood estimates; otherwise, it yields consistent but not fully efficient partial likelihood estimates.</description><author>Draganska, M.; Jain, D.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Probability judgments for continuous quantities: Linear combinations and calibration</title><link>http://www.example.com/articles/1</link><description>Hora, S. C.
Expert judgment elicitation is often required in probabilistic decision making and the evaluation of risk. One measure of the quality of probability distributions given by experts is calibration-the faithfulness of the probabilities in an empirically verifiable sense. A method of measuring calibration for continuous probability distributions is presented here. A discussion of the impact of using linear rules for combining such judgments is given and an empirical demonstration is given using data collected from experts participating in a large-scale risk study It is shown by theoretical argument that combining well-calibrated distributions of individual experts using linear rules can only result in reducing calibration. In contrast, it is demonstrated, both by example and empirically, that an equally weighted linear combination of experts who tend to be "overconfident" can produce distributions that are better calibrated than the experts' individual distributions. Using data from training exercises, it is shown that the improvement in calibration is rapid as the number of experts is increased from one to five or six, but there is only modest improvement from increasing the number of experts beyond that point.</description><author>Hora, S. C.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Consumer habituation</title><link>http://www.example.com/articles/1</link><description>Wathieu, L.
his paper examines how consumers' willingness to pay for goods is determined by past patterns of consumption. The central result is a theorem of interior maximum, which states that willingness to pay for a good is maximized at a moderate level of habitual consumption. The theorem is derived from a simple model of adaptive behavior that involves a shifting S-shaped value function. The detailed analysis of the impact of consumption frequency and intensity on willingness to pay reveals an unsuspected implication of diminishing sensitivity even as it leads to a formalization of consumer habituation patterns (including sensitization, habituation, and response recovery upon withdrawal) that matches and integrates the most robust empirical regularities attendant on nonassociative learning in neurobiology and behavioral psychology. An examination of the implications for demand dynamics and pricing highlights deterministic recurrent and transient patterns of consumption at higher price points.</description><author>Wathieu, L.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Keeping doors open: The effect of unavailability on incentives to keep options viable</title><link>http://www.example.com/articles/1</link><description>Shin, J.; Ariely, D.
Many of the options available to decision makers, such as college majors and romantic partners, can become unavailable if sufficient effort is not invested in them (taking classes, sending flowers). The question asked in this work is whether a threat of disappearance changes the way people value such options. In four experiments using "door games," we demonstrate that options that threaten to disappear cause decision makers to invest more effort and money in keeping these options open, even when the options themselves seem to be of little interest. This general tendency is shown to be resilient to information about the outcomes, to increased experience, and to the saliency of the cost. The last experiment provides initial evidence that the mechanism underlying the tendency to keep doors open is a type of aversion to loss rather than a desire for flexibility.</description><author>Shin, J.; Ariely, D.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decision analysis in Management science</title><link>http://www.example.com/articles/1</link><description>Smith, J. E.; Von Winterfeldt, D.
As part of the 50th anniversary of Management Science, the journal is publishing articles that reflect on the past, present, and future of the various subfields the journal represents. In this article, we consider decision analysis research as it has appeared in Management Science. After reviewing the foundations of decision analysis and the history of the journal's decision analysis department, we review a number of key developments in decision analysis research that have appeared in Management Science and offer some comments on the current state of the field.</description><author>Smith, J. E.; Von Winterfeldt, D.</author><pubDate>Sat, 01 May 2004 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>