<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Management Science11</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:07:42 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Introduction to the special issue on call center management</title><link>http://www.example.com/articles/1</link><description>Koole, Ger
nan</description><author>Koole, Ger</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call center outsourcing: Coordinating staffing level and service quality</title><link>http://www.example.com/articles/1</link><description>Ren, Z. Justin; Zhou, Yong-Pin
nan</description><author>Ren, Z. Justin; Zhou, Yong-Pin</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of simulation training on call center agent performance: A field-based investigation</title><link>http://www.example.com/articles/1</link><description>Murthy, Nagesh N.; Challagalla, Goutam N.; Vincent, Leslie H.; Shervani, Tasadduq A.
nan</description><author>Murthy, Nagesh N.; Challagalla, Goutam N.; Vincent, Leslie H.; Shervani, Tasadduq A.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis of the impact of team-based organizations in call center management</title><link>http://www.example.com/articles/1</link><description>Jouini, Oualid; Dallery, Yves; Nait-Abdallah, Rabie
nan</description><author>Jouini, Oualid; Dallery, Yves; Nait-Abdallah, Rabie</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Staffing of time-varying queues to achieve time-stable performance</title><link>http://www.example.com/articles/1</link><description>Feldman, Zohar; Mandelbaum, Avishai; Massey, William A.; Whitt, Ward
nan</description><author>Feldman, Zohar; Mandelbaum, Avishai; Massey, William A.; Whitt, Ward</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Staffing multiskill call centers via linear programming and simulation</title><link>http://www.example.com/articles/1</link><description>Cezik, Mehmet Tolga; L'Ecuyer, Pierre
nan</description><author>Cezik, Mehmet Tolga; L'Ecuyer, Pierre</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An exact and efficient algorithm for the constrained dynamic operator staffing problem for call centers</title><link>http://www.example.com/articles/1</link><description>Bhandari, Atul; Scheller-Wolf, Alan; Harchol-Balter, Mor
nan</description><author>Bhandari, Atul; Scheller-Wolf, Alan; Harchol-Balter, Mor</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call center outsourcing contract analysis and choice</title><link>http://www.example.com/articles/1</link><description>Aksin, O. Zeynep; de Vericourt, Francis; Karaesmen, Fikri
nan</description><author>Aksin, O. Zeynep; de Vericourt, Francis; Karaesmen, Fikri</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call center outsourcing: Coordinating staffing level and service quality</title><link>http://www.example.com/articles/1</link><description>Ren, Z. Justin; Zhou, Yong-Pin
In this paper, we study the contracting issues in an outsourcing supply chain consisting of a user company and a call center that does outsourcing work for the user company. We model the call center as a G/G/s queue with customer abandonment. Each call has a revenue potential, and we model the call center's service quality by the percentage of calls resolved (revenue realized). The call center makes two strategic decisions: how many agents to have and how much effort to exert to achieve service quality. We are interested in the contracts the user company can use to induce the call center to both staff and exert effort at levels that are optimal for the outsourcing supply chain (i.e., chain coordination). Two commonly used contracts are analyzed first: piecemeal and pay-per-call-resolved contracts. We show that although they can coordinate the staffing level, the resulting service quality is below system optimum. Then, depending on the observability and contractibility of the call center's effort, we propose two contracts that can coordinate both staffing and effort. These contracts suggest that managers pay close attention to service quality and its contractibility in seeking call center outsourcing.</description><author>Ren, Z. Justin; Zhou, Yong-Pin</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis of the impact of team-based organizations in call center management</title><link>http://www.example.com/articles/1</link><description>Jouini, Oualid; Dallery, Yves; Nait-Abdallah, Rabie
We investigate the benefits of migrating from a call center, where all agents are pooled and customers are treated indifferently by any agent, toward a call center where customers are grouped into clusters with dedicated teams of agents. Each cluster is referred to as a portfolio. Customers of the same portfolio are always served by an agent of the corresponding team. There is no specialization involved in this organization in the sense that all customer portfolios as well as all agent teams have (statistically) identical behaviors. The reason for moving to this organization is that dealing with teams of limited size allows a much better workforce management than the situation usually encountered in large call centers. The purpose of this paper is to examine how the benefits of moving to this new organization can outweigh the drawbacks. The drawbacks come from the fact that there is less of a pooling effect in the new organization than in the original one. The benefits come from the better human resource management that results in a higher efficiency of the agents, both in terms of speed and quality of the answers they provide to customers. Our analysis is supported by the use of some simple queueing models and provides some interesting insights. In particular, it appears that for some reasonable ranges of parameters, the new organization can outperform the original organization. We then extend the analysis to the case where, in addition to the identified customer portfolios, there is an additional flow of calls called out-portfolio flow. It is shown that this feature makes the new organization even more efficient.</description><author>Jouini, Oualid; Dallery, Yves; Nait-Abdallah, Rabie</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of simulation training on call center agent performance: A field-based investigation</title><link>http://www.example.com/articles/1</link><description>Murthy, Nagesh N.; Challagalla, Goutam N.; Vincent, Leslie H.; Shervani, Tasadduq A.
The most prevalent form of training call center agents is via classroom instruction coupled with roleplays. Role-play training has a theoretical base in behavior modeling that entails observation, practice, and feedback. Emerging simulation-based technologies offer enhancements to behavior modeling that are absent in role-play training. This study evaluates the effectiveness of simulation-based training (henceforth, simulation training) as a behavior modeling technique vis-a-vis role-play training in a real-world call center environment across tasks of different levels of complexity. We collaborate with call centers at two Fortune 50 firms and examine on-job performance metrics to evaluate the effectiveness of simulation training. The performance measures of interest are call accuracy and call duration because these are two important factors that influence customer satisfaction and productivity in call center operations. After controlling for factors such as trainee's learning and technology orientation, age, education, and call center experience, results show that simulation training outperforms role-playing-based training in terms of both accuracy and speed of processing customer calls. Further, the relative superiority of simulation training improves at higher levels of task complexity.</description><author>Murthy, Nagesh N.; Challagalla, Goutam N.; Vincent, Leslie H.; Shervani, Tasadduq A.</author><pubDate>Fri, 01 Feb 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Buyer-initiated vs. seller-initiated information revelation</title><link>http://www.example.com/articles/1</link><description>Bhardwaj, Pradeep; Chen, Yuxin; Godes, David
Sales presentations are the core of the selling process where salespeople provide information to prospects. One challenge is that the amount of information available to be potentially communicated may exceed salespeople's ability to communicate or customers' ability to process: there is limited "bandwidth" between the firm and customers. One important decision, then, is which information should customers see? What the firm chooses to tell customers may be informative in itself. When constrained to a "seller-initiated information revelation" format, where the firm chooses which feature to show, the firm never finds it optimal to offer more features than it is able to inform customers about. Consequently, customers never find credible a claim that the product has all features. The important implication is that price alone cannot serve as a signal of quality in this setting. In contrast, a "buyer-initiated information revelation" format, where customers decide which information to receive, increases the probability of a sale and also results in the production of higher quality products. In a competitive setting, by adopting buyer-initiated information revelation, firms are able to attain positive profits. This is due to the fact that customers infer the product's quality from the price along with the information revelation format. Customers know that at some prices, firms find it profitable to produce high-quality products and at other, lower, prices, this is not the case. Thus, customer empowerment leads to higher profits.</description><author>Bhardwaj, Pradeep; Chen, Yuxin; Godes, David</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customer loyalty programs: Are they profitable?</title><link>http://www.example.com/articles/1</link><description>Singh, Siddharth S.; Jain, Dipak C.; Krishnan, Trichy V.
Loyalty programs are very common in practice. Many researchers have worked at understanding the impact of loyalty programs on market competition and the mechanism behind it. Interestingly, almost all of the studies have explored a symmetric equilibrium where both of the competing firms offer a loyalty program. To our knowledge, the extant literature has not investigated in-depth whether asymmetric equilibrium can exist where only one firm chooses to offer a loyalty program and the other firm chooses to compete via lowering prices. Such a question is important because some markets do support such asymmetric equilibriums with respect to loyalty programs. Also, the existence of asymmetric equilibrium shows that a loyalty program need not be pro. table for some firms. In this paper, we use a game-theoretic framework to investigate specific types of customer loyalty programs that provide benefit to loyal customers in the form of discount over market prices. The model considers consumer switching and includes two types of consumer heterogeneity. The first type of heterogeneity concerns the differences between customers with respect to their liking for loyalty programs, and the second type concerns the differences among the loyalty program members with respect to their ability to collect enough loyalty points to redeem loyalty rewards. By analyzing a duopoly market, we find that both symmetric equilibrium (i.e., where both competing firms offer the loyalty program) and asymmetric equilibrium (i.e., where one firm alone offers the loyalty program) can be sustained. The paper explores conditions for the existence of these two equilibriums.</description><author>Singh, Siddharth S.; Jain, Dipak C.; Krishnan, Trichy V.</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Leadership and competition in network supply chains</title><link>http://www.example.com/articles/1</link><description>Majumder, Pranab; Srinivasan, Ashok
This paper considers network supply chains with price dependent demand by modelling them as large acyclic networks. Such large networks are common in the automobile and apparel industries. We develop a model to analyze the effect of these large-scale problems involving long sequences of contracts, and show that contract leadership, as well as leader position in the network, affect the performance of the entire supply chain. We generalize Spengler (Spengler, J. 1950. Vertical integration and anti-trust policy. J. Political Econom. 58 347-352) to a game on a "contract tree" for a particular supply chain and extend the concept of double marginalization so that it can be applied in the form of a transformation to each contract that is offered by one member to another in the "contract tree." We construct an algorithm to find the equilibrium solution, and derive the optimal location of the leader ("optimal" being the leader location that maximizes total supply chain profits). Our work formalizes many intuitive insights; for example, member profits are determined by systemwide rather than individual costs. Finally, we model Cournot competition between competing supply chains (both two heterogeneous trees and multiple identical trees) and show the effect of changes in leader position as well as cost structure on the equilibrium.</description><author>Majumder, Pranab; Srinivasan, Ashok</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling the dynamics of credit spreads with stochastic volatility</title><link>http://www.example.com/articles/1</link><description>Jacobs, Kris; Li, Xiaofei
This paper investigates a two-factor affine model for the credit spreads on corporate bonds. The first factor can be interpreted as the level of the spread and the second factor is the volatility of the spread. The riskless interest rate is modeled using a standard two-factor affine model, thus leading to a four-factor model for corporate yields. This approach allows us to model the volatility of corporate credit spreads as stochastic, and also allows us to capture higher moments of credit spreads. We use an extended Kalman filter approach to estimate our model on corporate bond prices for 108 firms. The model is found to be successful at fitting actual corporate bond credit spreads, resulting in a significantly lower root mean square error than a standard alternative model both in sample and out of sample. In addition, key properties of actual credit spreads such as the stochastic volatility of the credit spreads and the positive skewness of the credit spread distribution are better captured by the model.</description><author>Jacobs, Kris; Li, Xiaofei</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A no-arbitrage analysis of macroeconomic determinants of the credit spread term structure</title><link>http://www.example.com/articles/1</link><description>Wu, Liuren; Zhang, Frank Xiaoling
From a large array of economic and financial data series, this paper identifies three fundamental risk dimensions underlying an economy: inflation, real output growth, and financial market volatility. Furthermore, through a no-arbitrage model, the paper links the dynamics and market pricing of the three risk dimensions to the term structure of U.S. Treasury yields and corporate bond credit spreads. Model estimation shows that positive inflation shocks increase Treasury yields and widen credit spreads on corporate bonds across all maturities and credit-rating classes. Positive real output growth shocks also increase Treasury yields, but they suppress the credit spreads at low credit-rating classes, thus generating negative correlations between interest rates and credit spreads. The financial market volatility factor has a small and transient effect on the Treasury yield curve, but it exerts a strongly positive and persistent effect on the credit spread term structure. The paper provides a robust and internally consistent method for extracting systematic economic information from a large array of noisy observations and establishing how different risk dimensions of the fundamental economy interact with interest rate and credit risk.</description><author>Wu, Liuren; Zhang, Frank Xiaoling</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal second-stage outsourcing</title><link>http://www.example.com/articles/1</link><description>Saouma, Richard
Manufacturers have recently begun outsourcing product assembly and completion tasks to their suppliers. Such outsourcing solves several contracting problems but generates new incentive frictions between manufacturers and their suppliers. In this paper, we analyze a manufacturer's decision to outsource an assembly (second-stage) task to a preestablished supplier. We find that outsourcing second-stage tasks becomes more attractive as the cost of either the first- or second-stage activity rises. Outsourcing becomes less attractive when the supplier is unable to accept large levels of liability. The manufacturer is shown to prefer more testing when she outsources assembly to her supplier as opposed to when she assembles products in house. Last, we find that the contracting frictions identified persist when the supplier's work can be tested individually, albeit imperfectly.</description><author>Saouma, Richard</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic pricing and lead-time quotation for a multiclass make-to-order queue</title><link>http://www.example.com/articles/1</link><description>Celik, Sabri; Maglaras, Costis
This paper considers a profit-maximizing make-to-order manufacturer that offers multiple products to a market of price and delay sensitive users, using a model that captures three aspects of particular interest: first, the joint use of dynamic pricing and lead-time quotation controls to manage demand; second, the presence of a dual sourcing mode that can expedite orders at a cost; and third, the interaction of the aforementioned demand controls with the operational decisions of sequencing and expediting that the firm must employ to optimize revenues and satisfy the quoted lead times. Using an approximating diffusion control problem we derive near-optimal dynamic pricing, lead-time quotation, sequencing, and expediting policies that provide structural insights and lead to practically implementable recommendations. A set of numerical results illustrates the value of joint pricing and lead-time control policies.</description><author>Celik, Sabri; Maglaras, Costis</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic capacity rationing to induce early purchases</title><link>http://www.example.com/articles/1</link><description>Liu, Qian; van Ryzin, Garrett J.
Dynamic pricing offers the potential to increase revenues. At the same time, it creates an incentive for customers to strategize over the timing of their purchases. A firm should ideally account for this behavior when making its pricing and stocking decisions. In particular, we investigate whether it is optimal for a firm to create rationing risk by deliberately understocking products. Then, the resulting threat of shortages creates an incentive for customers to purchase early at higher prices. But when does such a strategy make sense? If it is pro. table to create shortages, what is the optimal amount of rationing risk to create? We develop a stylized model to study this problem. In our model, customers have heterogeneous valuations for the firm's product and face declining prices over two periods. Customers are assumed to have identical risk preferences and know the price path and fill rate in each period. Via its capacity choice, the firm is able to control the fill rate and, hence, the rationing risk faced by customers. Customers behave strategically and weigh the payoff of immediate purchases against the expected payoff of delaying their purchases. We analyze the capacity choice that maximizes the firm's profits. First, we consider a monopoly market and characterize conditions under which rationing is optimal. We examine how the optimal amount of rationing is affected by the magnitude of price changes over time and the degree of risk aversion among customers. We then analyze an oligopoly version of the model and show that competition reduces the firms' ability to profit from rationing. Indeed, there exists a critical number of firms beyond which a rationing equilibrium cannot be supported.</description><author>Liu, Qian; van Ryzin, Garrett J.</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Consumer privacy and marketing avoidance: A static model</title><link>http://www.example.com/articles/1</link><description>Hann, Il-Horn; Hui, Kai-Lung; Lee, Sang-Yong T.; Png, Ivan P. L.
We introduce the concept of marketing avoidance-consumer efforts to conceal themselves and to deflect marketing. The setting is one in which sellers market some item through solicitations to potential consumers, who differ in their benefit from the item and suffer harm from receiving solicitations. Concealment by one consumer induces sellers to shift solicitations to other consumers, whereas deflection does not. Solicitations cause two externalities: direct harm on consumers and the (indirect) cost of consumer concealment and deflection. We find that in markets where the marginal cost of solicitation is sufficiently low, efforts by low-benefit consumers to conceal themselves will increase the cost-effectiveness of solicitations and lead sellers to market more. However, concealment by high-benefit consumers leads sellers to market less. Furthermore, concealment by low-benefit consumers increases direct privacy harm, and consumer welfare is higher with deflection than concealment. Finally, it is optimal to impose a charge on solicitations.</description><author>Hann, Il-Horn; Hui, Kai-Lung; Lee, Sang-Yong T.; Png, Ivan P. L.</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Physicians' persistence and its implications for their response to promotion of prescription drugs</title><link>http://www.example.com/articles/1</link><description>Janakiraman, Ramkumar; Dutta, Shantanu; Sismeiro, Catarina; Stern, Philip
Motivated by the medical literature findings that physicians are inertial, we seek to understand (1) whether physicians exhibit structural persistence in drug choice (structural persistence occurs when the drug chosen for a patient depends structurally on the drug previously prescribed by the physician to other patients) and (2) whether persistence, if present, is a physician-specific characteristic or a physician state that can change over time. We further explore the role of promotional tools on persistence and drug choice, and we investigate whether physicians who exhibit persistence respond differently to three forms of sales promotion: one-to-one meetings (detailing), out-of-office meetings, and symposium meetings. Our results show significant levels of physician persistence in drug choice. We find that persistence is mostly a cross-sectional physician feature. Nonpersistent physicians appear to be responsive to detailing and symposium meetings, whereas persistent physicians seem to be responsive only to symposium meetings. Out-of-office meetings, such as golf or lunch, have no effect on physicians' drug choice. We also find that (1) older physicians and those who work in smaller practices are more likely to be persistent and (2) physicians who are more willing to receive sales force representatives have a lower likelihood of being persistent. Finally, we discuss implications for public policy from our rich set of results.</description><author>Janakiraman, Ramkumar; Dutta, Shantanu; Sismeiro, Catarina; Stern, Philip</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does the standardization process matter? A study of cost effectiveness in hospital drug formularies</title><link>http://www.example.com/articles/1</link><description>Kwon, Seok-Woo
While research on the cost effectiveness of standardization to date has focused on the impact of different degrees of standardization, it has paid insufficient attention to the process an organization uses to formulate and implement standardized procedures. Drawing on the organizational decision-making literature and procedural fairness literature, this study identifies a number of key process features in standardization and argues that variations in these process features across organizations help account for the varying success of standardization in achieving cost effectiveness. Using hospital drug standardization for coronary artery and pneumonia cases to ground much of the argument, I analyzed inpatient discharge data from Florida, Illinois, New York, and Texas, combined with an original survey of 243 hospital pharmacy directors. The results indicated that an increasing degree of standardization was associated with cost effectiveness when the level of formal objectivity in creating the standardized procedure was high and when there was due process in resolving disputes about the standardized procedure. This finding broadly supports the argument that the cost effectiveness of standardization depends not just on the degree of standardization but also on the process by which the standardized procedures are created and implemented.</description><author>Kwon, Seok-Woo</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How do decision frames influence the stock investment choices of individual investors?</title><link>http://www.example.com/articles/1</link><description>Kumar, Alok; Lim, Sonya Seongyeon
This study examines whether the framing mode (narrow versus broad) influences the stock investment decisions of individual investors. Motivated by the experimental evidence, which suggests that separate decisions are more likely to be narrowly framed than simultaneous decisions, we propose trade clustering as a proxy for narrow framing. Using this framing proxy, we show that investors who execute more clustered trades exhibit weaker disposition effects and hold better-diversified portfolios. We also find that the degree of trade clustering is related to investors' stock preferences and portfolio returns. Collectively, the evidence indicates that the choice of decision frames is likely to be an important determinant of investment decisions.</description><author>Kumar, Alok; Lim, Sonya Seongyeon</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Are overconfident CEOs born or made? Evidence of self-attribution bias from frequent acquirers</title><link>http://www.example.com/articles/1</link><description>Billett, Matthew T.; Qian, Yiming
W e explore the history of mergers and acquisitions made by individual CEOs. Our study has three main findings: (1) CEOs' first deals exhibit zero announcement effects while their subsequent deals exhibit negative announcement effects; (2) while acquisition likelihood increases in the performance associated with previous acquisitions, previous positive performance does not curb the negative wealth effects associated with subsequent deals; and (3) CEOs' net purchase of stock is greater preceding subsequent deals than it is for first deals. We interpret these results as consistent with self-attribution bias leading to overconfidence. We also find evidence that the market anticipates future deals based on the CEO's acquisition history and impounds such anticipation into stock prices.</description><author>Billett, Matthew T.; Qian, Yiming</author><pubDate>Sun, 01 Jun 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis of direct shipping policies in an inventory-routing problem with discrete shipping times</title><link>http://www.example.com/articles/1</link><description>Bertazzi, Luca
We consider the problem in which several products have to be shipped from a supplier to a set of retailers over an infinite time horizon. Each product is made available at the supplier and absorbed by each retailer at given constant production and consumption rates. For each product, the unit inventory cost at the supplier is equal to the unit inventory cost at the retailers. Shipments can be performed in each discrete time instant by a fleet of vehicles of given transportation capacity and routing is allowed. The aim is to determine shipping policies that minimize the sum of transportation cost and inventory cost both at the supplier and at the retailers. We study direct shipping policies, i.e., policies in which each retailer is served independently. We show the worst-case performance of the best single, best double, best triple, best frequency-based, and optimal direct shipping policies. In each of these policies each link is optimized independently. We prove that, in the worst case, the ratio between the cost of the optimal direct shipping policy and the optimal cost of the problem in which routing is allowed-i.e., the worst-case performance bound of this policy-is not greater than two whenever the unit volume on each link is not lower than 1/4 of the vehicle capacity. Moreover, if the unit volume is not lower than the capacity, the bound is about 1.21. These bounds are tight and obtained by applying frequency-based policies that make use of at most three different shipping frequencies and cannot be improved by allowing more frequencies or using time-based policies on each link. Computational results show that the best among the frequency-based direct shipping policies used to obtain the worst-case results gives an average percent increase error, with respect to a lower bound on the optimal cost of the problem in which routing is allowed, of 6.44% on a large set of randomly generated problem instances.</description><author>Bertazzi, Luca</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory management with advance demand information and flexible delivery</title><link>http://www.example.com/articles/1</link><description>Wang, Tong; Toktay, Beril L.
This paper considers inventory models with advance demand information and flexible delivery. Customers place their orders in advance, and delivery is flexible in the sense that early shipment is allowed. Specifically, an order placed at time t by a customer with demand lead time T should be fulfilled by period t + T; failure to fulfill it within the time window [t, t + T] is penalized. We consider two situations: (1) Customer demand lead times are homogeneous and demand arriving in period t is a scalar d(t) to be satisfied within T periods. We show that state-dependent (s, S) policies are optimal, where the state represents advance demands outside the supply lead-time horizon. We find that increasing the demand lead time is more beneficial than decreasing the supply lead time. (2) Customers are heterogeneous in their demand lead times. In this case, demands are vectors and may exhibit crossover, necessitating an allocation decision in addition to the ordering decision. We develop a lower-bound approximation based on an allocation assumption, and propose protection-level heuristics that yield upper bounds on the optimal cost. Numerical analysis quantifies the optimality gaps of the heuristics (2% on average for the best heuristic) and the benefit of delivery flexibility (14% on average using the best heuristic), and provides insights into when the heuristics perform the best and when flexibility is most beneficial.</description><author>Wang, Tong; Toktay, Beril L.</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contracting and information sharing under supply chain competition</title><link>http://www.example.com/articles/1</link><description>Ha, Albert Y.; Tong, Shilu
We investigate contracting and information sharing in two competing supply chains, each consisting of one manufacturer and one retailer. The two supply chains are identical, except they may have different investment costs for information sharing. The problem is studied using a two-stage game. In the first stage, the manufacturers decide whether to invest in information sharing. In the second stage, given the information structure created in the first stage, the manufacturers offer contracts to their retailers and the retailers engage in Cournot competition. We analyze the game for two different contract types. For the case of contract menus, a supply chain that does not have information sharing will lower its selling quantities because of the negative quantity distortions in the contract menus, thus creating a strategic disadvantage in Cournot competition. The value of information sharing to a supply chain is positive, and the dominant strategy of each supply chain is to invest in information sharing when the investment costs are low. We fully characterize the equilibrium information sharing decisions under different investment costs. For the case of linear price contracts, the value of information sharing,to a supply chain becomes negative, and the dominant strategy of each supply chain is not to invest in information sharing regardless of investment costs. Our results highlight the importance of contract type as a driver of the value of information sharing and the role of information sharing capability as a source of competitive advantage under supply chain competition.</description><author>Ha, Albert Y.; Tong, Shilu</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Designing pricing contracts for boundedly rational customers: Does the framing of the fixed fee matter?</title><link>http://www.example.com/articles/1</link><description>Ho, Teck-Hua; Zhang, Juanjuan
The format of pricing contracts varies substantially across business contexts, a major variable being whether a contract imposes a fixed fee payment. This paper examines how the use of the fixed fee in pricing contracts affects market outcomes of a manufacturer-retailer channel. Standard economic theories predict that channel efficiency increases with the introduction of the fixed fee and is invariant to its framing. We conduct a laboratory experiment to test these predictions. Surprisingly, the introduction of the fixed fee fails to increase channel efficiency. Moreover, the framing of the fixed fee does make a difference: an opaque frame as quantity discounts achieves higher channel efficiency than a salient frame as a two-part tariff, although these two contractual formats are theoretically equivalent. To account for these anomalies, we generalize the standard economic model by allowing the retailer's utilities to be reference dependent so that the up-front fixed fee payment is perceived as a loss and the subsequent retail profits as a gain. We embed this reference-dependent utility function in a quantal response equilibrium framework where the retailer is allowed to make decision mistakes due to computational complexity The key prediction of this behavioral model is that channel efficiency decreases with loss aversion for sufficiently Nash-rational retailers. Consistent with this prediction, the estimated loss-aversion coefficient is 1.37 in the two-part tariff condition, significantly higher than 1.27 in the quantity discount condition. At the same time, loss aversion dominates contract complexity in explaining the data. Lastly, we conduct a follow-up experiment to confirm the central role of loss aversion as a behavioral driver. In one condition, the retailer becomes less loss averse when we temporally compress the fixed fee payment and the realization of retail profits, which supports the loss aversion theory. In the other condition, the retailer's contract acceptance rate does not decline when we reward the manufacturer a higher cash payment for each experimental point earned, which rules out the competing hypothesis that the retailer rejects contract offers due to fairness concerns.</description><author>Ho, Teck-Hua; Zhang, Juanjuan</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>High touch through high tech: The impact of salesperson technology usage on sales performance via mediating mechanisms</title><link>http://www.example.com/articles/1</link><description>Ahearne, Michael; Jones, Eli; Rapp, Adam; Mathieu, John
Sales technology has been touted as a primary tool for enhancing customer relationship management. However, empirical research is sparse concerning the use of information technology (IT) and its effects on the relationship between salespersons and customers. Using an interdisciplinary research approach, we extend task-technology-fit (TTF) theory by examining the mechanisms through which use of IT by the sales force influences salesperson performance. We test a model that incorporates salespersons' customer service, attention to personal details, adaptability, and knowledge-key marketing constructs that could mediate IT's impact on salesperson performance. Results in a pharmaceutical sales setting indicate that IT use can improve customer service and salespersons' adaptability, leading to improved sales performance.</description><author>Ahearne, Michael; Jones, Eli; Rapp, Adam; Mathieu, John</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Security patch management: Share the burden or share the damage?</title><link>http://www.example.com/articles/1</link><description>Cavusoglu, Hasan; Cavusoglu, Huseyin; Zhang, Jun
Patch management is a crucial component of information security management. An important problem within this context from a vendor's perspective is to determine how to release patches to fix vulnerabilities in its software. From a firm's perspective, the issue is how to update vulnerable systems with available patches. In this paper, we develop a game-theoretic model to study the strategic interaction between a vendor and a firm in balancing the costs and benefits of patch management. Our objective is to examine the consequences of time-driven release and update policies. We first study a centralized system in a benchmark scenario to find the socially optimal time-driven patch management. We show that the social loss is minimized when patch-release and update cycles are synchronized. Next, we consider a decentralized system in which the vendor determines its patch-release policy and the firm selects its patch-update policy in a Stackelberg framework, assuming that release and update policies are either time driven or event driven. We develop a sufficient condition that guarantees that a time-driven release by the vendor and a time-driven update by the firm is the equilibrium outcome for patch management. However, in this equilibrium, the patch-update cycle of the firm may not be synchronized with the patch-release cycle of the vendor, making it impossible to achieve the socially optimal patch management in the decentralized system. Therefore, we next examine cost sharing and liability as possible coordination mechanisms. Our analysis shows that cost sharing itself may achieve synchronization and social optimality However, liability by itself cannot achieve social optimality unless patch-release and update cycles are already synchronized without introducing any liability. Our results also demonstrate that cost sharing and liability neither complement nor substitute each other. Finally, we show that an incentive-compatible contract on cost sharing can be designed to achieve coordination in case of information asymmetry.</description><author>Cavusoglu, Hasan; Cavusoglu, Huseyin; Zhang, Jun</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal policy for software vulnerability disclosure</title><link>http://www.example.com/articles/1</link><description>Arora, Ashish; Telang, Rahul; Xu, Hao
Software vulnerabilities represent a serious threat to cybersecurity, most cyberattacks exploit known vulnerabilities. Unfortunately, there is no agreed-upon policy for their disclosure. Disclosure policy (which sets a protected period given to a vendor to release the patch for the vulnerability) indirectly affects the speed and quality of the patch that a vendor develops. Thus, CERT/CC and similar bodies acting in the public interest can use disclosure to influence the behavior of vendors and reduce social cost. This paper develops a framework to analyze the optimal timing of disclosure. We formulate a model involving a social planner who sets the disclosure policy and a vendor who decides on the patch release. We show that the vendor typically releases the patch less expeditiously than is socially optimal. The social planner optimally shrinks the protected period to push the vendor to deliver the patch more quickly, and sometimes the patch release time coincides with disclosure. We extend the model to allow the proportion of users implementing patches to depend upon the quality (chosen by the vendor) of the patch. We show that a longer protected period does not always result in a better patch quality. Another extension allows for some fraction of users to use "work-arounds." We show that the possibility of work-arounds can provide the social planner with more leverage, and hence the social planner shrinks the protected period. Interestingly, the possibility of work-arounds can sometimes increase the social cost due to the negative externalities imposed by the users who are able to use the work-arounds on the users who are not.</description><author>Arora, Ashish; Telang, Rahul; Xu, Hao</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory record inaccuracy: An empirical analysis</title><link>http://www.example.com/articles/1</link><description>DeHoratius, Nicole; Raman, Ananth
Traditional inventory models, with a few exceptions, do not account for the existence of inventory record inaccuracy (IRI), and those that do treat IRI as random. This study explores IRI observed both within and across product categories and retail stores. Examining nearly 370,000 inventory records from 37 stores of one retailer, we find 65% to be inaccurate. We characterize the distribution of IRI and show, using hierarchical linear modeling (HLM), that 26.4% of the total variance in IRI lies between product categories and that 2.7% lies between stores. We identify several factors that mitigate record inaccuracy, such as inventory auditing practices, and several factors that exacerbate record inaccuracy, such as the complexity of the store environment and the distribution structure. Collectively, these covariates explain 67.6% and 69.0% of the variance in IRI across stores and product categories, respectively. Our findings underscore the need to design processes to reduce the occurrence of IRI and highlight factors that can be incorporated into inventory planning tools developed to account for its presence.</description><author>DeHoratius, Nicole; Raman, Ananth</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition and cooperation in decentralized push and pull assembly systems</title><link>http://www.example.com/articles/1</link><description>Granot, Daniel; Yin, Shuya
In this paper, we study a decentralized assembly system consisting of a single assembler who buys complementary components from independent suppliers under two contracting schemes: push and pull. In both schemes, the component suppliers are allowed to freely form coalitions (or alliances) among themselves to better coordinate their pricing or production decisions. We show that the sole driver of the inefficiency in a push system, which is due to horizontal decentralization of suppliers, is the number of alliances that were formed. Specifically, it is shown that in a push system, the assembler's profit, the total profit of all suppliers and the consumers' surplus are all decreasing in the number of coalitions, and are thus maximized when the grand coalition is formed. We further carry out a stability analysis of coalition structures to verify to what extent suppliers can reduce or eliminate the inefficiency due to their decentralization by forming alliances. We show that in a push system with more than two suppliers and a power demand distribution, myopic suppliers would act independently, resulting with a least efficient channel, which makes all channel members, as well as the end consumers, worse off. On the other hand, we prove that farsighted suppliers would form the grand coalition and thus be able to completely eliminate the inefficiency stemming from their decentralization. Finally, it is shown that, in contrast to a push system, in a pull system the suppliers can easily coordinate their production quantities to eliminate the inefficiency due to their decentralization.</description><author>Granot, Daniel; Yin, Shuya</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A constant approximation algorithm for the one-warehouse multiretailer problem</title><link>http://www.example.com/articles/1</link><description>Levi, Retsef; Roundy, Robin; Shmoys, David; Sviridenko, Maxim
Deterministic inventory theory provides streamlined optimization models that attempt to capture trade-offs in managing the flow of goods through a supply chain. We will consider two well-studied deterministic inventory models, called the one-warehouse multiretailer (OWMR) problem and its special case the joint replenishment problem (JRP), and give approximation algorithms with worst-case performance guarantees. That is, for each instance of the problem, our algorithm produces a solution with cost that is guaranteed to be at most 1.8 times the optimal cost; this is called a 1.8-approximation algorithm. Our results are based on an LP-rounding approach; we provide the first constant approximation algorithm for the OWMR problem and improve the previous results for the JRP.</description><author>Levi, Retsef; Roundy, Robin; Shmoys, David; Sviridenko, Maxim</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quantity discounts under demand uncertainty</title><link>http://www.example.com/articles/1</link><description>Altintas, Nihat; Erhun, Feryal; Tayur, Sridhar
To motivate buyers to increase their order quantity, suppliers often rely on a well-established and widely used approach-they offer quantity discounts. This practice is in large part driven to obtain improved economies in transportation through higher truckload utilization. Recently, transportation rates, which are increasing faster than other costs, have become a larger portion of total net landed cost, placing the traditional quantity-discount practices under scrutiny. Many suppliers are left perplexed as to why their approach is not effective anymore, and some are even concerned that their overall profits may have actually decreased due to their discount parameters. In this paper, we study a multiperiod model, with a buyer facing stochastic end-item demand and a supplier offering an all-units quantity discount to him, to understand better the dynamics of such systems. We provide guidelines and insights on how to set effective discount parameters, and when not to expect much from them. We derive the optimal policy of the buyer, develop insights as to why the policy is complex, study the supplier's profit as a function of her offered quantity-discount scheme (accommodating the buyer's optimal policy), and discover a new phenomenon that is distinct and structurally different from the well-known bullwhip effect.</description><author>Altintas, Nihat; Erhun, Feryal; Tayur, Sridhar</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exact distribution of visiting rates</title><link>http://www.example.com/articles/1</link><description>Nadarajah, Saralees; Kotz, Samuel
The exact distribution of visiting rates does not appear to be widely known in the marketing literature. Here, analytical expressions are derived for the exact distribution, and their performance is compared with some known approximation. Some computer programs are provided for practical use.</description><author>Nadarajah, Saralees; Kotz, Samuel</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Testing file sharing's impact on music album sales in cities</title><link>http://www.example.com/articles/1</link><description>Liebowitz, Stan J.
Using a data set including album sales, Internet penetration, and various demographic measures for 99 American cities over the period 1998-2003, this paper empirically examines the extent to which file sharing has caused the U.S. decline in sound-recording sales over that period. Also examined is the impact of the Internet on entertainment activities so as to help cleanse the Internet penetration coefficient of that impact. The conclusion from this analysis is that file sharing appears to have caused the entire decline in record sales and appears to have vitiated what otherwise would have been growth in the industry.</description><author>Liebowitz, Stan J.</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interior additivity and subjective probability assessment of continuous, variables</title><link>http://www.example.com/articles/1</link><description>Clemen, Robert T.; Ulu, Canan
One of the goals of psychological research on subjective probability judgment is to develop prescriptive procedures that can improve such judgments. In this paper, our aim is to reduce partition dependence, a judgmental bias that arises from the particular way in which a state space is partitioned for the purposes of making probability judgments. We explore a property of subjective probabilities called interior additivity (IA). Our story begins with a psychological model of subjective probability judgment that exhibits IA. The model is a linear combination of underlying support for the event in question and a term that reflects a prior belief that all elements in the state space partition are equally likely. The model is consistent with known properties of subjective probabilities, such as binary complementarity, subadditivity, and partition dependence, and has several additional properties related to IA. We present experimental evidence to support our model. The model further suggests a simple prescriptive method based on IA that decision and risk analysts can use to reduce partition dependence, and we present preliminary empirical evidence demonstrating the effectiveness of the method.</description><author>Clemen, Robert T.; Ulu, Canan</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A dynamic competitive forecasting model incorporating dyadic decision making</title><link>http://www.example.com/articles/1</link><description>Ding, Min; Eliashberg, Jehoshua
New products are often launched sequentially, by different firms, and the purchasing decisions are sometimes made by dyads. This paper proposes a new model that explicitly considers dyadic decision making in drug prescription and allows assessment of the relative influence that physicians and patients have in making decisions concerning new as well as existing ethical drugs. Modeling sequentially launched competing products in a category allows for parsing out effects that are hard to differentiate in models designed to capture only a single product's dynamics. The proposed model is applied to prescription drug data sets in the pharmaceutical industry, and it also explicitly captures both physicians' and patients' pretrial and posttrial utilities of each drug in the therapeutic category. Based on the model's fit and out-of-sample forecasting performance, we find that, in many cases, the incorporation of the dyadic decision making leads to better performance vis-A-vis models where such decision making is not explicitly considered. We also find that in many cases the posttrial utility of a drug is greater than its corresponding pretrial utility, lending partial empirical support to the prevailing industry practice of spending on various activities (e.g., sampling to physicians) needed to get potential patients to try a new drug. The proposed model enables managers to predict in advance the sales of sequentially launched new drugs and plan the new product launch and strategy accordingly. The model is also applicable to other product categories involving more than a single decision maker, including business-to-business products (e.g., office equipment) as well as to products targeting children (e.g., toys).</description><author>Ding, Min; Eliashberg, Jehoshua</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Regret and feedback information in first-price sealed-bid auctions</title><link>http://www.example.com/articles/1</link><description>Engelbrecht-Wiggans, Richard; Katok, Elena
We investigate the effect of regret-related feedback information on bidding behavior in sealed-bid first-price auctions. Two types of regret are possible in this auction format. A winner of the auction may regret paying too much relative to the second highest bid, and a loser may regret missing an opportunity to win at a favorable price. In theory, under very general conditions, being sensitive to winning and paying too much should result in lower average bids, and being sensitive to missing opportunities to win at a favorable price should result in higher bids. For example, the U.S. Government's policy of revealing losing bids may cause regret-sensitive bidders to anticipate regret and bid conservatively, decreasing the government's revenue. We test these predictions in the laboratory and find strong support for both.</description><author>Engelbrecht-Wiggans, Richard; Katok, Elena</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call center Outsourcing contracts under information asymmetry</title><link>http://www.example.com/articles/1</link><description>Hasija, Sameer; Pinker, Edieal J.; Shumsky, Robert A.
In this paper, we examine contracts to coordinate the capacity decision of a vendor who has been hired by a client to provide call center support. We consider a variety of contracts, all based on our observations of contracts used by one large vendor. We examine the role of different contract features such as pay-per-time, pay-per-call, service-level agreements, and constraints on service rates and abandonment. We show how different combinations of these contract features enable client firms to better manage vendors when there is information asymmetry about worker productivity. In particular, we focus on how different contracts can coordinate by yielding the system-optimal capacity decision by the vendor and consider how profits are allocated between the client and the vendor.</description><author>Hasija, Sameer; Pinker, Edieal J.; Shumsky, Robert A.</author><pubDate>Tue, 01 Apr 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>When is versioning optimal for information goods?</title><link>http://www.example.com/articles/1</link><description>Bhargava, Hemant K.; Choudhary, Vidyanand
This paper provides insights about when versioning is an optimal strategy for information goods. Our characterization of this class of goods is that variable costs are invariant with quality, including the special case of zero variable costs. Our analysis assumes a monopoly firm that has an existing product in the market and has an opportunity to segment the market by introducing additional lower-quality versions. We derive a simple decision rule for determining the optimality of versioning based on the solution to a single-product maximization problem. Versioning is optimal when the optimal market share of the lower-quality version, offered alone, is greater than the optimal market share of the high-quality version, offered alone. A firm can profitably employ versioning for an information good if it can design the lower quality in a way that, relative to their valuations for the high-end version, high-type consumers have a lower relative valuation for the lower quality than do low-type consumers. When variable costs increase, a firm that offered only one product version need not consider adding another version. When variable costs decrease, the firm should explore adding a lower-quality version.</description><author>Bhargava, Hemant K.; Choudhary, Vidyanand</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing an option on revenue from an innovation: An application to movie box office revenue</title><link>http://www.example.com/articles/1</link><description>Chance, Don M.; Hillebrand, Eric; Hilliard, Jimmy E.
We develop a model for valuing revenue streams from innovations. The stochastic properties of revenue from innovations create a more difficult environment in which to value options than when the underlying is a security. There is no initial revenue, and cumulative revenue cannot decrease. Revenues from innovations are characterized by different lives and different rates of the resolution of uncertainty. A common deterministic model for predicting revenue from an innovation is known as the Bass model. We embed the Bass model in a gamma process, resulting in a stochastic process with moments proportional to the mean of the Bass model. To illustrate this model we choose the valuation of options on movie box office revenue. These options enable film distributors to manage the risk of a movie, and they offer diversification opportunities for investors. We develop the econometric methodology for ex ante parameter estimation and a Bayesian updating scheme using Markov chain Monte Carlo simulation as data after release become available. Call prices obtained using the maximum likelihood (ML) parameter estimates from the full data set closely approximate the average discounted value of ex post call payouts that would have occurred at option maturity.</description><author>Chance, Don M.; Hillebrand, Eric; Hilliard, Jimmy E.</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Heterogeneity and network structure in the dynamics of diffusion: Comparing agent-based and differential equation models</title><link>http://www.example.com/articles/1</link><description>Rahmandad, Hazhir; Sterman, John
When is it better to use agent-based (AB) models, and when should differential equation (DE) models be used? Whereas DE models assume homogeneity and perfect mixing within compartments, AB models can capture heterogeneity across individuals and in the network of interactions among them. AB models relax aggregation assumptions, but entail computational and cognitive costs that may limit sensitivity analysis and model scope. Because resources are limited, the costs and benefits of such disaggregation should guide the choice of models for policy analysis. Using contagious disease as an example, we contrast the dynamics of a stochastic AB model with those of the analogous deterministic compartment DE model. We examine the impact of individual heterogeneity and different network topologies, including fully connected, random, Watts-Strogatz small world, scale-free, and lattice networks. Obviously, deterministic models yield a single trajectory for each parameter set, while stochastic models yield a distribution of outcomes. More interestingly, the DE and mean AB dynamics differ for several metrics relevant to public health, including diffusion speed, peak load on health services infrastructure, and total disease burden. The response of the models to policies can also differ even when their base case behavior is similar. In some conditions, however, these differences in means are small compared to variability caused by stochastic events, parameter uncertainty, and model boundary. We discuss implications for the choice among model types, focusing on policy design. The results apply beyond epidemiology: from innovation adoption to financial panics, many important social phenomena involve analogous processes of diffusion and social contagion.</description><author>Rahmandad, Hazhir; Sterman, John</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of uncertain intellectual property rights on the market for ideas: Evidence from patent grant delays</title><link>http://www.example.com/articles/1</link><description>Gans, Joshua S.; Hsu, David H.; Stern, Scott
This paper considers the impact of the intellectual property (IP) system on the timing of cooperation/licensing by start-up technology entrepreneurs. If the market for technology licenses is efficient, the timing of licensing is independent of whether IP has already been granted. In contrast, the need to disclose complementary (yet unprotected) knowledge, asymmetric information or search costs may retard efficient technology transfer. In these cases, reductions in uncertainty surrounding the scope and extent of IP rights may facilitate trade in the market for ideas. We employ a data set combining information about cooperative licensing and the timing of patent allowances (the administrative event when patent rights are clarified). Although preallowance licensing does occur, the hazard rate for achieving a cooperative licensing agreement significantly increases after patent allowance. Moreover, the impact of the patent system depends on the strategic and institutional environment in which firms operate. Patent allowance plays a particularly important role for technologies with longer technology life cycles or that lack alternative appropriation mechanisms such as copyright, reputation, or brokers. The findings suggest that imperfections in the market for ideas may be important, and that formal IP rights may facilitate gains from technological trade.</description><author>Gans, Joshua S.; Hsu, David H.; Stern, Scott</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fragmented property rights and incentives for R&amp;D</title><link>http://www.example.com/articles/1</link><description>Clark, Derek J.; Konrad, Kai A.
Where product innovation requires several complementary patents, fragmented property rights can limit firms' willingness to invest in R&amp;D. We consider the research intensity in multiple simultaneous R&amp;D contests and how it depends on whether firms already hold relevant patents as well as the availability of an option to invent around. A measure of technological uncertainty is also analyzed. The multiple patent product involves an important hold-up problem that can reduce the overall R&amp;D effort. Invent-around options moderate this problem. We also analyze targeted equilibria in which the aim of R&amp;D can be to hold up a rival.</description><author>Clark, Derek J.; Konrad, Kai A.</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sequential testing of product designs: Implications for learning</title><link>http://www.example.com/articles/1</link><description>Erat, Sanjiv; Kavadias, Stylianos
ast research in new product development (NPD) has conceptualized prototyping as a " design- build- testanalyze" cycle to emphasize the importance of the analysis of test results in guiding the decisions made during the experimentation process. New product designs often involve complex architectures and incorporate numerous components, and this makes the ex ante assessment of their performance dif. cult. Still, design teams often learn from test outcomes during iterative test cycles enabling them to infer valuable information about the performances of (as yet) untested designs. We conceptualize the extent of useful learning from analysis of a test outcome as depending on two key structural characteristics of the design space, namely whether the set of designs are " close" to each other (i. e., the designs are similar on an attribute level) and whether the design attributes exhibit nontrivial interactions (i. e., the performance function is complex). This study explicitly considers the design space structure and the resulting correlations among design performances, and examines their implications for learning. We derive the optimal dynamic testing policy, and we analyze its qualitative properties. Our results suggest optimal continuation only when the previous test outcomes lie between two thresholds. Outcomes below the lower threshold indicate an overall low performing design space and, consequently, continued testing is suboptimal. Test outcomes above the upper threshold, on the other hand, merit termination because they signal to the design team that the likelihood of obtaining a design with a still higher performance (given the experimentation cost) is low. We find that accounting for the design space structure splits the experimentation process into two phases: the initial exploration phase, in which the design team focuses on obtaining information about the design space, and the subsequent exploitation phase in which the design team, given their understanding of the design space, focuses on obtaining a "good enough" configuration. Our analysis also provides useful contingency-based guidelines for managerial action as information gets revealed through the testing cycle. Finally, we extend the optimal policy to account for design spaces that contain distinct design subclasses.</description><author>Erat, Sanjiv; Kavadias, Stylianos</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does component sharing help or hurt reliability? An empirical study in the automotive industry</title><link>http://www.example.com/articles/1</link><description>Ramdas, Kamalini; Randall, Taylor
Component sharing-the use of a component on multiple products within a firm's product line-is widely practiced as a means of offering high variety at low cost. Although many researchers have examined tradeoffs involved in component sharing, little research has focused on the impact of component sharing on quality. In this paper, we examine how component sharing impacts one dimension of quality-reliability-defined as mean time to failure. Design considerations suggest that a component designed uniquely for a product will result in higher reliability due to the better fit of the component within the architecture of the product. On the other hand, the learning curve literature suggests that greater experience with a component can improve conformance quality, and can increase reliability via learning from end-user feedback. The engineering literature suggests that improved conformance in turn increases reliability. Sharing a component across multiple products increases experience, and hence, should increase reliability. Using data from the automotive industry, we find support for the hypothesis that higher component reliability is associated with higher cumulative experience with a component. Further, we find support for the hypothesis that higher component reliability is associated with a component that has been designed uniquely for a product. This finding suggests that the popular design strategy of component sharing can in some cases compromise product quality, via reduced reliability.</description><author>Ramdas, Kamalini; Randall, Taylor</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theoretical framework for managing the new product development portfolio: When and how to use strategic buckets</title><link>http://www.example.com/articles/1</link><description>Chao, Raul O.; Kavadias, Stylianos
Developing the "right" new products is critical to firm success and is often cited as a key competitive dimension. This paper explores new product development (NPD) portfolio strategy and the balance between incremental and radical innovation. We characterize innovative effort through a normative theoretical framework that addresses a popular practice in NPD portfolio management: the use of strategic buckets. Strategic buckets encourage the division of the overall NPD resource budget into smaller, more focused budgets that are defined by the type of innovative effort (incremental or radical). We show that time commitment determines the balance between incremental and radical innovation. When managers execute this balance, they are often confounded by (i) environmental complexity, defined as the number of unknown interdependencies among technology and market parameters that determine product performance; and (ii) environmental instability, the probability of changes to the underlying performance functions. Although both of these factors confound managers, we find that they have completely opposite effects on the NPD portfolio balance. Environmental complexity shifts the balance toward radical innovation. Conversely, environmental instability shifts the balance toward incremental innovation. Risk considerations and implications for theory and practice are also discussed.</description><author>Chao, Raul O.; Kavadias, Stylianos</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Appropriability and commercialization: Evidence from MIT inventions</title><link>http://www.example.com/articles/1</link><description>Dechenaux, Emmanuel; Goldfarb, Brent; Shane, Scott; Thursby, Marie
The effects of appropriability on invention have been well studied, but there has been little analysis of the effect of appropriability on the commercialization of existing inventions. Exploiting a database of 805 attempts by private firms to commercialize inventions licensed from the Massachusetts Institute of Technology (MIT) between 1980 and 1996, we explore the influence of several appropriability mechanisms on the commercialization and termination of projects to develop products based on university inventions. Our central hypothesis is that the relationship between a licensee's decision to either terminate or commercialize the invention is driven by the current market value of the invention, as well as the option value of delaying its commercialization. We use a competing risks framework that allows for nonparametric heterogeneity and correlated risks. We find that better appropriability in the sense of more effective patent strength and secrecy has a strong negative effect on the hazard of license termination. The effectiveness of learning has a strong positive effect on the hazard of technology commercialization, while lead time has a negative effect.</description><author>Dechenaux, Emmanuel; Goldfarb, Brent; Shane, Scott; Thursby, Marie</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of product introduction delays on operating performance</title><link>http://www.example.com/articles/1</link><description>Hendricks, Kevin B.; Singhal, Vinod R.
his paper provides empirical evidence on the effect of product introduction delays on accounting-based measures of operating performance. Based on a diverse set of 450 publicly traded firms that experienced product introduction delays, we find that delays have a statistically significant negative effect on profitability. Depending on the method used to estimate abnormal performance, the median abnormal decline in return on assets (ROA) ranges from 2.70% to 3.44% over a three-year period around the year of the delay announcement. The median decline in sales over assets ranges from 5.92% to 10.99%, and the median decline in return on sales ranges from 1.48% to 3.06%. Cross-sectional regression analysis indicates that the impact of delays on abnormal ROA is more negative for smaller firms, and for firms that are more pro. table before the delay. Furthermore, the impact is more negative for firms that operate in industries that are larger and more profitable. We also find a positive association between abnormal ROA and abnormal stock price performance around the product introduction delay announcements.</description><author>Hendricks, Kevin B.; Singhal, Vinod R.</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The hidden perils of career concerns in R&amp;D organizations</title><link>http://www.example.com/articles/1</link><description>Siemsen, Enno
This research examines how certain economic incentives can lead agents to prefer difficult solutions to their organizational tasks that reduce their likelihood of obtaining a successful task outcome. Drawing on the career concerns literature, I model a decision context in which agents have the operational autonomy to choose among different solutions. Analyzing this model reveals that agents with career concerns will, under certain conditions, deliberately reduce their likelihood of succeeding with a task by choosing a solution that makes their task more difficult. These findings suggest that highly capable agents prefer moderately difficult tasks to showcase their capability, while less-capable agents prefer highly difficult tasks to mask their lack of capability. The robustness of these results is examined in various extensions. Furthermore, the paper provides insights on how incentives can be designed to alleviate this problem.</description><author>Siemsen, Enno</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The dual role of modularity: Innovation and imitation</title><link>http://www.example.com/articles/1</link><description>Ethiraj, Sendil K.; Levinthal, Daniel; Roy, Rishi R.
Modularity has been heralded as an organizational and technical architecture that enhances incremental and modular innovation. Less attention has been paid to the possible implications of modular architectures for imitation. To understand the implications of modular designs for competitive advantage, one must consider the dual impact of modularity on innovation and imitation jointly. In an attempt to do so, we set up three alternative structures that vary in the extent of modularity and hence in the extent of design complexity: nonmodular, modular, and nearly modular designs. In each structure, we examine the trade-offs between innovation benefits and imitation deterrence. The results of our computational experiments indicate that modularization enables performance gains through innovation but, at the same time, sets the stage for those gains to be eroded through imitation. In contrast, performance differences between the leaders and imitators persist in the nearly modular and the nonmodular structures. Overall, we find that design complexity poses a significant trade-off between innovation benefits (i.e., generating superior strategies that create performance differences) and imitation deterrence (i.e., preserving the performance differences). We also examine the robustness of our results to variations in imitation accuracy. In addition to documenting the overall robustness of our principal finding, the ancillary analyses provide a more nuanced rendering of the relationship between the architecture of complexity and imitation efforts.</description><author>Ethiraj, Sendil K.; Levinthal, Daniel; Roy, Rishi R.</author><pubDate>Thu, 01 May 2008 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interfirm collaboration networks: The impact of large-scale network structure on firm innovation</title><link>http://www.example.com/articles/1</link><description>Schilling, Melissa A.; Phelps, Corey C.
The structure of alliance networks influences their potential for knowledge creation. Dense local clustering provides information transmission capacity in the network by fostering communication and cooperation. Nonredundant connections contract the distance between firms and give the network greater reach by tapping a wider range of knowledge resources. We propose that firms embedded in alliance networks that exhibit both high clustering and high reach (short average path lengths to a wide range of firms) will have greater innovative output than firms in networks that do not exhibit these characteristics. We find support for this proposition in a longitudinal study of the patent performance of 1,106 firms in 11 industry-level alliance networks.</description><author>Schilling, Melissa A.; Phelps, Corey C.</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Complexity and the character of stock returns: Empirical evidence and a model of asset prices based on complex investor learning</title><link>http://www.example.com/articles/1</link><description>Linn, Scott C.; Tay, Nicholas S. P.
Empirical evidence on the distributional characteristics of common stock returns indicates: (1) A power-law tail index close to three describes the behavior of the positive tail of the survivor function of returns (pr(r &gt; x) similar to x(-alpha)), a reflection of fat tails; (2) general linear and nonlinear dependencies exist in the time series of returns; (3) the time-series return process is characterized by short-run dependence (short memory) in both returns as well as their volatility, the latter usually characterized in the form of autoregressive conditional heteroskedasticity; and (4) the time-series return process probably does not exhibit long memory, but the squared returns process does exhibit long memory. We propose a model of complex, self-referential learning and reasoning amongst economic agents that jointly produces security returns consistent with these general observed facts and which are supported here by empirical results presented for a benchmark sample of 50 stocks traded on the New York Stock Exchange. The market we postulate is populated by traders who reason inductively while compressing information into a few fuzzy notions that they can in turn process and analyze with fuzzy logic. We analyze the implications of such behavior for the returns on risky securities within the context of an artificial stock market model. Dynamic simulation experiments of the market are conducted, from which market-clearing prices emerge, allowing us to then compute realized returns. We test the effects of varying values of the parameters of the model on the character of the simulated returns. The results indicate that the model proposed in this paper can jointly account for the presence of a power-law characterization of the positive tail of the survivor function of returns with exponent on the order of three, for autoregressive conditional heteroskedasticity, for long memory in volatility, and for general nonlinear dependencies in returns.</description><author>Linn, Scott C.; Tay, Nicholas S. P.</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analyzing consumer-product graphs: Empirical findings and applications in recommender systems</title><link>http://www.example.com/articles/1</link><description>Huang, Zan; Zeng, Daniel D.; Chen, Hsinchun
We apply random graph modeling methodology to analyze bipartite consumer-product graphs that represent sales transactions to better understand consumer purchase behavior in e-commerce settings. Based on two real-world e-commerce data sets, we found that such graphs demonstrate topological features that deviate significantly from theoretical predictions based on standard random graph models. In particular, we observed consistently larger-than-expected average path lengths and a greater-than-expected tendency to cluster. Such deviations suggest that the consumers' product choices are not random even with the consumer and product attributes hidden. Our findings provide justification for a large family of collaborative filtering-based recommendation algorithms that make product recommendations based only on previous sales transactions. By analyzing the simulated consumer-product graphs generated by models that embed two representative recommendation algorithms, we found that these recommendation algorithm-induced graphs generally provided a better match with the real-world consumer-product graphs than purely random graphs. However, consistent deviations in topological features remained. These findings motivated the development of a new recommendation algorithm based on graph partitioning, which aims to achieve high clustering coefficients similar to those observed in the real-world e-commerce data sets. We show empirically that this algorithm significantly outperforms representative collaborative filtering algorithms in situations where the observed clustering coefficients of the consumer-product graphs are sufficiently larger than can be accounted for by these standard algorithms.</description><author>Huang, Zan; Zeng, Daniel D.; Chen, Hsinchun</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The statistical mechanics of complex product development: Empirical and analytical results</title><link>http://www.example.com/articles/1</link><description>Braha, Dan; Bar-Yam, Yaneer
In recent years, understanding the structure and function of complex networks has become the foundation for explaining many different real-world complex biological, technological, and informal social phenomena. Techniques from statistical physics have been successfully applied to the analysis of these networks, and have uncovered surprising statistical structural properties that have also been shown to have a major effect on their functionality dynamics, robustness, and fragility. This paper examines, for the first time, the statistical properties of strategically important organizational networks-networks of people engaged in distributed product development (PD)-and discusses the significance of these properties in providing insight into ways of improving the strategic and operational decision making of the organization. We show that the structure of information flow networks that are at the heart of large-scale product development efforts have properties that are similar to those displayed by other social, biological, and technological networks. In this context, we also identify novel properties that may be characteristic of other information-carrying networks. We further present a detailed model and analysis of PD dynamics on complex networks, and show how the underlying network topologies provide direct information about the characteristics of these dynamics. We believe that our new analysis methodology and empirical results are also relevant to other organizational information-carrying networks.</description><author>Braha, Dan; Bar-Yam, Yaneer</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Call-center labor cross-training: It's a small world after all</title><link>http://www.example.com/articles/1</link><description>Iravani, Seyed M. R.; Kolfal, Bora; Van Oyen, Mark P.
It is well known that flexibility can be created in manufacturing and service operations by using multipurpose production sources such as cross-trained labor, flexible machines, or flexible factories. We focus on flexible service centers, such as inbound call centers with cross-trained agents, and model them as parallel queueing systems with flexible servers. We propose a new approach to analyzing flexibility arising from the multifunctionality of sources of production. We create a work sharing (WS) network model for which its average shortest path length (APL) metric can predict the more effective of two alternative cross-training structures in terms of customer waiting times. We show that the APL metric of small world network (SWN) theory is one. simple deterministic solution approach to the complex stochastic problem of designing effective workforce cross-training structures in call centers.</description><author>Iravani, Seyed M. R.; Kolfal, Bora; Van Oyen, Mark P.</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Membership herding and network stability in the open source community: The Ising perspective</title><link>http://www.example.com/articles/1</link><description>Oh, Wonseok; Jeon, Sangyong
The aim of this paper is twofold: (1) to conceptually understand membership dynamics in the open source software (OSS) community, and (2) to explore how different network characteristics (i.e., network size and connectivity) influence the stability of an OSS network. Through the lens of Ising theory, which is widely accepted in physics, we investigate basic patterns of interaction and present fresh conceptual insight into dynamic and reciprocal relations among OSS community members. We also perform computer simulations based on empirical data collected from two actual OSS communities. Key findings include: (1) membership herding is highly present when external influences (e.g., the availability of other OSS projects) are weak, but decreases significantly when external influences increase, (2) propensity for membership herding is most likely to be seen in a large network with random connectivity, and (3) for large networks, when external influences are weak, random connectivity will result in higher network strength than scale-free connectivity (as external influences increase, however, the reverse phenomenon is observed). In addition, scale-free connectivity appears to be less volatile than random connectivity in response to an increase in the strength of external influences. We conclude with several implications that may be of significance to OSS stakeholders in particular, and to a broader range of online communities in general.</description><author>Oh, Wonseok; Jeon, Sangyong</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Patterned interactions in complex systems: Implications for exploration</title><link>http://www.example.com/articles/1</link><description>Rivkin, Jan W.; Siggelkow, Nicolaj
Scholars who view organizational, social, and technological systems as sets of interdependent decisions have increasingly used simulation models from the biological and physical sciences to examine system behavior. These models shed light on an enduring managerial question: How much exploration is necessary to discover a good configuration of decisions? The models suggest that, as interactions across decisions intensify and local optima proliferate, broader exploration is required. The models typically assume, however, that the interactions among decisions are distributed randomly. Contrary to this assumption, recent empirical studies of real organizational, social, and technological systems show that interactions among decisions are highly patterned. Patterns such as centralization, small-world connections, power-law distributions, hierarchy, and preferential attachment are common. We embed such patterns into an NK simulation model and obtain dramatic results: Holding fixed the total number of interactions among decisions, a shift in the pattern of interaction can alter the number of local optima by more than an order of magnitude. Thus, the long-ran value of broader exploration is significantly greater in the face of some interaction patterns than in the face of others. We develop simple, intuitive rules of thumb that allow a decision maker to examine two interaction patterns and determine which warrants greater investment in broad exploration. We also find that, holding fixed the interaction pattern, an increase in the number of interactions raises the number of local optima regardless of the pattern. This validates prior comparative static results with respect to the number of interactions, but highlights an important implicit assumption in earlier work-that the underlying interaction pattern remains constant as interactions become more numerous.</description><author>Rivkin, Jan W.; Siggelkow, Nicolaj</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bilateral collaboration and the emergence of innovation networks</title><link>http://www.example.com/articles/1</link><description>Cowan, Robin; Jonard, Nicolas; Zimmermann, Jean-Benoit
In this paper, we model the formation of innovation networks as they emerge from bilateral decisions. In contrast to much of the literature, here firms only consider knowledge production, and not network issues, when deciding on partners. Thus, we focus attention on the effects of the knowledge and information regime on network formation. The effectiveness of a bilateral collaboration is determined by cognitive, relational, and structural embeddedness. Innovation results from the recombination of knowledge held by the partners to the collaboration, and its success is determined in part by the extent to which firms' knowledge complement each other. Previous collaborations (relational embeddedness) increase the probability of a successful collaboration, as does information gained from common third parties (structural embeddedness). Repeated alliance formation creates a network. Two features are central to the innovation process: how firms pool their knowledge resources, and how, firms derive information about potential partners. When innovation is decomposable into separate subtasks, networks tend to be dense; when structural embeddedness is important, networks become cliquish. For some regions in this parameter space, small worlds emerge.</description><author>Cowan, Robin; Jonard, Nicolas; Zimmermann, Jean-Benoit</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cooperation in evolving social networks</title><link>http://www.example.com/articles/1</link><description>Hanaki, Nobuyuki; Peterhansl, Alexander; Dodds, Peter S.; Watts, Duncan J.
We study the problem of cooperative behavior emerging in an environment where individual behaviors and interaction structures coevolve. Players not only learn which strategy to adopt by imitating the strategy of the best-performing player they observe, but also choose with whom they should interact by selectively creating and/or severing ties with other players based on a myopic cost-benefit comparison. We find that scalable cooperation-that is, high levels of cooperation in large populations-can be achieved in sparse networks, assuming that individuals are able to sever ties unilaterally and that new ties can only be created with the mutual consent of both parties. Detailed examination shows that there is an important trade-off between local reinforcement and global expansion in achieving cooperation in dynamic networks. As a result, networks in which ties are costly and local structure is largely absent tend to generate higher levels of cooperation than those in which ties are made easily and friends of friends interact with high probability, where the latter result contrasts strongly with the usual intuition.</description><author>Hanaki, Nobuyuki; Peterhansl, Alexander; Dodds, Peter S.; Watts, Duncan J.</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Complex systems - A new paradigm for the integrative study of management, physical, and technological systems</title><link>http://www.example.com/articles/1</link><description>Amaral, Luis A. Nunes; Uzzi, Brian
In this introductory note, we describe the motivation for this special issue on complex systems. We begin by noting the potential management opportunities offered by recent advances in complexity science. After defining the nature of complex systems and the many ways they are expressed in organizations and markets, we briefly describe the main tools and concepts of complexity theory. We close with a brief review of the articles in this issue and their relevance to the interests and concerns of managers.</description><author>Amaral, Luis A. Nunes; Uzzi, Brian</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Emergent properties of a new financial market: American venture capital syndication, 1960-2005</title><link>http://www.example.com/articles/1</link><description>Kogut, Bruce; Urso, Pietro; Walker, Gordon
The recorded transactions of venture capital investments permit a direct examination of the Braudel hypothesis that regional markets evolve dynamically and interdependently in reference to a global system. This hyothesis contradicts the popular belief that regional financial development is anchored in dense clusters. Using methods of complex graphs, we analyze 159,561 transactions over nearly 45 years to demonstrate the rapid emergence of a national network of syndications. A giant component emerges early in the history of the industry, which subsumes the regional and sectoral subgraphs. The results confirm the Braudel hypothesis over the role of regional clusters, rejects preferential attachment in favor of repeated ties among trusted partners, and emphasizes the importance of dynamics and complex weighted graphs for the analysis of social and economic behavior.</description><author>Kogut, Bruce; Urso, Pietro; Walker, Gordon</author><pubDate>Sun, 01 Jul 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risk mitigation in newsvendor networks: Resource diversification, flexibility, sharing, and hedging</title><link>http://www.example.com/articles/1</link><description>Van Mieghem, Jan A.
This paper studies how judicious resource allocation in networks mitigates risk. Theory is presented for general utility functions and mean-variance formulations and is illustrated with networks featuring resource diversification, flexibility (e.g., inventory substitution), and sharing (commonality). In contrast to single-resource settings, risk-averse newsvendors may invest more in networks than risk-neutral newsvendors: some resources and even total spending may exceed risk-neutral levels. With normally distributed demand, risk-averse newsvendors change resource levels roughly proportionally to demand variance, while risk-neutral agents adjust only proportionally to standard deviation. Two effects explain this operational hedge and suggest rules of thumb for strategic placement of safety capacity and inventory in networks: (1) Risk pooling suggests rebalancing capacity toward inexpensive resources that serve lower-profit variance markets. This highlights the role of profit variance (instead of demand variance) in risk-averse network investment. (2) Ex post revenue maximization suggests rebalancing capacity toward substitutable flexible but away from shared capacity when markets differ in profitability. Capacity imbalance and allocation flexibility thus mitigate profit risk and truly are operational hedges.</description><author>Van Mieghem, Jan A.</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A market-based optimization algorithm for distributed systems</title><link>http://www.example.com/articles/1</link><description>Guo, Zhiling; Koehler, Gary J.; Whinston, Andrew B.
In this paper, a market-based decomposition method for decomposable linear systems is developed. The solution process iterates between a master problem that solves the market-matching problem, and subproblems that solve the agents' bundle-determination problems. Starting from any initial price and feasible allocation, system optimality can be achieved under a dynamic market-trading algorithm in a finite number of trades. The final market-clearing prices are discovered by this market trading and an efficient allocation is achieved by direct, wealth-improving resource exchanges among self-interested agents. Certain types of strategic behavior by the agents and a dealer in the marketplace are studied as well. Our proposed market mechanism addresses price dynamics, incentive issues, and economic transactions of real-world, distributed decision-making situations more realistically than traditional decomposition approaches. In addition, it can be operated in both synchronous and asynchronous environments. We provide a market-based paradigm for decentralized problem solving and information processing that can be easily implemented to support real-time optimization of distributed systems.</description><author>Guo, Zhiling; Koehler, Gary J.; Whinston, Andrew B.</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does transaction misalignment matter for firm survival at all stages of the industry life cycle?</title><link>http://www.example.com/articles/1</link><description>Argyres, Nicholas; Bigelow, Lyda
Research on industry life cycles suggests that competitive pressures are more severe during the shakeout stage, which could be associated with the emergence of a dominant design, than at other stages. Transaction-cost theory, on the other hand, assumes generally competitive markets and does not address the industry life cycle. It therefore implies that transaction-cost economizing is a superior firm strategy regardless of the stage of the life cycle. This paper seeks to reconcile these two streams of research by investigating whether aligning transactions with governance modes in accordance with transaction-cost prescriptions has a differential effect on firm survival in preshakeout versus shakeout stages of the industry life cycle. Analyzing data from the early U.S. auto industry (1917-1933), we find that while transaction misalignment did not have a significant impact on firm survival during the preshakeout stage or during the period as a whole, it did have a significantly larger negative impact on survival during the shakeout stage than during the preshakeout stage. We also find that the negative effects of misalignment on survival were significantly weaker for larger firms during the shakeout stage. This suggests that applications of transaction-cost theory which assume uniformly severe selection pressures across the industry life cycle and uniform effects of misalignment across firms of different sizes could be misleading. It also suggests that theories of the industry life cycle could usefully take transaction costs into account along with production costs in their analyses of competition over the life cycle.</description><author>Argyres, Nicholas; Bigelow, Lyda</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning from experience in software development: A multilevel analysis</title><link>http://www.example.com/articles/1</link><description>Boh, Wai Fong; Slaughter, Sandra A.; Espinosa, J. Alberto
This study examines whether individuals, groups, and organizational units learn from experience in software development and whether this learning improves productivity. Although prior research has found the existence of learning curves in manufacturing and service industries, it is not clear whether learning curves also apply to knowledge work like software development. We evaluate the relative productivity impacts from accumulating specialized experience in a system, diversified experience in related and unrelated systems, and experience from working with others on modification requests (MRs) in a telecommunications firm, which uses an incremental software development methodology. Using multilevel modeling, we analyze extensive data archives covering more than 14 years of systems development work on a major telecommunications product dating from the beginning of its development process. Our findings reveal that the relative importance of the different types of experience differs across levels of analysis. Specialized experience has the greatest impact on productivity for MRs completed by individual developers, whereas diverse experience in related systems plays a larger role in improving productivity for MRs and system releases completed by groups and organizational units. Diverse experience in unrelated systems has the least influence on productivity at all three levels of analysis. Our findings support the existence of learning curves in software development and provide insights into when specialized or diverse experience may be more valuable.</description><author>Boh, Wai Fong; Slaughter, Sandra A.; Espinosa, J. Alberto</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fairness and channel coordination</title><link>http://www.example.com/articles/1</link><description>Cui, Tony Haitao; Raju, Jagmohan S.; Zhang, Z. John
We incorporate the concept of fairness in a conventional dyadic channel to investigate how fairness may affect channel coordination. We show that when channel members are concerned about fairness, the manufacturer can use a simple wholesale price above her marginal cost to coordinate this channel both in terms of achieving the maximum channel profit and in terms of attaining the maximum channel utility Thus, channel coordination may not require an elaborate pricing contract. A constant wholesale price will do.</description><author>Cui, Tony Haitao; Raju, Jagmohan S.; Zhang, Z. John</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Existence of coordinating transshipment prices in a two-location inventory model</title><link>http://www.example.com/articles/1</link><description>Hu, Xinxin; Duenyas, Izak; Kapuscinski, Roman
We consider a two-location production/inventory model where each location makes production decisions and is subject to uncertain capacity. Each location optimizes its own profits. Transshipment (at a cost) is allowed from one location to another. We focus on the question of whether one can globally set a pair of coordinating transshipment prices, i.e., payments that each party has to make to the other for the transshipped goods, that induce the local decision makers to make inventory and transshipment decisions that are globally optimal. A recent paper suggests, for a special case of our model, that there always exists a unique pair of coordinating transshipment prices. We demonstrate through a counterexample that this statement is not correct and derive sufficient and necessary conditions under which it would hold. We show that in some conditions, coordinating prices may exist for only a narrow range of problem parameters and explore conditions when this can happen. Finally, we study the effects of demand and capacity variability on the magnitude of coordinating transshipment prices.</description><author>Hu, Xinxin; Duenyas, Izak; Kapuscinski, Roman</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The price of anarchy in supply chains: Quantifying the efficiency of price-only contracts</title><link>http://www.example.com/articles/1</link><description>Perakis, Georgia; Roels, Guillaume
In this paper, we quantify the efficiency of decentralized supply chains that use price-only contracts. With Ia price-only contract, a buyer and a seller agree only on a constant transaction price, without specifying the amount that will be transferred. It is well known that these contracts do not provide incentives to the parties to coordinate their inventory/capacity decisions. We measure efficiency with the price of anarchy (PoA), defined as the largest ratio of profits between the integrated supply chain (that is, fully coordinated) and the decentralized supply chain. We characterize the efficiency of various supply chain configurations: push or pull inventory positioning, two or more stages, serial or assembly systems, single or multiple competing suppliers, and single or multiple competing retailers.</description><author>Perakis, Georgia; Roels, Guillaume</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effects of information-revelation policies under market-structure uncertainty</title><link>http://www.example.com/articles/1</link><description>Arora, Ashish; Greenwald, Amy; Kannan, Karthik; Krishnan, Ramayya
Geographically dispersed sellers in electronic reverse marketplaces such as those hosted by market-makers like Ariba are uncertain about the number of competitors they face in any given market session. We refer to this uncertainty about the number of competitors as market-structure uncertainty. Over the course of several market sessions sellers learn about the competitive nature of the marketplace. How they learn to reduce the market-structure uncertainty depends on the market-transparency scheme, or the revelation policy adopted. A revelation policy determines the extent to which information-the number of sellers in a session, their bidding patterns, etc.-is revealed to sellers. Because these policies control what sellers learn, and how they bid in future sessions, they determine buyer surplus. Possibly because market-structure uncertainty is more prevalent in information technology-enabled marketplaces than traditional ones, prior work has not addressed the impact of revelation policies on this type of uncertainty. Currently, there is little guidance available to buyers in choosing, the appropriate revelation policy. To address this information-technology-enabled problem, we use game theory to compare the buyer surplus generated under a set of revelation policies commonly used in electronic reverse marketplaces. We demonstrate that the policy that generates the least amount of market-structure uncertainty for the sellers always maximizes buyer surplus. We further investigate to provide intuition regarding how bidders' reactions to overcome uncertainty differs with the nature of uncertainty, and how those reactions impact buyer surplus.</description><author>Arora, Ashish; Greenwald, Amy; Kannan, Karthik; Krishnan, Ramayya</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of prior decisions on subsequent valuations in a costly contemplation model</title><link>http://www.example.com/articles/1</link><description>Ofek, Elie; Yildiz, Muhamet; Haruvy, Ernan
This paper develops and tests a model of how recall of information from past decisions affects subsequent related decisions. A boundedly rational individual has to determine her willingness to pay for a good that she previously considered purchasing at a given price, or provide valuations for a set of goods that she previously ranked in order of preference. The individual is ex ante uncertain about her utility from consumption of the goods and can exert costly cognitive effort to reduce this uncertainty. We show that incorporating information from a prior decision has three primary effects: (a) Valuations are expected to exhibit higher variance in particular, the spread of valuations between the most and least preferred alternatives increases; (b) decision makers will, in expectation, exert more effort during the valuation phase; and (c) the relative impact of prior decisions on valuation spread increases, the more each attribute contributes to overall utility. The model predictions are then tested in a series of controlled lab experiments.</description><author>Ofek, Elie; Yildiz, Muhamet; Haruvy, Ernan</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Industry level supplier-driven IT spillovers</title><link>http://www.example.com/articles/1</link><description>Cheng, Zhuo; Nault, Barrie R.
We model and estimate the effects to downstream productivity from information technology (IT) investments made upstream. Specifically, we examine how an industry's productivity is affected by the IT capital stock of its suppliers. These supplier-driven IT spillovers occur because, due to competition in the supplying industry, quality benefits from suppliers' IT investments can pass downstream. If the output deflators of supplying industries (consequently the intermediate input deflator of the using industries) do not capture the quality improvement from IT, then the output productivity of the supplying industries is mismeasured or misassigned. We develop and empirically test a model capturing these supplier-driven effects using data on 85 manufacturing industries at the three-digit SIC code level. We find that for a 10.5% increase in suppliers' IT capital, the suppliers' output increases by 0.63%-0.70%, which is more than covering the cost of the increase in suppliers' IT capital. In addition, this increase in suppliers' IT capital increases the average downstream industry's output by $66-$72 million, thereby confirming substantial supplier-driven IT spillovers downstream. We also infer the magnitude of the measurement error of the price deflator of the intermediate input resulting from the failure to account for IT-related quality improvement, finding that the measured price deflator overestimates the true deflator by approximately 30% at the mean level of IT capital.</description><author>Cheng, Zhuo; Nault, Barrie R.</author><pubDate>Wed, 01 Aug 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic agency with renegotiation and managerial tenure</title><link>http://www.example.com/articles/1</link><description>Sabac, Florin
T his paper proves the renegotiation-proofness principle for a dynamic LEN (linear contracts, exponential utility, normal distributions) model and examines the impact of repeated renegotiation on incentives and managerial tenure when performance information is serially correlated. In addition to providing a general solution to a multiperiod agency problem with serially correlated performance measures, this paper characterizes optimal managerial tenure/turnover policies as a function of the time-series properties of performance measures. With negatively correlated performance measures, the principal prefers longer managerial tenure, and no turnover is optimal. With positively correlated performance measures, absent a switching cost, turnover every period is optimal. In the presence of a fixed switching cost, interior optimal turnover policies exist if the performance measures are positively correlated. Switching costs are necessary, but not sufficient for interior optimal tenure. The optimal turnover policies present an alternative to theories of performance-driven managerial turnover and are consistent with evidence that a majority of managerial turnovers are (age-related) normal retirements.</description><author>Sabac, Florin</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managerial motivation dynamics and incentives</title><link>http://www.example.com/articles/1</link><description>Kocabiyikoglu, Ayse; Popescu, Ioana
Firms can increase profitability by appropriately motivating managers. We investigate drivers of managerial motivation, and propose how firms can use performance pay to alter motivational patterns. We focus on the agent's optimal effort decision in trading off compensation utility with effort cost in a static and dynamic setting. Surprisingly, we find that lower risk aversion or increased pay are not necessarily motivating factors, and identify the relevant effort drivers underlying the agent's utility and compensation plan. We characterize properties of agents' preferences for output lotteries (risk aversion, aggressiveness, prudence) that trigger systematic motivational patterns with respect to a variety of factors, such as the agent's productivity and past performance, time to evaluation, the firm's capabilities, and market factors. Our insights are robust, holding under very general modeling assumptions on preferences, rewards, and the stochastic effort-output function.</description><author>Kocabiyikoglu, Ayse; Popescu, Ioana</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Temporary and permanent buyout prices in online auctions</title><link>http://www.example.com/articles/1</link><description>Gallien, Jeremie; Gupta, Shobhit
Buyout options allow bidders to instantly purchase at a specified price an item listed for sale through an 10 online auction. A temporary buyout option disappears once a regular bid is submitted, whereas a permanent option remains available until it is exercised or the auction ends. Such buyout price may be static and remain constant throughout the auction, or dynamic and vary as the auction progresses. We formulate a game-theoretic model featuring time-sensitive bidders with independent private values and Poisson arrivals but endogenous bidding times to answer the following questions: How should a seller set the buyout price (if at all)? What are the implications of using a temporary buyout option relative to a permanent one? What is the potential benefit associated with using a dynamic buyout price? For all buyout option types we exhibit a Nash equilibrium in bidder strategies, argue that this equilibrium constitutes a plausible outcome prediction, and study the problem of maximizing the corresponding seller revenue. Our numerical experiments suggest that when any participant is time sensitive, the seller may significantly increase his utility by introducing a buyout option, but that dynamic buyout prices may not provide a substantial advantage over static ones. Furthermore, whereas permanent buyout options yield higher predicted revenue than temporary options, they also provide additional incentives for late bidding and may therefore not be always more desirable.</description><author>Gallien, Jeremie; Gupta, Shobhit</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Online auction and list price revenue management</title><link>http://www.example.com/articles/1</link><description>Caldentey, Rene; Vulcano, Gustavo
W e analyze a revenue management problem in which a seller facing a Poisson arrival stream of consumers operates an online multiunit auction. Consumers can get the product from an alternative list price channel. We consider two variants of this problem: In the first variant, the list price is an external channel run by another firm. In the second one, the seller manages both the auction and the list price channels. Each consumer, trying to maximize his own surplus, must decide either to buy at the posted price and get the item at no risk, or to join the auction and wait until its end, when the winners are revealed and the auction price is disclosed. Our approach consists of two parts. First, we study structural properties of the problem, and show that the equilibrium strategy for both versions of this game is of the threshold type, meaning that a consumer will join the auction only if his arrival time is above a function of his own valuation. This consumer's strategy can be computed using an iterative algorithm in a function space, provably convergent under some conditions. Unfortunately, this procedure is computationally intensive. Second, and to overcome this limitation, we formulate an asymptotic version of the problem, in which the demand rate and the initial number of units grow proportionally large. We obtain a simple closed-form expression for the equilibrium strategy in this regime, which is then used as an approximate solution to the original problem. Numerical computations show that this heuristic is very accurate. The asymptotic solution culminates in simple and precise recipes of how bidders should behave, as well as how the seller should structure the auction, and price the product in the dual-channel case.</description><author>Caldentey, Rene; Vulcano, Gustavo</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the benefits of collaborative forecasting partnerships between retailers and manufacturers</title><link>http://www.example.com/articles/1</link><description>Aviv, Yossi
This paper studies the potential benefits of collaborative forecasting (CF) partnerships in a supply chain that consists of a manufacturer and a retailer. To reflect the reality in production environments, we propose a scorecard that captures inventory considerations, production smoothing, and adherence-to-plans. We present a prescriptive convex-cost production planning model for the manufacturer, and a replenishment model for the retailer. We use our integrative reference model to study the potential benefits of CF partnerships. Overall, we find that the benefits of CF depend on the following key characteristics of the supply chain: the relative explanatory power of the supply chain partners, the supply side agility, and the internal service rate. CF is expected to bring high benefits to the supply chain when the manufacturer has the largest relative explanatory power. But quite disappointingly, in these cases a CF partnership does not appear to be valuable to the manufacturer. When the retailer is the dominant observer of market signals, CF typically yields a "win-win" outcome. In order to effectively act upon the information exchanged via CF, the supply side needs to be sufficiently agile. The benefits reported in this paper should be considered as conservative. This is because CF partnerships often bring better information, improved decision support technologies, as well as process improvement to the trading partners. Consequently, the supply side agility can be improved. If this indeed happens, the compound benefits of CF can be dramatically higher than our conservative estimates. Finally, we provide a qualitative discussion of the possible role of internal service rates in supply chains, either as planning parameters to improve performance, or as a mechanism for sharing the benefits of CF between the trading partners.</description><author>Aviv, Yossi</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Location strategies and knowledge spillovers</title><link>http://www.example.com/articles/1</link><description>Alcacer, Juan; Chung, Wilbur
Given the importance of proximity for knowledge spillovers, we examine firms' location choices expecting differences in firms' strategies. Firms will locate to maximize their net spillovers as a function of locations' knowledge activity, their own capabilities, and competitors' anticipated actions. Using new entrants into the United States from 1985 to 1994, we find that firms favor locations with academic innovative activity. Other results highlight differences in firms' location. strategies suggesting that firms consider not only gains from inward knowledge spillovers but also the possible cost of outward spillovers. While less technologically advanced firms favor locations with high levels of industrial innovative activity, technologically advanced firms choose only locations with high levels of academic activity and avoid locations with industrial activity to distance themselves from competitors.</description><author>Alcacer, Juan; Chung, Wilbur</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic spot trading in supply chains</title><link>http://www.example.com/articles/1</link><description>Mendelson, Haim; Tunca, Tunay I.
In a variety of industries ranging from agriculture to electronics and oil, procurement takes place through a combination of bilateral fixed-price contracts and open market trading among supply chain participants, which allows them to improve supply chain performance by utilizing new demand and cost information. The strategic behavior of the participants in these markets interacts with the way fixed-price contracts are formulated and significantly affects supply chain efficiency. In this paper, we develop a strategic model that allows endogenous price formation in an industrial spot market where supply chain participants have private information. Utilizing the model, we analyze the equilibrium of a dynamic game between a single supplier and multiple manufacturers who first contract with the supplier at a fixed price and then trade on a spot market. We study how such trading affects supply chain performance and show that it does not eliminate fixed-price contracting even though the fixed price is determined under inferior information. We find that it reduces prices, increases the quantities produced, and improves supply chain profits and consumer surplus. However, depending on the information structure of the supply chain, spot trading may make either the supplier or the manufacturers worse off. Our results show how the informational regime affects the profitability of supply chain participants and the allocation of quantities between the procurement venues. We show that beyond a threshold level, the effect of increasing supply uncertainty, or decreasing either the demand uncertainty or the information asymmetry among the manufacturers, is to increase the percentage procured on the spot market as well as the overall quantity procured and sold, and to decrease prices. As the number of manufacturers increases, procurement shifts from fixed-price contracting to spot trading and in the limit, the supply chain is both fully coordinated and informationally efficient. We also show that in many cases, the supplier may gain strategic advantage by sharing some of her cost information with the manufacturers.</description><author>Mendelson, Haim; Tunca, Tunay I.</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Intertemporal pricing with strategic customer behavior</title><link>http://www.example.com/articles/1</link><description>Su, Xuanming
his paper develops a model of dynamic pricing with endogenous intertemporal demand. In the model, there is a monopolist who sells a finite inventory over a finite time horizon. The seller adjusts prices dynamically to maximize revenue. Customers arrive continually over the duration of the selling season. At each point in time, customers may purchase the product at current prices, remain in the market at a cost to purchase later, or exit, and they wish to maximize individual utility. The customer population is heterogeneous along two dimensions: they may have different valuations for the product and different degrees of patience (waiting costs). We demonstrate that heterogeneity in both valuation and patience is important because they jointly determine the structure of optimal pricing policies. In particular, when high-value customers are proportionately less patient, markdown pricing policies are effective because the high-value customers would buy early at high prices while the low-value customers are willing to wait (i.e., they are not lost). On the other hand, when the high-value customers are more patient than the low-value customers, prices should increase over time to discourage inefficient waiting. Contrary to intuition, we find that strategic waiting by customers may sometimes benefit the seller: when low-value customers wait, they compete for availability with high-value customers and thus increase their willingness to pay. Our results also shed light on how the composition of the customer population affects optimal revenue, consumer surplus, and social welfare. Finally, we consider the long-run problem of selecting the optimal initial stocking quantity.</description><author>Su, Xuanming</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and lead time decisions in decentralized supply chains</title><link>http://www.example.com/articles/1</link><description>Liu, Liming; Parlar, Mahmut; Zhu, Stuart X.
his paper studies a decentralized supply chain consisting of a supplier and a retailer facing price- and leadtime-sensitive demands. A Stackelberg game is constructed to analyze the price and lead time decisions by the supplier as the leader and the retailer as the follower. The equilibrium strategies of the two players are obtained. Using the performance of the corresponding centralized system as a benchmark, we show that decentralized decisions in general are inefficient and lead to inferior performance due to the double marginalization effect. However, further analysis shows that the decision inefficiency is strongly influenced by market and operational factors, and if the operational factors are dominating, it may not be significant. This shows that before pursuing a coordination strategy with retailers, a supplier should first improve his or her own internal operations.</description><author>Liu, Liming; Parlar, Mahmut; Zhu, Stuart X.</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The strategic perils of delayed differentiation</title><link>http://www.example.com/articles/1</link><description>Anand, Krishnan S.; Girotra, Karan
he value of delayed differentiation (also known as postponement) for a monopolist has been extensively studied in the operations literature. We analyze the case of (imperfectly) competitive markets with demand uncertainty, wherein the choice of supply chain configuration (i.e., early or delayed differentiation) is endogenous to the competing firms. We characterize firms' choices in equilibrium and analyze the effects of these choices on quantities sold, profits, consumer surplus, and welfare. We demonstrate that purely strategic considerations not previously identified in the literature play a pivotal role in determining the value of delayed differentiation. In the face of either entry threats or competition, these strategic effects can significantly diminish the value of delayed differentiation. In fact, under plausible conditions, these effects dominate the traditional risk-pooling benefits associated with delayed differentiation, in which case early differentiation is the dominant strategy for firms, even under cost parity with delayed differentiation. We extend the main model to study the effects of alternate market structures, asymmetric markets, and inventory holdback. Our results-in particular that for a broad range of parameter values, early differentiation is a dominant strategy even under cost parity with delayed differentiation-are robust to these relaxations.</description><author>Anand, Krishnan S.; Girotra, Karan</author><pubDate>Tue, 01 May 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance bounds for flexible systems requiring setups</title><link>http://www.example.com/articles/1</link><description>Singh, Mahender P.; Srinivasan, Mandyam M.
Many organizations use product variety as one possible strategy for increasing their competitiveness. They have installed flexible manufacturing systems because these systems offer a powerful means for accommodating production and assembly of a variety of products. However, increased product variety comes at a cost. For instance, if the resource requires to be set up each time it switches to operate on a new product, the resulting delays and costs could negate the intended benefits of increased product variety. Analyzing these flexible resources for optimal design and operation is therefore very important. To address such issues, we model a flexible resource, serving multiple products, using a queueing model-more precisely, a polling model. In this model, a single server attends to multiple service centers (queues) at which requests arrive and queue up for service, performing a setup at a polled queue only if that queue is nonempty. This is the state dependent (SD) polling model. Exact analysis of the SD polling model is inherently very complex. This paper presents a very efficient procedure to compute a hierarchy of successively improving bounds on the values of performance measures obtained from the SD polling model with the exact solution as its limit. This procedure can be applied to quickly estimate performance measures for large SD polling models previously deemed analytically intractable.</description><author>Singh, Mahender P.; Srinivasan, Mandyam M.</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A contract and balancing mechanism for sharing capacity in a communication network (vol 53, pg 1029, 2007)</title><link>http://www.example.com/articles/1</link><description>Anderson, Edward; Kelly, Frank; Steinberg, Richard; Waters, Robert
In a recent paper, the first three authors proposed a method for determining how much to charge users of a communication network when they share bandwidth, and studied the existence and form of Nash equilibria for players' choices of capacity. However, the proof of one of the propositions in that paper contained a flaw. In this note, we prove that the original proposition is true under an additional condition, and provide two examples to show that this condition is necessary.</description><author>Anderson, Edward; Kelly, Frank; Steinberg, Richard; Waters, Robert</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Gain-loss separability and coalescing in risky decision making</title><link>http://www.example.com/articles/1</link><description>Birnbaum, Michael H.; Bahra, Jeffrey P.
This experiment tested two behavioral properties of risky decision making-gain-loss separability (GLS) and coalescing. Cumulative prospect theory (CPT) implies both properties, but the transfer of attention exchange (TAX) model violates both. Original prospect theory satisfies GLS but may or may not satisfy coalescing, depending on whether editing rules are assumed. A configural form of CPT proposed by Wu and Markle [Wu, G., A. B. Markle. 2004. An empirical test of gain-loss separability in prospect theory. Working Paper 06-25-04, Graduate School of Business University of Chicago] violates GLS, but satisfies coalescing. New tests were designed and conducted to test these theories against specific predictions of a TAX model. This model used parameters estimated from previous data, together with simple new assumptions to extend TAX to gambles with negative and mixed consequences. Contrary to all three forms of prospect theory, systematic violations of both coalescing and GLS were observed. Violations of GLS were confirmed by analyses of individual data patterns by means of an error model in which each choice can have a different rate of error. Without estimating any parameters from the new data, the TAX model predicted the majority choices in the new data fairly well, correctly predicting when modal choices would violate GLS, when they would satisfy it, and when indifference would be observed.</description><author>Birnbaum, Michael H.; Bahra, Jeffrey P.</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance of portfolios optimized with estimation error</title><link>http://www.example.com/articles/1</link><description>Siegel, Andrew F.; Woodgate, Arterniza
We explain the poor out-of-sample performance of mean-variance optimized portfolios, developing theoretical bias adjustments for estimation risk by asymptotically expanding future returns of portfolios formed with estimated weights. We provide closed-form non-Bayesian adjustments of classical estimates of portfolio mean and standard deviation. The adjustments significantly reduce bias in international equity portfolios, increase economic gains, and are robust to sample size and to nonnormality. Dominant terms grow linearly with the number of assets and decline inversely with the number of past time periods. Under suitable conditions, Sharpe-ratio maximizing tangency portfolios become more diversified. Using these approximation methods it may be possible to assess, before investing, the effect of statistical estimation error on portfolio performance.</description><author>Siegel, Andrew F.; Woodgate, Arterniza</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Service performance analysis and improvement for a ticket queue with balking customers</title><link>http://www.example.com/articles/1</link><description>Xu, Susan H.; Gao, Long; Ou, Jihong
Queueing systems managed by ticket technology are widely used in service industries as well as government offices. Upon arriving at a ticket queue, each customer is issued a numbered ticket. The number currently being served is displayed. An arriving customer balks if the difference between his ticket number and the displayed number exceeds his patience level. We propose a Markov chain model of a ticket queue and develop effective evaluation tools. These tools can help management quantify the service level and identify the performance gap between the ticket queue and the conventional physical queue, in which a waiting line is formed. We gain insights about the ways customer service is affected by information loss in the ticket queue. In particular, we show that ticket and physical queues have significantly different balking probabilities when customer patience is low and the system traffic is heavy. We also propose an improvement to the ticket queue that provides each customer with his expected waiting time conditioned on his observed number difference, which is shown to raise the performance of the ticket queue to that of the physical queue.</description><author>Xu, Susan H.; Gao, Long; Ou, Jihong</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis and comparison of queues with different levels of delay information</title><link>http://www.example.com/articles/1</link><description>Guo, Pengfei; Zipkin, Paul
Information about delays can enhance service quality in many industries. Delay information can take many forms, with different degrees of precision. Different levels of information have different effects on customers and therefore on the overall system. To explore these effects, we consider a queue with balking under three levels of delay information: no information, partial information (the system occupancy), and full information (the exact waiting time). We assume Poisson arrivals, independent exponential service times, and a single server. Customers decide whether to stay or balk based on their expected waiting costs, conditional on the information provided. We show how to compute the key performance measures in the three systems, obtaining closed-form solutions for special cases. We then compare the three systems. We identify some important cases where more accurate delay information improves performance. In other cases, however, information can actually hurt the provider or the customers.</description><author>Guo, Pengfei; Zipkin, Paul</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Electronic B2B marketplaces with different ownership structures</title><link>http://www.example.com/articles/1</link><description>Yoo, Byungjoon; Choudhary, Vidyanand; Mukhopadhyay, Tridas
This paper analyzes electronic marketplaces with different ownership structures: biased marketplaces and neutral marketplaces. Biased marketplaces can be either buyer-owned or supplier-owned, whereas neutral marketplaces are owned by independent third parties. We develop a single-period model, with fulfilled expectations equilibrium. The buyers experience positive network effects that are a function of the number of suppliers and the suppliers receive similar positive network effects depending on the number of buyers. We develop a general model with atomistic buyers and suppliers. We find that biased marketplaces set prices to induce greater participation (demand) from both buyers and suppliers compared to a neutral marketplace. This counterintuitive result can be understood in the context of the positive cross-network effects experienced by buyers and suppliers and the added benefit to the owner of a biased marketplace from participating in the marketplace. Biased marketplaces also provide greater social welfare compared to neutral marketplaces.</description><author>Yoo, Byungjoon; Choudhary, Vidyanand; Mukhopadhyay, Tridas</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Category management and coordination in retail assortment planning in the presence of basket shopping consumers</title><link>http://www.example.com/articles/1</link><description>Cachon, Gerard P.; Kok, A. Gurhan
This paper studies the assortment planning problem with multiple merchandise categories and basket shopping consumers (i.e., consumers who desire to purchase from multiple categories). We present a duopoly model in which retailers choose prices and variety level in each category and consumers make their store choice between retail stores and a no-purchase alternative based on their utilities from each category. The common practice of category management (CM) is an example of a decentralized regime for controlling assortment because each category manager is responsible for maximizing his or her assigned category's profit. Alternatively, a retailer can make category decisions across the store with a centralized regime. We show that CM never finds the optimal solution and provides both less variety and higher prices than optimal. In a numerical study, we demonstrate that profit loss due to CM can be significant. Finally, we propose a decentralized regime that uses basket profits, a new metric, rather than accounting profits. Basket profits are easily evaluated using point-of-sale data, and the proposed method produces near-optimal solutions.</description><author>Cachon, Gerard P.; Kok, A. Gurhan</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>When do employees become entrepreneurs?</title><link>http://www.example.com/articles/1</link><description>Hellmann, Thomas
This paper examines an economic theory of when employees become entrepreneurs. It jointly addresses the two fundamental questions of when employees generate innovations, and whether these innovations are developed as internal ventures or outside the firm. The model shows that if generating innovations distracts employees from their assigned tasks, firms may discourage innovation. Firms may reject profitable opportunities that fall outside of their core activities. If employees own the intellectual property (IP), they may leave to do a start-up. The allocation of IP rights also affects the generation of innovation. The external entrepreneurial environment is a complement to firm-internal innovation. If the external environment is particularly good, firms may embrace employee innovation and take advantage of it through spin-offs.</description><author>Hellmann, Thomas</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How much did the liberty shipbuilders forget?</title><link>http://www.example.com/articles/1</link><description>Thompson, Peter
This paper produces new estimates of the rate of organizational forgetting in the well-known case study of U.S. wartime ship production. Estimates obtained using data constructed from primary sources at the National Archives yield rates of forgetting that are much smaller than previously reported, and may well be zero. The richness of the data makes it possible to control for variations in the product mix, to explore alternative formulations for the learning curve, and to investigate the relationship between organizational forgetting and labor turnover.</description><author>Thompson, Peter</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implementing new practices: An empirical study of organizational learning in hospital intensive care units</title><link>http://www.example.com/articles/1</link><description>Tucker, Anita L.; Nembhard, Ingrid M.; Edmondson, Amy C.
This paper contributes to research on organizational learning by investigating specific learning activities undertaken by improvement project teams in hospital intensive care units and proposing an integrative model to explain implementation success. Organizational learning is important in this context because medical knowledge changes constantly and hospital care units must learn new practices if they are to provide high-quality care. To develop a model of factors affecting improvement project teams driving essential organizational learning in health care, we draw from three streams of related research-best-practice transfer (BPT), team learning (TL), and process change (PC). To test the model's hypotheses, we collected data from 23 neonatal intensive care units seeking to implement new or improved practices. We first analyzed the frequency of specific learning activities reported by improvement project participants and discovered two distinct factors: learn-what (activities that identify current best practices) and learn-how (activities that operationalize practices in a given setting). Next, ordinary least squares (OLS) regression analyses supported three of our four hypotheses. Specifically, a high level of supporting evidence for a unit's portfolio of improvement projects was associated with implementation success. Learn-how was positively associated with implementation success, but learn-what was not. Psychological safety was associated with learn-how, which was found to mediate between psychological safety and implementation success.</description><author>Tucker, Anita L.; Nembhard, Ingrid M.; Edmondson, Amy C.</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>From story line to box office: A new approach for green-lighting movie scripts</title><link>http://www.example.com/articles/1</link><description>Eliashberg, Jehoshua; Hui, Sam K.; Zhang, Z. John
Movie studios often have to choose among thousands of scripts to decide which ones to turn into movies. Despite the huge amount of money at stake, this process-known as green-lighting in the movie industry is largely a guesswork based on experts' experience and intuitions. In this paper, we propose a new approach to help studios evaluate scripts that will then lead to more profitable green-lighting decisions. Our approach combines screenwriting domain knowledge, natural-language processing techniques, and statistical learning methods to forecast a movie's return on investment (ROI) based only on textual information available in movie scripts. We test our model in a holdout decision task to show that our model is able to significantly improve a studio's gross ROI.</description><author>Eliashberg, Jehoshua; Hui, Sam K.; Zhang, Z. John</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An analysis of short-term responses to threats of terrorism</title><link>http://www.example.com/articles/1</link><description>Pinker, Edieal J.
Two important defensive mechanisms available to governments combating terrorism are warnings and the deployment of physical resources. Warnings are relatively inexpensive to issue but their effectiveness suffers from false alarms. Physical deployments of trained security personnel can directly thwart attacks but are expensive and need to be targeted to specific locations. In this paper, we model the joint optimization of defenses against terrorist attacks based on warnings and physical deployments when there is uncertainty in the timing and location of attacks. We model both private warnings issued to security forces and public warnings broadcast to the general public. By structuring the trade-offs faced by decision makers in a formal way, we try to shed light on an important public policy problem. We show that the interaction between the use of warnings and physical defenses is complex and significant. For public warnings, we also model the possible response of terrorists and show how these responses influence the effectiveness of such warnings.</description><author>Pinker, Edieal J.</author><pubDate>Fri, 01 Jun 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The role of pre-entry experience, entry timing, and product technology strategies in explaining firm survival</title><link>http://www.example.com/articles/1</link><description>Bayus, Barry L.; Agarwal, Rajshree
Studying the U.S. personal computer industry from its inception in 1974 through 1994, we address the following questions. What product technology strategies increase the survival chances of entrants into new, technologically dynamic industries? Does the effectiveness of these strategies differ by pre-entry experience? Does the effectiveness of these strategies differ by when firms enter a new industry? Consistent with the published literature, we find that diversifying entrants have an initial survival advantage over entrepreneurial startups. But, we find the reverse for later entrants: startups that enter later in the industry have a survival advantage over the later entering diversifying entrants. We explain this finding in terms of the firms' product technology strategies (i.e., offering products based on the technology standard and products incorporating the latest technology), pre-entry experience, and entry timing. Our findings highlight that it is crucial to study what firms do after they enter a new industry to more completely understand their ultimate performance.</description><author>Bayus, Barry L.; Agarwal, Rajshree</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The machine maintenance and sale age model of Kamien and Schwartz revisited</title><link>http://www.example.com/articles/1</link><description>Bensoussan, Alain; Sethi, Suresh P.
In this paper, we revisit and clarify the celebrated machine maintenance and sale age model of Kamien and Schwartz (KS) involving a machine subject to failure. KS formulate and solve the problem as a deterministic optimal control problem with the probability of the machine failure as the state variable. Thus, they obtain deterministic optimal maintenance and sale date. We study two underlying stochastic models with known and random machine modes, and clarify the relationship between the resulting value functions to that of KS. In particular, our maintenance and sale date decisions, when the machine is in operation, are precisely the ones obtained from the deterministic solution of KS. We explain why that is so. Moreover, we provide a sufficient condition for an optimal maintenance and sale date policy that is missing in KS. We describe many applications of the KS model in areas other than that of machine maintenance. We conclude the paper with extensions of the KS problem that are stochastic control problems not easily solvable or not at all solvable as deterministic problems.</description><author>Bensoussan, Alain; Sethi, Suresh P.</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A framework for reconciling attribute values from multiple data sources</title><link>http://www.example.com/articles/1</link><description>Jiang, Zhengrui; Sarkar, Sumit; De, Prabuddha; Dey, Debabrata
Because of the heterogeneous nature of different data sources, data integration is often one of the most challenging tasks in managing modern information systems. While the existing literature has focused on problems such as schema integration and entity identification, it has largely overlooked a basic question: When an attribute value for a real-world entity is recorded differently in different databases, how should the ''best'' value be chosen from the set of possible values? This paper provides an answer to this question. We first show how a probability distribution over a set of possible values can be derived. We then demonstrate how these probabilities can be used to solve a given decision problem by minimizing the total cost of type I, type II, and misrepresentation errors. Finally, we propose a framework for integrating multiple data sources when a single ''best'' value has to be chosen and stored for every attribute of an entity.</description><author>Jiang, Zhengrui; Sarkar, Sumit; De, Prabuddha; Dey, Debabrata</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Consumers ' price sensitivities across complementary categories</title><link>http://www.example.com/articles/1</link><description>Duvvuri, Sri Devi; Ansari, Asim; Gupta, Sunil
In this paper, we examine the pattern of correlation among consumer price sensitivities for customer purchase incidence decisions across complementary product categories. We use a hierarchical Bayesian multivariate probit model to uncover this pattern. We estimated this model using purchase incidence data for six categories involving three pairs of complementary products. Our results show a new and interesting pattern of correlation among price parameters of complementary products. For example, we find that the correlation of own-price sensitivities of complementary products is negative. These results are consistent across the three complementary pairs of products. We also investigate the reason for this counterintuitive result. Finally, we present some managerial implications of our model. We show how our model can be used for cross-category targeting decisions by retailers. We find that compared to nontargeted discounting, the average profitability gain from customized discounting across the three category pairs is only 1.29% when complementarity is ignored, but this gain improves to 8.26% when full complementarity is taken into account. We also investigate whether ignoring the complex pattern of correlation has implications for managerial actions regarding targeting and optimal discounting. We find that retailers can make misleading inferences about the impact of targeted discounts when they ignore cross-category effects in modeling.</description><author>Duvvuri, Sri Devi; Ansari, Asim; Gupta, Sunil</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selecting a selection procedure</title><link>http://www.example.com/articles/1</link><description>Branke, Juergen; Chick, Stephen E.; Schmidt, Christian
Selection procedures are used in a variety of applications to select the best of a finite set of alternatives. ''Best'' is defined with respect to the largest mean, but the mean is inferred with statistical sampling, as in simulation optimization. There are a wide variety of procedures, which begs the question of which selection procedure to select. The main contribution of this paper is to identify, through extensive experimentation, the most effective selection procedures when samples are independent and normally distributed. We also (a) summarize the main structural approaches to deriving selection procedures, (b) formalize new sampling allocations and stopping rules, (c) identify strengths and weaknesses of the procedures, (d) identify some theoretical links between them, and (e) present an innovative empirical test bed with the most extensive numerical comparison of selection procedures to date. The most efficient and easiest to control procedures allocate samples with a Bayesian model for uncertainty about the means and use new adaptive stopping rules proposed here.</description><author>Branke, Juergen; Chick, Stephen E.; Schmidt, Christian</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal feature advertising design under competitive clutter</title><link>http://www.example.com/articles/1</link><description>Pieters, Rik; Wedel, Michel; Zhang, Jie
This study investigates consumers' attention to retail feature ads and proposes a method to optimize the design of the ads. Utilizing a large dataset of consumers' attention to over 1,100 individual feature ads collected with eye-tracking technology, we analyze the effects of the five key design elements of feature ads-brand, text, pictorial, price, and promotion-on consumers' attention to them. Attention is measured in terms of selection and gaze duration. We focus on the effects of the surface sizes of the design elements. A key feature of our model is that it takes into account the impact of visual clutter in the ad display page. To capture the clutter effects, we propose two new entropy-based measures that characterize the salience of feature ads in their competitive environment based on Attention Engagement Theory. In a Bayesian framework, we simultaneously estimate the parameters of the model and optimize the design of feature ads in terms of surface sizes of the five design elements. Our optimization results and comparisons with alternative design approaches indicate that significant improvements in attention to feature advertising can be achieved without increase in costs, and that the resultant optimal feature ad designs create win-win opportunities for manufacturers and retailers.</description><author>Pieters, Rik; Wedel, Michel; Zhang, Jie</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Convertible bond underpricing: Renegotiable covenants, seasoning, and convergence</title><link>http://www.example.com/articles/1</link><description>Chan, Alex W. H.; Chen, Nai-fu
We investigate the long-standing puzzle on the underpricings of convertible bonds. We hypothesize that the observed underpricing is induced by the possibility that a convertible bond might renegotiate on some of its covenants, e.g., an imbedded put option in financial difficulties. Consistent with our hypothesis, we find that the initial underpricing Is larger for lower rated bonds. The underpricing worsens if the issuer experiences subsequent financial difficulties. However, conditional on no rating downgrades, our main empirical result shows that convertible bond prices do converge to their theoretical prices within two years. This seasoning period is shorter for higher rated convertible bonds.</description><author>Chan, Alex W. H.; Chen, Nai-fu</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Final-offer arbitration and risk aversion in bargaining</title><link>http://www.example.com/articles/1</link><description>Hanany, Eran; Kilgour, D. Marc; Gerchak, Yigal
Negotiations are often conducted under the stipulation that an impasse is to be resolved using final-offer arbitration (FOA). In fact, FOA frequently is not needed; in Major League Baseball, for instance, more than 80% of the salary negotiations that could go to arbitration instead reach a bargained agreement. We show that the risk aversion of at least one side explains this phenomenon. We then model pay negotiation in baseball by applying a bargaining solution with a variable disagreement outcome representing FOA, studying the existence of pure Nash equilibrium initial offers and their effects on the player's eventual pay, and considering the Nash solution as a special case.</description><author>Hanany, Eran; Kilgour, D. Marc; Gerchak, Yigal</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of statistical information on risk and ambiguity attitudes, and on rational insurance decisions</title><link>http://www.example.com/articles/1</link><description>Wakker, Peter P.; Timmermans, Danielle R. M.; Machielse, Irma
This paper presents a field study into the effects of statistical information concerning risks on willingness to take insurance, with special attention being paid to the usefulness of these effects for the clients (the insured). Unlike many academic studies, we were able to use in-depth individual interviews of a large representative sample from the general public (N = 476). The statistical information that had the most interesting effects, "individual own past-cost information," unfortunately enhanced adverse selection, which we could directly verify because the real health costs of the clients were known. For a prescriptive evaluation this drawback must be weighted against some advantages: a desirable interaction with risk attitude, increased customer satisfaction, and increased cost awareness. Descriptively, ambiguity seeking was found rather than ambiguity aversion, and no risk aversion was found for loss outcomes. Both findings, obtained in a natural decision context, deviate from traditional views in risk theory but are in line with prospect theory. We confirmed prospect theory's reflection at the level of group averages but falsified it at the individual level.</description><author>Wakker, Peter P.; Timmermans, Danielle R. M.; Machielse, Irma</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Simulation of coherent risk measures based on generalized scenarios</title><link>http://www.example.com/articles/1</link><description>Lesnevski, Vadim; Nelson, Barry L.; Staum, Jeremy
In financial risk management, coherent risk measures have been proposed as a way to avoid undesirable properties of measures such as value at risk that discourage diversification and do not account for the magnitude of the largest, and therefore most serious, losses. A coherent risk measure equals the maximum expected loss under several different probability measures, and these measures are analogous to "populations" or "systems" in the ranking-and-selection literature. However, unlike in ranking and selection, here it is the value of the maximum expectation under any of the probability measures, and not the identity of the probability measure that attains it, that is of interest. We propose procedures to form fixed-width, simulation-based confidence intervals for the maximum of several expectations, explore their correctness and computational efficiency, and illustrate them on risk-management problems. The availability of efficient algorithms for computing coherent risk measures will encourage their use for improved risk management.</description><author>Lesnevski, Vadim; Nelson, Barry L.; Staum, Jeremy</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Probability elicitation, scoring rules, and competition among forecasters</title><link>http://www.example.com/articles/1</link><description>Lichtendahl, Kenneth C.; Winkler, Robert L.
Probability forecasters who are rewarded via a proper scoring rule may care not only about the score, but also about their performance relative to other forecasters. We model this type of preference and show that a competitive forecaster who wants to do better than another forecaster typically should report more extreme probabilities, exaggerating toward zero or one. We consider a competitive forecaster's best response to truthful reporting and also investigate equilibrium reporting functions in the case where another forecaster also cares about relative performance. We show how a decision maker can revise probabilities of an event after receiving reported probabilities from competitive forecasters and note that the strategy of exaggerating probabilities can make well-calibrated forecasters (and a decision maker who takes their reported probabilities at face value) appear to be overconfident. However, a decision maker who adjusts appropriately for the misrepresentation of probabilities by one or more forecasters can still be well calibrated. Finally, to try to overcome the forecasters' competitive instincts and induce cooperative behavior, we develop the notion of joint scoring rules based on business sharing and show that these scoring rules are strictly proper.</description><author>Lichtendahl, Kenneth C.; Winkler, Robert L.</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coherence and consistency of investors' probability judgments</title><link>http://www.example.com/articles/1</link><description>Budescu, David V.; Du, Ning
This study investigates the quality of direct probability judgments and quantile estimates with a focus on calibration and consistency. The two response modes use different measures of miscalibration, so it is difficult to directly compare their relative (in)accuracy. We employed a more refined within-subject design in which decision makers (DMs) used both response modes to make judgments about a random sample of stocks accompanied by identical information to facilitate comparison between the two judgment methods. DMs judged the probabilities that the stocks will reach a certain threshold, provided lower and upper bounds of these forecasts, and estimated median, 50%, 70%, and 90% confidence intervals of their future prices. We found that the judgments were internally consistent and coherent, but in most cases they were slightly miscalibrated. We used several new methods of analysis that allow for more precise and reliable comparison between the two response modes. We inferred point probability estimates for the target events from the confidence intervals and analyzed them by the same methods applied to binary judgments. Interestingly, when we quantified miscalibration in identical fashion for both methods we did not find evidence of differential levels of miscalibration for the probability judgments and the confidence intervals. We discuss the theoretical and practical implications of these results.</description><author>Budescu, David V.; Du, Ning</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Template use and the effectiveness of knowledge transfer</title><link>http://www.example.com/articles/1</link><description>Jensen, Robert J.; Szulanski, Gabriel
This paper is a direct empirical examination of the fundamental claim that use of templates enhances the effectiveness of knowledge transfer. We explore the effect of template use through an eight-year, in-depth field investigation of Rank Xerox (now Xerox Europe). The field investigation covers three sequential transfer efforts in 15 western European countries. The investigation reveals a naturally occurring, repeated-treatment quasi experiment that allows us to test the hypothesis that the use of templates enhances the effectiveness of knowledge transfer. "Observations" in this experiment measure the extent of adoption and performance of the practice at the recipient units. The "treatment" is the use of a template during the transfer.</description><author>Jensen, Robert J.; Szulanski, Gabriel</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Choosing among living-donor and cadaveric livers</title><link>http://www.example.com/articles/1</link><description>Alagoz, Oguzhan; Maillart, Lisa M.; Schaefer, Andrew J.; Roberts, Mark S.
The only therapy for a patient with end-stage liver disease (ESLD) is liver transplantation, which is performed by using either a cadaveric liver from a deceased donor or a portion of a living-donor's liver. This study addresses the following decision problem for an ESLD patient with an available living donor. Should she have a transplantation now or wait? If she decides to have the transplantation now, should she use her living-donor liver or a cadaveric liver for transplantation? We formulate this problem as a discrete-time, infinite-horizon Markov decision process model and solve it using clinical data. Because living donors are typically related to the recipient, we incorporate a disutility associated with using the living-donor liver as opposed to using a cadaveric liver. We perform a structural analysis of the model, including a set of intuitive conditions that ensure the existence of structured policies such as an at-most-three-region (AM3R) optimal policy. Our computational experiments confirm that the optimal policy is typically of AM3R type.</description><author>Alagoz, Oguzhan; Maillart, Lisa M.; Schaefer, Andrew J.; Roberts, Mark S.</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The debate on influencing doctors' decisions: Are drug characteristics the missing link?</title><link>http://www.example.com/articles/1</link><description>Venkataraman, Sriram; Stremersch, Stefan
Decision making by physicians on patients' treatment has come under increased public scrutiny. In fact, there is a fair amount of debate on the effects of marketing actions of pharmaceutical firms toward physicians and their impact on physician prescription behavior. While some scholars find a strong and positive influence of marketing actions, some find only moderate effects, and others even find negative effects. Debate is also mounting on the role of other influencers (such as patient requests) in physician decision making, both on prescriptions and sample dispensing. The authors argue that one factor that may tip the balance in this debate is the role of drug characteristics, such as a drug's effectiveness and a drug's side effects. Using a unique data set, they show that marketing efforts-operationalized as detailing and symposium meetings of firms to physicians-and patient requests do affect physician decision making difterentially across brands. Moreover, they find that the responsiveness of physicians' decision making to marketing efforts and patient requests depends upon the drug's effectiveness and side effects. This paper presents clear guidelines for public policy and managerial practice and envisions that the study of the role of drug characteristics, such as effectiveness and side effects, may lead to valuable insights in this surging public debate.</description><author>Venkataraman, Sriram; Stremersch, Stefan</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managerial turnover and strategic change</title><link>http://www.example.com/articles/1</link><description>Sliwka, Dirk
The connection between strategic change and managerial turnover is studied within a model where managers decide on a firm's strategy Managers as well as firm owners care for the long-term success of a company, but managers are also interested in their own reputation. Due to reputational concerns, managers are reluctant to alter strategic decisions they themselves made in the past even when internal accounting information indicates that they should do so. It is shown that it may well be optimal in some cases to dismiss managers of higher ability while someone less talented may be kept in office when strategic change has to be enforced.</description><author>Sliwka, Dirk</author><pubDate>Thu, 01 Nov 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conditions that shape the learning curve: Factors that increase the ability and opportunity to learn</title><link>http://www.example.com/articles/1</link><description>Wiersma, Eelke
Prior studies examining factors that influence the learning curve mainly focus on settings in which firms adopt new products or technologies or open new plants or assembly lines. Less is known, however, about how more mature firms learn, when they are further down the learning curve. To gain insight into factors that enhance learning in this situation, I examine factors that increase both the ability and the opportunity to learn. I hypothesize that the ability to learn is enhanced by the presence of a moderate amount of temporary employees in the workforce and by providing employees with related variation in tasks, measured by product heterogeneity. In addition, I hypothesize that opportunities for learning are created when there is some slack in resources and when there are no problems in other important performance dimensions that consume employee attention. These hypotheses are examined using data of the Royal Dutch Mail, which has 27 geographically dispersed regions. Although these 27 regions are homogeneous with respect to their tasks, internal organization, type of products delivered, and technology used, their learning rates differ considerably. In the sample of 972 observations used for this analysis, I find that this variation in learning rates is explained by the percentage of temporary employees used, the level of excess capacity, the degree of product heterogeneity, and the degree to which regions face problems in other important performance dimensions. These findings provide insight into strategies that help managers in designing work processes to maintain a positive learning curve.</description><author>Wiersma, Eelke</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implications of renegotiation for optimal contract flexibility and investment</title><link>http://www.example.com/articles/1</link><description>Plambeck, Erica L.; Taylor, Terry A.
In a stylized model of biopharmaceutical. contract manufacturing, this paper shows how the potential for renegotiation influences the optimal structure of supply contracts, investments in innovation and capacity; the way scarce capacity is allocated, and firms' resulting profits. Two buyers contract for capacity with a common manufacturer. Then, the buyers invest in innovation (product development and marketing) and the manufacturer builds capacity. Finally, the firms may renegotiate to allow a buyer facing poor market conditions to purchase less than the contractual commitment and a buyer facing favorable conditions to purchase more. We show that renegotiation can greatly increase the firms' investments and profits, provided that the contracts are designed correctly. Failing to anticipate renegotiation leads to contracts that allow too much flexibility in the buyer's order quantity, and perform poorly relative to contracts designed to anticipate renegotiation. We provide clear conditions under which quantity flexibility contracts with renegotiation coordinate the system. Where quantity flexibility contracts fail, employing tradable options improves performance.</description><author>Plambeck, Erica L.; Taylor, Terry A.</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implications of breach remedy and renegotiation design for innovation and capacity</title><link>http://www.example.com/articles/1</link><description>Plambeck, Erica L.; Taylor, Terry A.
A manufacturer writes supply contracts with N buyers. Then, the buyers invest in innovation, and the manufacturer builds capacity. Finally, demand is realized, and the firms renegotiate the supply contracts to achieve an efficient allocation of capacity among the buyers. The court remedy for breach of contract (specific performance versus expectation damages) affects how the firms share the gain from renegotiation, and hence how the firms make investments ex ante. The firms may also engage in renegotiation design, inserting simple clauses into the supply contract to shape the outcome of renegotiation. For example, when a buyer grants a financial ''hostage'' to the manufacturer or is charged a per them penalty for delay in bargaining, the manufacturer captures the gain from renegotiation. ''Tradable options,'' which grant buyers the right to trade capacity without intervention from the manufacturer, return the gain from renegotiation to the buyers. This paper proves that, under surprisingly general conditions, the firms can coordinate their investments with the simplest of supply contracts (fixed-quantity contracts). This may require renegotiation design, and certainly requires that the firms understand the breach remedy and set their contract parameters accordingly.</description><author>Plambeck, Erica L.; Taylor, Terry A.</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance contracting in after-sales service supply chains</title><link>http://www.example.com/articles/1</link><description>Kim, Sang-Hyun; Cohen, Morris A.; Netessine, Serguei
Performance-based contracting is reshaping service support supply chains in capital-intensive industries such as aerospace and defense. Known as ''power by the hour'' in the private sector and as ''performance-based logistics'' (PBL) in defense contracting, it aims to replace traditionally used fixed-price and cost-plus contracts to improve product availability and reduce the cost of ownership by tying a supplier's compensation to the output value of the product generated by the customer (buyer). To analyze implications of performance-based relationships, we introduce a multitask principal-agent model to support resource allocation and use it to analyze commonly observed contracts. In our model the customer (principal) faces a product availability requirement for the ''uptime'' of the end product. The customer then offers contracts contingent on availability to n suppliers (agents) of the key subsystems used in the product, who in turn exert cost reduction efforts and set spare-parts inventory investment levels. We show that the first-best solution can be achieved if channel members are risk neutral. When channel members are risk averse, we find that the second-best contract combines a fixed payment, a cost-sharing incentive, and a performance incentive. Furthermore, we study how these contracts evolve over the product deployment life cycle as uncertainty in support cost changes. Finally, we illustrate the application of our model to a problem based on aircraft maintenance data and show how the allocation of performance requirements and contractual terms change under various environmental assumptions.</description><author>Kim, Sang-Hyun; Cohen, Morris A.; Netessine, Serguei</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investigating the risk-return relationship of information technology investment: Firm-level empirical analysis</title><link>http://www.example.com/articles/1</link><description>Dewan, Sanjeev; Shi, Charles; Gurbaxani, Vijay
This paper develops empirical proxy measures of information technology (IT) risk and incorporates them into the usual empirical models for analyzing IT returns: production function and market value specifications. The results suggest that IT capital investments make a substantially larger contribution to overall firm risk than non-IT capital investments. Further, firms with higher IT risk have a higher marginal product of IT relative to firms with low IT risk. In the market value specification, the impact of IT risk is positive and significant, and inclusion of the IT risk term substantially reduces the coefficient on IT capital. We estimate that about 30% of the gross return on IT investment corresponds to the risk premium associated with IT risk. Taken together, our results show that IT risk provides part of the explanation for the unusually high valuations of IT capital investment in recent research.</description><author>Dewan, Sanjeev; Shi, Charles; Gurbaxani, Vijay</author><pubDate>Sat, 01 Dec 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of digital sharing technologies on music markets: A survival analysis of albums on ranking charts</title><link>http://www.example.com/articles/1</link><description>Bhattacharjee, Sudip; Gopal, Ram D.; Lertwachara, Kaveepan; Marsden, James R.; Telang, Rahul
Recent technological and market forces have profoundly impacted the music industry. Emphasizing threats from peer-to-peer (P2P) technologies, the industry continues to seek sanctions against individuals who offer a significant number of songs for others to copy. Combining data on the performance of music albums on the Billboard charts with file sharing data from a popular network, we assess the impact of recent developments related to the music industry on survival of music albums on the charts and evaluate the specific impact of P2P sharing on an album's survival on the charts. In the post-P2P era, we find significantly reduced chart survival except for those albums that debut high on the charts. In addition, superstars and female artists continue to exhibit enhanced survival. Finally, we observe a narrowing of the advantage held by major labels. The second phase of our study isolates the impact of file sharing on album survival. We find that, although sharing does not hurt the survival of top-ranked albums, it does have a negative impact on low-ranked albums. These results point to increased risk from rapid information sharing for all but the "cream of the crop."</description><author>Bhattacharjee, Sudip; Gopal, Ram D.; Lertwachara, Kaveepan; Marsden, James R.; Telang, Rahul</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic bid-shading and sequential auctioning with learning from past prices</title><link>http://www.example.com/articles/1</link><description>Zeithammer, Robert
This paper analyzes sequential auctioning of single units of an indivisible good to a fluctuating population composed of overlapping generations of unit-demand bidders. Two phenomena emergent in such a market are investigated: forward-looking bidding strategies, and closed-loop selling strategies that involve learning from past prices. The buyers shade their bids down, i.e., bid less than they would in a single isolated auction, whenever they expect the seller to sell another unit of the good in the near future. Unlike in exogenous sequences of auctions, the optimal bidding strategy thus depends on the seller's selling strategy. The converse dependence also occurs: the seller can learn about current demand from past realized prices, and sell only in periods with high-enough demand. Such learning depends on the extent of bid-shading because the seller needs to invert the bidding strategy to learn. In equilibrium, buyer bid-shading persists even when the seller does not sell in every period, but it is self-regulating in that it eventually vanishes when the existence of the market is threatened by low seller profits. In this sense, auction markets have a "self-preservation instinct." General properties of learning about current demand from past auction prices are also investigated and characterized.</description><author>Zeithammer, Robert</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Ranking contingent monitoring systems</title><link>http://www.example.com/articles/1</link><description>Fagart, Marie-Cecile; Sinclair-Desgagne, Bernard
This paper seeks to provide a ranking of information systems in a setting of contingent monitoring. Control strategies that make the acquisition of additional information conditional on observing certain outcomes largely elude the existing ranking criteria. We show that this happens because contingent monitoring involves more than the classical trade-off between risk sharing and incentives; it also requires a balancing of incentives and downside risk. We then develop a refinement of the most common information system orderings that conveys this feature. This allows us to reinterpret and generalize some of the literature's key results concerning, for instance, auditing policies with independent or with correlated signals and monitoring systems where the precision of an added signal is endogenous.</description><author>Fagart, Marie-Cecile; Sinclair-Desgagne, Bernard</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A single-product inventory model for multiple demand classes</title><link>http://www.example.com/articles/1</link><description>Arslan, Hasan; Graves, Stephen C.; Roemer, Thomas A.
We consider a single-product inventory system that serves multiple demand classes, which differ in their shortage costs or service-level requirements. We assume a critical-level control policy, and a backorder clearing mechanism in which we treat a backorder for a lower-priority class equivalent to a reserve-stock shortfall for the higher-priority class. We show the equivalence between this inventory system and a serial inventory system. Based on this equivalence, we develop a model for cost evaluation and optimization under the assumptions of Poisson demand, deterministic replenishment lead time, and a continuous-review (Q, R) policy with rationing. We propose a computationally efficient heuristic and develop a bound on its performance. We provide a numerical experiment to show the effectiveness of the heuristic and the value from a rationing policy. Finally, we describe how to extend the model to permit service times, and how to extend the model to a multi-echelon setting.</description><author>Arslan, Hasan; Graves, Stephen C.; Roemer, Thomas A.</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing a single-product assemble-to-order system with technology innovations</title><link>http://www.example.com/articles/1</link><description>Xu, Susan H.; Li, Zhaofin
We consider a multicomponent, single-product assemble-to-order (ATO) system that faces frequent, component-based technology innovations. For each component, there are two technologies with overlapping life cycles coexisting in the market. All cost parameters associated with each technology (procurement cost, salvage value, etc.) evolve dynamically. We investigate two technology-inventory coordination schemes, one is at the strategic level, where technology and inventory decisions are sequentially made using partial information, and another is at the operational level, where technology and inventory decisions are jointly made using full information. The performance gap between the two coordination schemes quantifies the value of incorporating dynamic inventory information in technology management. We develop effective solution techniques and approximation methods and characterize their policy structures. Our numerical study indicates that the strategic-level technology-inventory coordination is generally sufficient, but the operational-level coordination becomes necessary when demand variability is high and salvage loss is heavy. We also propose a hybrid technology-inventory coordination scheme, whereby the firm adopts a technology management plan using the strategic-level coordination scheme, but executes it dynamically by adapting to inventory information, using a heuristic proposed in this paper. Our numerical study suggests that the hybrid strategy can virtually achieve the performance of the optimal operational level coordination. Our analysis provides guidelines for the effective technology-adoption and inventory-control coordination strategies in the ATO system with rapid innovations.</description><author>Xu, Susan H.; Li, Zhaofin</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Valuing R&amp;D projects in a portfolio: Evidence from the pharmaceutical industry</title><link>http://www.example.com/articles/1</link><description>Girotra, Karan; Terwiesch, Christian; Ulrich, Karl T.
Understanding the value of a product development project is central to a firm's choice of project portfolio. The value of a project to a firm depends not only on its properties but also on the other projects being developed by the firm. This is due to interactions with the other projects that address the same consumer need and require the same development resources. In this study, we empirically investigate the structure and significance of these portfolio-level project interactions. Using a self-developed pharmaceutical industry data set, we conduct an event study around the failure of phase III clinical trials and their effect on the market valuation of the firm. The study exploits the natural experiment of a product development failure to give us a measure of the value of a drug development project to a firm. We then explain the variance in the value of projects based on interactions with other projects in the firm's portfolio. We find that the presence of other projects targeting the same market and a build-up of projects that require the same development resources reduce the value of a development project. In addition to providing evidence on the significance and structure of these portfolio-level project interactions, the empirical model estimated in this paper also provides a data-driven approach to valuing projects that may be relevant to licensing transactions.</description><author>Girotra, Karan; Terwiesch, Christian; Ulrich, Karl T.</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An integrated model for hybrid securities</title><link>http://www.example.com/articles/1</link><description>Das, Sanjiv R.; Sundaram, Rangarajan K.
We develop a model for pricing securities whose value may depend simultaneously on equity, interestrate, and default risks. The framework may also be used to extract probabilities of default (PD) functions from market data. Our approach is entirely based on observables such as equity prices and interest rates, rather than on unobservable processes such as firm value. The model stitches together in an arbitrage-free setting a constant elasticity of variance (CEV) equity model (to represent the behavior of equity prices prior to default), a default intensity process, and a Heath-Jarrow-Morton (HJM) model for the evolution of riskless interest rates. The model captures several stylized features such as a negative relation between equity prices and equity volatility, a negative relation between default intensity and equity prices, and a positive relationship between default intensity and equity volatility. We embed the model on a discrete-time, recombining lattice, making implementation feasible with polynomial complexity. We demonstrate the simplicity of calibrating the model to market data and of using it to extract default information. The framework is extensible to handling correlated default risk and may be used to value distressed convertible bonds, debt-equity swaps, and credit portfolio products such as collateralized debt obligations (CDOs). Applied to the CDX INDU (credit default index-industrials) Index, we find the S&amp;P 500 index explains credit premia.</description><author>Das, Sanjiv R.; Sundaram, Rangarajan K.</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The fragility of time: Time-insensitivity and valuation of the near and far future</title><link>http://www.example.com/articles/1</link><description>Ebert, Jane E. J.; Prelec, Drazen
We propose that the temporal dimension is fragile in that choices are insufficiently sensitive to it, and second, such sensitivity as exists is exceptionally malleable, unlike other dimensions such as money, which are attended by default. To test this, we axiomatize a "constant-sensitivity" discount function, and in four studies, we show that the degree of time-sensitivity is inadequate relative to the compound discounting norm, and strongly susceptible to manipulation. Time-sensitivity is increased by a comparative within-subject presentation (Experiment 1), direct instruction (Experiment 3), and provision of a visual cue for time duration (Experiment 4); time-sensitivity is decreased using a time pressure manipulation (Experiment 2). In each study, the sensitivity manipulation has an opposite effect on near-future and far-future valuations: Increased sensitivity decreases discounting in the near future and increases discounting in the far future. In contrast, such sensitivity manipulations have little effect on the money dimension.</description><author>Ebert, Jane E. J.; Prelec, Drazen</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Should price increases be targeted? Pricing power and selective vs. across-the-board price increases</title><link>http://www.example.com/articles/1</link><description>Krislma, Aradhna; Feinberg, Fred M.; Zhang, Z. John
Firms in many industries experience protracted periods of pricing power, the ability to successfully enact price increases. In these situations, firms must decide not only whether to raise prices, but to whom. Specifically, in a competitive context, they must determine whether it is more profitable to increase prices across-the-board or to a specific segment of their customer base. While selective price decreases are ubiquitous in practice (e.g., better deals to potential new customers by phone carriers; better deals to current customers by various magazines), to our knowledge selective price increases are relatively rare. We illustrate the benefits of targeted price increases, and, as such, we expand the repertoire of firms' promotional policies. To that end, we explore a scenario where, two competing firms must decide whether to increase prices to the entire market or only to a specific segment. Targeted price increases (TPI), i.e., being offered an unchanged price (selectively) when others are subject to price increases, can be offered to Loyals (those who bought from the firm in the previous period) or Switchers (those who did not). The effects of TPIs are estimated through a laboratory experiment and an associated stochastic model, each allowing for both rational (Loyalty, Switching) and behaviorist (Betrayal, Jealousy) effects. We find that TPIs can indeed yield beneficial results (greater retention for Loyals or greater attraction of Switchers) and greater profits in certain circumstances. Results for TPI are additionally benchmarked against those for targeted price decreases and are found to differ. The range of effects stemming from the experiment can be used in a competitive analysis to yield equilibrium strategies for the two firms. In this case, we find that-depending on the magnitude of the price increase, market shares of the two firms, and price knowledge across consumer segments-a firm may wish to embrace targeted price increases in some situations, to institute across-the-board price increases in others, and to not enact any price increases in still others. We show that a firm can sacrifice considerable profit if it settles on a suboptimal pricing strategy (e.g., wrongly. instituting an across-the-board increase), favors the wrong segment (e.g., Switchers instead of Loyals), or ignores "behaviorist" effects (Betrayal or Jealousy).</description><author>Krislma, Aradhna; Feinberg, Fred M.; Zhang, Z. John</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fair payments for efficient allocations in public sector combinatorial auctions</title><link>http://www.example.com/articles/1</link><description>Day, Robert W.; Raghavan, S.
Motivated by the increasing use of auctions by government agencies, we consider the problem of fairly pricing public goods in a combinatorial auction. A well-known problem with the incentive-compatible Vickrey-Clarke-Groves (VCG) auction mechanism is that the resulting prices may not be in the core. Loosely speaking, this means the payments of the winners could. be so low, that there are bidders who would have been willing to pay more than the payments of the winning bidders. Clearly, this "unfair" outcome is unacceptable for a public sector auction. Recent advances in auction theory suggest that combinatorial auctions resulting in efficient outcomes and bidder-Pareto-optimal core payments offer a viable practical alternative to address this problem. This paper confronts two critical issues facing the bidder-Pareto-optimal core payment. First, motivated to minimize a bidder's ability to benefit through strategic manipulation (through collusive agreement or unilateral action), we demonstrate the strength of a mechanism that minimizes total payments among all such auction outcomes, narrowing the previously broad solution concept. Second, we address the computational difficulties of achieving these outcomes with a constraint-generation approach,promising to broaden the range of applications for which bidder-Pareto-optimal core pricing achieves a comfortably rapid solution.</description><author>Day, Robert W.; Raghavan, S.</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Yahoo! for Amazon: Sentiment extraction from small talk on the web</title><link>http://www.example.com/articles/1</link><description>Das, Sanjiv R.; Chen, Mike Y.
Extracting sentiment from text is a hard semantic problem. We develop a methodology for extracting small investor sentiment from stock message boards. The algorithm comprises different classifier algorithms coupled together by a voting scheme. Accuracy levels are similar to widely used Bayes classifiers, but false positives are lower and sentiment accuracy higher. Time series and cross-sectional aggregation of messaged information improves the quality of the resultant sentiment index, particularly in the presence of slang and ambiguity. Empirical applications evidence a relationship with stock values-tech-sector postings are related to stock index levels, and to volumes and volatility. The algorithms may be used to assess the impact on investor opinion of management announcements, press releases, third-party news, and regulatory changes.</description><author>Das, Sanjiv R.; Chen, Mike Y.</author><pubDate>Sat, 01 Sep 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Auctioning supply contracts</title><link>http://www.example.com/articles/1</link><description>Chen, Fangruo
This paper studies a procurement problem with one buyer and multiple potential suppliers who hold private information about their own production costs. Both the purchase quantity and the price need to be determined. An optimal procurement strategy for the buyer requires the buyer to first design a supply contract that specifies a payment for each possible purchase quantity and then invites the suppliers to bid for this contract. The auction can be conducted in many formats such as the English auction, the Dutch auction, the first-priced, sealed-bid auction, and the Vickrey auction. The winner is the supplier with the highest bid, and is given the decision right for the quantity produced and delivered. Applying this theory to a newsvendor model with supply-side competition, this paper establishes a connection between the above optimal procurement strategy and a common practice in the retail industry, namely, the use of slotting allowances and vendor-managed inventory. Also discussed in the newsvendor context are the role of well-known supply contracts such as returns contracts and revenue-sharing contracts in procurement auctions, the scenarios where the buyer and suppliers may possess asymmetric information about the demand distribution, and how the cost of supply-demand mismatch is affected by supply-side competition. Finally, this paper compares the optimal procurement strategy with a simpler but suboptimal strategy where the buyer first determines a purchase quantity and then seeks the lowest-cost supplier for the quantity in an auction.</description><author>Chen, Fangruo</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information and inventory in distribution channels</title><link>http://www.example.com/articles/1</link><description>Iyer, Ganesh; Narasimhan, Chakravarthi; Niraj, Rakesh
We examine the trade-offs between demand information and inventory in a distribution channel. While better demand information has a positive direct effect for the manufacturer in improving the efficiency of holding inventory in a channel, it can also have the strategic effect of increasing retail prices and limiting the extraction of retail profits. Having inventory in the channel can help the manufacturer to manage retail pricing behavior while better extracting retail surplus. Thus, even if the information system is perfectly reliable, the manufacturer might not always want to institute an information-enabled channel over a channel with inventory. We show this first in a channel with a single retailer, where the channel with perfect information is preferred over the channel with inventory only if the marginal cost of production is sufficiently high. We also analyze a channel with an imperfectly reliable information system and find that if the manufacturer were to choose the precision of the demand information system, it might not prefer perfect information, even if such information was costless to acquire. In a channel with competing retailers, the channel with perfect information is preferred when retail competition is sufficiently intense. Thus, the presence of inventory can play a role in managing competition among retailers and in helping the manufacturers to appropriate surplus especially when retailers are sufficiently differentiated.</description><author>Iyer, Ganesh; Narasimhan, Chakravarthi; Niraj, Rakesh</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incentives that induce task-related effort, helping, and knowledge sharing in workgroups</title><link>http://www.example.com/articles/1</link><description>Siemsen, Enno; Balasubramanian, Sridhar; Roth, Aleda V.
Cooperation and coordination among employees can yield significant productivity gains. In this study, we explore the design of optimal incentive systems that induce task-related effort, helping, and knowledge sharing within workgroups. We identify three distinct types of employee linkages that must be accommodated in the design of effective incentive systems: (1) outcome linkages, whereby the outcome of one employee's task is influenced by that of another; (2) help linkages, whereby each employee can directly expend effort on helping another; and (3) knowledge linkages, whereby each employee can share job-related knowledge with another. We analytically investigate the effect of each type of employee linkage, and some combinations of these linkages, on the optimal design of incentive systems. Our analytical results demonstrate how, by optimally weighting individual-level and workgroup-level incentives, managers can balance the need to induce cooperation and coordination among employees with the need to manage employees' incentive-related risk. Counter to conventional wisdom, we also demonstrate that both group and individual incentives are necessary to facilitate cooperative behaviors such as knowledge sharing in workgroups. Further, we empirically test some of the insights developed from the analytical models; our empirical findings support these analytical results.</description><author>Siemsen, Enno; Balasubramanian, Sridhar; Roth, Aleda V.</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Campaign spending limits and political advertising</title><link>http://www.example.com/articles/1</link><description>Soberman, David; Sadoulet, Loik
Traditionally, research on political campaigns has focused on the positioning of parties and not on how parties communicate with the electorate. We construct a model where two parties fund both the "creative" and media" elements of political advertising and examine how campaign budgets affect advertising strategies in the context of a political campaign. Our key finding is that tight campaign limits stimulate aggressive advertising on the part of competing parties, while generous budgets often lead to parties acting defensively. The analysis also provides an explanation for the increasingly partisan campaigns that the Republicans and Democrats have taken in recent elections. When there is significant polarization amongst noncommitted voters and campaign spending limits are higher, we find that parties "retrench" toward traditional constituencies.</description><author>Soberman, David; Sadoulet, Loik</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain relationships and contracts: The impact of repeated interaction on capacity investment and procurement</title><link>http://www.example.com/articles/1</link><description>Taylor, Terry A.; Plambeck, Erica L.
Consider a firm developing an innovative product. Due to market pressures, production must begin soon after the product development effort is complete, which requires that an upstream supplier invests in capacity while the design of the product and production process are in flux. Because the product is ill-defined at this point in time, the firms are unable to write court-enforceable contracts that specify the terms of trade or the supplier's capacity investment. However, the firms can adopt an informal agreement (relational contract) regarding the terms of trade and capacity investment. The potential for future business provides incentive for the firms to adhere to the relational contract. We show that the optimal relational contract may be complex, requiring the buyer to order more than her demand to indirectly monitor the supplier's capacity investment. We propose a simpler relational contract and show that it performs very well for a broad range of parameters. Finally, we identify characteristics of the business environment that make relational contracting particularly valuable.</description><author>Taylor, Terry A.; Plambeck, Erica L.</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal risk taking with flexible income</title><link>http://www.example.com/articles/1</link><description>Cvitanic, Jaksa; Goukasian, Levon; Zapatero, Fernando
We study the portfolio selection problem of an investor who can optimally exert costly effort for more income. The possibility of generating more income, if necessary, increases the risk-taking appetite of the investor. We find the optimal allocation to the risky security as a proportion of financial wealth and as a proportion of the total wealth, defined as the combination of the financial wealth and the human capital of the investor. When the investor's objective is the maximization of the terminal wealth, we show that the optimal allocation to the risky security is a hump-shaped function of the investment horizon. However, when the investor maximizes utility from intertemporal consumption, the optimal allocation in the risky security is a constant proportion of the total wealth of the investor.</description><author>Cvitanic, Jaksa; Goukasian, Levon; Zapatero, Fernando</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Credit risk in a network economy</title><link>http://www.example.com/articles/1</link><description>Cossin, Didier; Schellhorn, Henry
We develop a structural model of credit risk in a network economy, where any firm can lend to any other firm, so that each firm is subject to counterparty risk either from direct borrowers or from remote firms in the network. This model takes into account the role of each firm's cash management. We show that we can obtain a semiclosed form formula for the price of debt and equity when cash accounts are buffers to bankruptcy risk. As in other structural models, the strategic bankruptcy decision of shareholders drives credit spreads, and differentiates debt from equity Cash-flow risk also causes credit-risk interdependencies between firms. Our model applies to the case where not only financial flows but also operations are dependent across firms. We use queueing theory to obtain our semiclosed form formulae in steady state. We perform a simplified implementation of our model to the U.S. automotive industry, and show how we infer the impact on a supplier's credit spreads of revenue changes in a manufacturer or even in a large car dealer. We also obtain prices for first-to-default and second-to-default basket credit default swaps.</description><author>Cossin, Didier; Schellhorn, Henry</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Real options in technology licensing</title><link>http://www.example.com/articles/1</link><description>Ziedonis, Arvids A.
This paper examines the use of options contracts by firms acquiring rights to commercialize university technologies. By combining information about the sequence of licensing decisions with characteristics of the firms and technologies involved, I explore factors that shape decisions to purchase and exercise option contracts for these technologies. Decisions by firms that considered but did not purchase an option or a license are included in the sample. Consistent with the basic premise of real-options theory, I find that firms are more likely to purchase option contracts for more uncertain technologies. Also in line with theoretical predictions, I find that firms that are better able to evaluate an external technology are less likely to purchase options before licensing. The results also highlight more complex motives for exercising options in technology licensing. On the one hand, firms appear to benefit from their ability to learn about the technology during the option period. On the other hand, firms that are better able to "absorb" the technology during the contract period may have reduced incentives to subsequently license the invention.</description><author>Ziedonis, Arvids A.</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Loss aversion under prospect theory: A parameter-free measurement</title><link>http://www.example.com/articles/1</link><description>Abdellaoui, Mohammed; Bleichrodt, Han; Paraschiv, Corina
A growing body of qualitative evidence shows that loss aversion, a phenomenon formalized in prospect theory, can explain a variety of field and experimental data. Quantifications of loss aversion are, however, hindered by the absence of a general preference-based method to elicit the utility for gains and losses simultaneously. This paper proposes such a method and uses it to measure loss aversion in an experimental study without making any parametric assumptions. Thus, it is the first to obtain a parameter-free elicitation of prospect theory's utility function on the whole domain. Our method also provides an efficient way to elicit utility midpoints, which are important in axiornatizations of utility. Several definitions of loss aversion have been put forward in the literature. According to most definitions we find strong evidence of loss aversion, at both the aggregate and the individual level. The degree of loss aversion varies with the definition used, which underlines the need for a commonly accepted definition of loss aversion.</description><author>Abdellaoui, Mohammed; Bleichrodt, Han; Paraschiv, Corina</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The performance consequences of ambidexterity in strategic alliance formations: Empirical investigation and computational theorizing</title><link>http://www.example.com/articles/1</link><description>Lin, Zhiang; Yang, Haibin; Demirkan, Irem
Although alliance studies have generally favored an ambidextrous approach between exploration and exploitation, they tend to overlook a firm's characteristics, its industry constraints, or the dynamic network in which the firm is embedded. This study examines the ambidexterity hypothesis and its boundary conditions with a unique research method. We not only analyze empirical data from five U.S. industries spanning eight years, but also expand theoretical insights to the network level by building a computer simulation model. Both our empirical and simulation results reveal the contingencies of the ambidexterity hypothesis in alliance formation. Our findings show that although an ambidextrous formation of alliances benefits large firms, a focused formation of either exploratory or exploitative alliances benefits small firms. In an uncertain environment an ambidextrous formation enhances firm performance but so does a focused formation in a stable environment. Finally, the simulation model demonstrates that a firm's centrality and structural hole positions in network relations can moderate the relationships between alliance formation choices and firm performance, and that the ambidexterity hypothesis may be limited to the earlier stage of the network. Our study provides critical evidence into the viability of adopting a dynamic network perspective in understanding the ambidexterity hypothesis and advancing strategic alliance research beyond static and dyadic levels.</description><author>Lin, Zhiang; Yang, Haibin; Demirkan, Irem</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Brand value in social interaction</title><link>http://www.example.com/articles/1</link><description>Kuksov, Dmitri
This paper explores the consumer value of publicly associating oneself with a brand image. The economic value of such association to the consumer of a brand is coming from its affect on the information exchange between consumers engaged in a search for partnerships with each other. It turns out that the brand use can be valuable to consumers for communication even when they do not have the proper incentives to make simple conversations valuable or informative. In particular, when the correlation of the interests of agents in a partnership is low, conversations are not very informative, while brand use remains informative and valuable. Furthermore, the more widespread the brand use is, the less truthful (and informative) one can expect conversations to be. In addition, the consumer value of a brand image is shown to have an inverse-U shape in the difficulty of searching, as consumers look for conformity when a search is difficult, and conversations become more and more truthful when a search becomes very easy.</description><author>Kuksov, Dmitri</author><pubDate>Mon, 01 Oct 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Outsourcing via service competition</title><link>http://www.example.com/articles/1</link><description>Benjaafar, Saif; Elahi, Ehsan; Donohue, Karen L.
We consider a single buyer who wishes to outsource a fixed demand for a manufactured good or service at a fixed price to a set of potential suppliers. We examine the value of competition as a mechanism for the buyer to elicit service quality from the suppliers. We compare two approaches the buyer could use to orchestrate this competition: (1) a supplier-allocation (SA) approach, which allocates a proportion of demand to each supplier with the proportion allocated to a supplier increasing in the quality of service the supplier promises to offer, and (2) a supplier-selection (SS) approach, which allocates all demand to one supplier with the probability that a particulhr supplier is selected increasing in the quality of service to which the supplier commits. In both cases, suppliers incur a cost whenever they receive a positive portion of demand, with this cost increasing in the quality of service they offer and the demand they receive. The analysis reveals that (a) a buyer could indeed orchestrate a competition among potential suppliers to promote service quality, (b) under identical allocation functions, the existence of a demand-independent service cost gives a distinct advantage to SS-type competitions, in terms of higher service quality for the buyer and higher expected profit for the supplier, (c) the relative advantage of SS versus SA depends on the magnitude of demand-independent versus demand-dependent service costs, (d) in the presence of a demand-independent service cost, a buyer should limit the number of competing suppliers under SA competition but impose no such limits under SS competition, and (e) a buyer can induce suppliers to provide higher service levels by selecting an appropriate allocation function. We illustrate the impact of these results through three example applications.</description><author>Benjaafar, Saif; Elahi, Ehsan; Donohue, Karen L.</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Size really matters - New insights for start-ups' survival</title><link>http://www.example.com/articles/1</link><description>Raz, Ornit; Gloor, Peter A.
This paper presents new evidence regarding a firm's probability for survival, based on the network structure of the firm's managers. We found that start-ups that have larger informal communication networks increased their chance to survive external shock. Original data have been collected from Israeli software start-ups during the dot-com economic growth. About eight years later, we added information about their ability to survive the burst of the dot-com. bubble. From a theoretical point of view, this paper highlights the power of the classic social networks approach in explaining organizational performance. From a practical point of view, these findings offer some guidelines for managers of start-ups. Our results show that the size of informal interfirm networks really matters.</description><author>Raz, Ornit; Gloor, Peter A.</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modularity and the impact of buyer-supplier relationships on the survival of suppliers</title><link>http://www.example.com/articles/1</link><description>Hoetker, Glenn; Swaminathan, Anand; Mitchell, Will
Modularity in product design and flexible supply chains is increasingly common in buyer-supplier relationships. Although the benefits of supply chain flexibility and component modularity for end-product manufacturers are accepted, little is known about their impact on suppliers. We advance the literature on modularity by exploring how three aspects of a supplier's relationships with its customers affect the supplier's survival: duration of buyer-supplier relationships, autonomy from customers, and links to prominent buyers. We compared the effects of these aspects of buyer-supplier relationships for low- and high-modularity components. Using data on U.S. carburetor and clutch manufacturers from 1918 to 1942, we found that suppliers of high-modularity components benefited more from autonomy provided by potential customers, whereas suppliers of low-modularity components benefited more from ties to higher status customers. Both benefited from autonomy generated by existing customers. Thus, relationships that require trust and extensive sets of interfirm routines, as do those for low-modularity components, led to both greater relationship benefits and greater constraints.</description><author>Hoetker, Glenn; Swaminathan, Anand; Mitchell, Will</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic technology choice and capacity investment under demand uncertainty</title><link>http://www.example.com/articles/1</link><description>Goyal, Manu; Netessine, Serguei
This paper studies the impact of competition on a firm's choice of technology (product-flexible or product-dedicated) and capacity investment decisions. Specifically, we model two firms competing with each other in two markets characterized by price-dependent and uncertain demand. The firms make three decisions in the following sequence: choice of technology (technology game), capacity investment (capacity game), and production quantities (production game). The technology and capacity games occur while the demand curve is still uncertain, and the production game is postponed until after the demand curve is revealed. We develop best-response functions for each firm in the technology game and compare how a monopolist and a cluopolist respond to a given flexibility premium. We show that the firms may respond to competition by adopting a technology which is the same as or different from what the competitor adopts. We conclude that contrary to popular belief, flexibility is not always the best response to competition-flexible and dedicated technologies may coexist in equilibrium. We demonstrate that as the difference between the two market sizes increases a cluopolist is willing to pay less for flexible technology, whereas the decision of a monopolist is not affected. Further, we find that a firm that invests in flexibility benefits from a low correlation between demands for two products, but the extent of this benefit differs depending on the competitor's technology choice. Our results indicate that higher demand substitution may or may not promote the adoption of flexibility under competition, whereas it always facilitates the adoption of flexibility without competition. Finally, we show that contrary to intuition, as the competitor's cost of capacity increases, the premium a flexible firm is willing to pay for flexibility decreases.</description><author>Goyal, Manu; Netessine, Serguei</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic IT investments: The impact of switching cost and declining IT cost</title><link>http://www.example.com/articles/1</link><description>Demirhan, Didem; Jacob, Varghese S.; Raghunathan, Srinivasan
The declining cost of information technology (IT) over time provides the later entrant in information-intensive industries a cost advantage. On the other hand, the earlier entrant has the potential to build and retain its market share if consumers incur a cost in switching to the later entrant. We investigate the impact of a decline in the IT cost and the switching cost on IT investment strategies of firms. We find that a declining IT cost always hurts the early entrant's profit. The early entrant may assume an aggressive investment strategy or a defensive investment strategy in response to a decline in the IT cost, depending on whether the switching cost relative to the extent of decline in the IT cost is high or low, respectively. A decline in IT cost also hurts the later entrant's profit if the switching cost is high. A surprising result is that when the decline in the IT cost is higher than a critical value, a higher switching cost increases consumer surplus. When firms control the switching cost, the early entrant increases its investment in quality and switching cost and maintains its quality and its market-share leadership irrespective of the extent of decline in the IT cost.</description><author>Demirhan, Didem; Jacob, Varghese S.; Raghunathan, Srinivasan</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Asymmetric consumer learning and inventory competition</title><link>http://www.example.com/articles/1</link><description>Gaur, Vishal; Park, Young-Hoon
We develop a model of consumer learning and choice behavior in response to uncertain service in the marketplace. Learning could be asymmetric, that is, consumers may associate different weights with positive and negative experiences. Under this consumer model, we characterize the steady-state distribution of demand for retailers given that each retailer holds a constant in-stock service level. We then consider a noncooperative game in steady state between two retailers competing on the basis of their service levels. The demand distributions of retailers in this game are modeled using a multiplicative aggregate market-share model in which the mean demands are obtained from the steady-state results for individual purchases, but the model is simplified in other respects for tractability. Our model yields a unique pure strategy Nash equilibrium. We show that asymmetry in consumer learning has a significant impact on the optimal service levels, market shares, and profits of the retailers. When retailers have different costs, it also determines the extent of competitive advantage enjoyed by the lower-cost retailer.</description><author>Gaur, Vishal; Park, Young-Hoon</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Testing a life-cycle theory of cooperative interorganizational relationships: Movement across stages and performance</title><link>http://www.example.com/articles/1</link><description>Jap, Sandy D.; Anderson, Erin
This research examines the evolution of cooperative interorganizational relationships and provides an empirical test of four propositions from the DSO (Dwyer et al. 1987) hfe-cycle theory, and one proposition from the RV (Ring and Van cle Ven 1994) theory of relationship development. Using primary data from over 1,500 resellers in a channel of distribution, we find that the mature phase is not the pinnacle of the relationship lifecycle; relationship properties (e.g., relationship harmony, overall dependence, and the reseller's trust in the manufacturer) in this stage are no different than in the build-up phase. However, relationship properties that support relationship expansion (e.g., goal congruence and information exchange norms) reach their zenith in the build-up phase and afterwards fade into the background. All of the various relationship properties hit their nadir in the decline phase. We also examine the development of relationships over a five-year period and consider whether movement across the stages in accordance with DSO's theory has the same association to overall performance evaluations as movement through regressive patterns. We find that a negative history extracts a price: Movement through regressive patterns is negatively related to performance, and these relationships do not enjoy a fresh start. Instead, these movements can last for an extended period of time and are negatively related to performance outcomes during the decline phase. Thus, the development path taken appears to be related to the results achieved. Finally, we also find evidence of the critical role that individual sales representatives play in creating successful interorganizational relationships.</description><author>Jap, Sandy D.; Anderson, Erin</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic assortment with demand learning for seasonal consumer goods</title><link>http://www.example.com/articles/1</link><description>Caro, Felipe; Gallien, Jeremie
Companies such as Zara and World Co. have recently implemented novel product development processes and supply chain architectures enabling them to make more product design and assortment decisions during the selling season, when actual demand information becomes available. How should such retail firms modify their product assortment over time in order to maximize overall profits for a given selling season? Focusing on a stylized version of this problem, we study a finite horizon multiarmed bandit model with several plays per stage and Bayesian learning. Our analysis involves the Lagrangian relaxation of weakly coupled dynamic programs (I)Ps), results contributing to the emerging theory of DP cluality and various approximations. It yields a closed-form dynamic index policy capturing the key exploration versus exploitation trade-off and associated suboptimality bounds. In numerical experiments its performance proves comparable to that of other closed-form heuristics described in the literature, but this policy is particularly easy to implement and interpret. This last feature enables extensions to more realistic versions of the motivating dynamic assortment problem that include implementation delays, switching costs, and demand substitution effects.</description><author>Caro, Felipe; Gallien, Jeremie</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Replenishment strategies in inventory/distribution systems</title><link>http://www.example.com/articles/1</link><description>Guerbuez, Mustafa Cagri; Moinzadeh, Kamran; Zhou, Yong-Pin
In this paper, we study the impact of coordinated replenishment and shipment in inventory/distribution systems. We analyze a system with multiple retailers and one outside supplier. Random demand occurs at each retailer, and the supplier replenishes all the retailers. In traditional inventory models, each retailer orders directly from the supplier whenever the need arises. We present a new, centralized ordering policy that orders for all retailers simultaneously. The new policy is equivalent to the introduction of a warehouse with no inventory that is in charge of the ordering, allocation, and distribution of inventory to the retailers. Under such a policy, orders for some retailers may be postponed or expedited so that they can be batched with other retailers' orders, which results in savings in ordering and shipping costs. In addition to the policy we propose for supplying inventory to the retailers, we also consider three other policies that are based on these well-known policies in the literature: (a) can-order policy, (b) echelon inventory policy, and (c) fixed-replenishment interval policy Furthermore, we create a framework for simultaneously making inventory and transportation decisions by incorporating the transportation costs (or limited truck capacities). We numerically compare the performance of our proposed policy with these policies to identify the settings in which each policy would perform well.</description><author>Guerbuez, Mustafa Cagri; Moinzadeh, Kamran; Zhou, Yong-Pin</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bias and variance approximation in value function estimates</title><link>http://www.example.com/articles/1</link><description>Mannor, Shie; Simester, Duncan; Sun, Peng; Tsitsiklis, John N.
We consider a finite-state, finite-action, infinite-horizon, discounted reward Markov decision process and study the bias and variance in the value function estimates that result from empirical estimates of the model parameters. We provide closed-form approximations for the bias and variance, which can then be used to derive confidence intervals around the value function estimates. We illustrate and validate our findings using a large database describing the transaction and mailing histories for customers of a mail-order catalog firm.</description><author>Mannor, Shie; Simester, Duncan; Sun, Peng; Tsitsiklis, John N.</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An extreme value approach to estimating interest-rate volatility: Pricing implications for interest-rate options</title><link>http://www.example.com/articles/1</link><description>Bali, Turan G.
This paper proposes an extreme value approach to estimating interest-rate volatility and shows that during the extreme movements of the U.S. Treasury market the volatility of interest-rate changes is underestimated by the standard approach that uses the thin-tailed normal distribution. The empirical results indicate that (1) the volatility of maximal and minimal changes in interest rates declines as time-to-maturity rises, yielding a downward-sloping volatility curve for the extremes; (2) the minimal changes are more volatile than the maximal changes for all data sets and for all asymptotic distributions used; (3) the minimal changes in Treasury yields have fatter tails than the maximal changes; and (4) for both the maxima and minima, the extreme changes in short-term rates have thicker tails than the extreme changes in long-term rates. This paper extends the standard option-pricing models with lognormal forward rates to accomrnodate significant kurtosis observed in the interest-rate data. This paper introduces a closed-form option-pricing model based on the generalized extreme value distribution that successfully removes the well-known pricing bias of the lognormal distribution.</description><author>Bali, Turan G.</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capturing flexible heterogeneous utility curves: A Bayesian spline approach</title><link>http://www.example.com/articles/1</link><description>Kim, Jin Gyo; Menzefricke, Ulrich; Feinberg, Fred M.
Empirical evidence suggests that decision makers often weight successive additional units of a valued attribute or monetary endowment unequally, so that their utility functions are intrinsically nonlinear or irregularly shaped. Although the analyst may impose various functional specifications exogenously, this approach is ad hoc, tedious, and reliant on various metrics to decide which specification is "best." In this paper, we develop a method that yields individual-level, flexibly shaped utility functions for use in choice models. This flexibility at the individual level is accomplished through splines of the truncated power basis type in a general additive regression framework for latent utility Because the number and location of spline knots are unknown, we use the birth-death process of Denison et al. (1998) and Green's (1995) reversible jump method. We further show how exogenous constraints suggested by theory, such as monotonicity of price response, can be accommodated. Our formulation is particularly suited to estimating reaction to pricing, where individual-level monotonicity is justified theoretically and empirically, but linearity is typically not. The method is illustrated in a conjoint application in which all covariates are splined simultaneously and in three panel data sets, each of which has a single price spline. Empirical results indicate that piecewise linear splines with a modest number of knots fit these data well, substantially better than heterogeneous linear and log-linear a priori specifications. In terms of price response specifically, we find that although aggregate market-level curves can be nearly linear or loglinear, individuals often deviate widely from either. Using splines, hold-out prediction improvement over the standard heterogeneous probit model ranges from 6% to 14% in the scanner applications and exceeds 20% in the conjoint study. Moreover, "optimal" profiles in conjoint and aggregate price response curves in the scanner applications can differ markedly under the standard and the spline-based models.</description><author>Kim, Jin Gyo; Menzefricke, Ulrich; Feinberg, Fred M.</author><pubDate>Thu, 01 Feb 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The economics of remanufacturing under limited component durability and finite product life cycles</title><link>http://www.example.com/articles/1</link><description>Geyer, Roland; Van Wassenhove, Luk N.; Atasu, Atalay
This paper models and quantifies the cost-savings potential of production systems that collect, remanufacture, and remarket end-of-use products as perfect substitutes while facing the fundamental supply-loop constraints of limited component durability and finite product life cycles. The results demonstrate the need to carefully coordinate production cost structure, collection rate, product life cycle, and component durability to create or maximize production cost savings from remanufacturing.</description><author>Geyer, Roland; Van Wassenhove, Luk N.; Atasu, Atalay</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Minimizing information loss and preserving privacy</title><link>http://www.example.com/articles/1</link><description>Menon, Syam; Sarkar, Sumit
The need to hide sensitive information before sharing databases has long been recognized. In the context of data mining, sensitive information often takes the form of itemsets that need to be suppressed before the data is released. This paper considers the problem of minimizing the number of nonsensitive itemsets lost while concealing sensitive ones. It is shown to be an intractably large version of an NP-hard problem. Consequently, a two-phased procedure that involves the solution of two smaller NP-hard problems is proposed as a practical and effective alternative. In the first phase, a procedure to solve a sanitization problem identifies how the support for sensitive itemsets could be eliminated from a specific transaction by removing the fewest number of items from it. This leads to a modified frequent itemset hiding problem, where transactions to be sanitized are selected such that the number of nonsensitive itemsets lost, while concealing sensitive ones, is minimized. Heuristic procedures are developed for these problems using intuition derived from their integer programming formulations. Results from computational experiments conducted on a publicly available retail data set and three large data sets generated using IBM's synthetic data generator indicate that these approaches are very effective, solving problems involving up to 10 million transactions in a short period of time. The results also show that the process of sanitization. has considerable bearing on the quality of solutions obtained.</description><author>Menon, Syam; Sarkar, Sumit</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A good sign for multivariate risk taking</title><link>http://www.example.com/articles/1</link><description>Eeckhoudt, Louis; Rey, Beatrice; Schlesinger, Harris
Decisions under risk are often multidimensional, where the preferences of the decision maker depend on example, an individual might be concerned about both her level of wealth and the condition of her health. Many times the signs of successive cross-derivatives of a utility function play an important role in these models. However, there has not been a simple and intuitive interpretation for the meaning of such derivatives. The purpose of this paper is to give such an interpretation. In particular, we provide an equivalence between the signs of these cross-derivatives and individual preference within a particular class of simple lotteries.</description><author>Eeckhoudt, Louis; Rey, Beatrice; Schlesinger, Harris</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Equivalent information for multiobjective interactive procedures</title><link>http://www.example.com/articles/1</link><description>Luque, Mariano; Caballero, Rafael; Molina, Julian; Ruiz, Francisco
Despite the mathematical properties and algorithmic features of an interactive method, that method's success usually lies in the kind of information it requires from the decision maker. In some cases, this information constrains the decision maker. If she does not find it easy and comfortable to answer the questions posed by the algorithm, then she will very likely give inconsistent answers, and the method will fail to find her most preferred solution. Therefore, it is of interest to find relations among the different kinds of information (local weights, local trade-offs, reference points, etc.) to allow the decision maker to choose what kind of questions he wants to answer, and to provide him with enough information to give such answers. In this paper, we define equivalent information-that is, different kinds of information-that produces the same solution when used in their corresponding interactive schemes.</description><author>Luque, Mariano; Caballero, Rafael; Molina, Julian; Ruiz, Francisco</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Mean-variance-skewness portfolio performance gauging: A general shortage function and dual approach</title><link>http://www.example.com/articles/1</link><description>Briec, Walter; Kerstens, Kristiaan; Jokung, Octave
This paper proposes a nonparametric efficiency measurement approach for the static portfolio selection problem in mean-variance-skewness space. A shortage function is defined that looks for possible increases in return and skewness and decreases in variance. Global optimality is guaranteed for the resulting optimal portfolios. We also establish a link to a proper indirect mean-variance-skewness utility function. For computational reasons, the optimal portfolios resulting from this dual approach are only locally optimal. This framework permits to differentiate between portfolio efficiency and allocative efficiency, and a convexity efficiency component related to the difference between the primal, nonconvex approach and the dual, convex approach. Furthermore, in principle, information can be retrieved about the revealed risk aversion and prudence of investors. An empirical section on a small sample of assets serves as an illustration.</description><author>Briec, Walter; Kerstens, Kristiaan; Jokung, Octave</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The role of production lead time and demand uncertainty in marketing durable goods</title><link>http://www.example.com/articles/1</link><description>Desai, Preyas S.; Koenigsberg, Oded; Purohit, Devavrat
Firms often have to make their production decisions under conditions of demand uncertainty. This is especially true for product categories such as automobiles and technology goods where the lead time needed for manufacturing forces firms to make production decisions well in advance of the selling season. Once the firm has produced the goods, the available production volume affects the firm's subsequent marketing decisions. In this paper, we study the relationship between the firm's production and marketing decisions for a durable goods manufacturer. We develop a dynamic model of a durable product market in which the demand functions are developed from a micromodeling of consumer utility functions and an equilibrium analysis of consumer strategies. After taking into account the demand uncertainty as well as the potential for cannibalization of future sales, the manufacturer makes its production and sales decisions. We find that the firm's optimal inventory level is U-shaped in the durability of the product and that the firm suffers a larger loss due to uncertainty when it is leases rather than sells its products. Furthermore, unlike the case for nondurables, for durable goods we find that the effect of uncertainty persists even after the uncertainty has been resolved.</description><author>Desai, Preyas S.; Koenigsberg, Oded; Purohit, Devavrat</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competitive bundling and counterbundling with generalist and specialist firms</title><link>http://www.example.com/articles/1</link><description>Ghosh, Bikram; Balachander, Subramanian
Bundling, which is the practice of selling two or more products or services in a package, is a pervasive marketing practice and is often used as a strategic competitive tool. However, there has not been enough consideration of competitive bundling situations in which exit of a competitor is not a concern. In this paper, we address this issue by identifying conditions under which strategic competitors may or may not resort to bundling when competitor exit considerations are absent. We study competition between a multiproduct generalist firm and two single-product specialist firms in two product categories, one of which has undifferentiated products and the other has differentiated products. In our model, the specialist firms can form an alliance to bundle their products in competing with the generalist firm. In contrast to the previous literature, we find that concurrent bundling by competitors, if it occurs in equilibrium, is profitable. We also find that when one competitor bundles and the other does not, the bundling firm gains a greater share of customers and makes a higher profit. However, when conditions favor counterbundling by a competitor, such counterbundling helps the competitor retain its customers. Finally, we note that under other market conditions, concurrent bundling by competitors escalates price competition to the extent that retaining customers through bundling is not profitable. In such a case, we show that strategic competitors are better off having asymmetric product lines with one competitor bundling and the other selling unbundled.</description><author>Ghosh, Bikram; Balachander, Subramanian</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on strategic dynamics</title><link>http://www.example.com/articles/1</link><description>Ghemawat, Pankaj; Cassiman, Bruno
This introductory essay connects the various contributions included in the special issue on strategic dynamics and contrasts them with the static analyses that predominate in the strategy field. In addition to highlighting a variety of methodological approaches, the contributions shed substantive light on strategic dynamics at the value system and industry and firm levels. Taken together, they also suggest some broad directions for further work aimed at making dynamics a more important part of the future of the field of strategy.</description><author>Ghemawat, Pankaj; Cassiman, Bruno</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Shipment consolidation: Who pays for it and how much?</title><link>http://www.example.com/articles/1</link><description>Dror, Moshe; Hartman, Bruce C.
This paper examines the subject of cost allocation in a multiple product inventory system, allowing for consolidation of shipments. If we order multiple items using an economic order quantity (EOQ) policy, and consolidate shipments, part of the ordering cost is shared, and part is specific to each. item; we want to find the consolidation choice with optimal total cost and divide the cost fairly among the individual items. Such a fair division is central to a costing system in which no group of items subsidizes the others; there are no free riders! We use a cooperative inventory game to determine when this can be done. This game is usually not concave, so we want to know what consolidation combinations determine when this cost can be fairly divided, using the core of the game. We prove that consolidation of all the items is cheaper exactly if there are fair cost allocations (core of the game is not empty), which happens when the portion of the ordering cost common to all items is not too small. We further show how sensitive the nonempty core result is to adjustments in the cost parameters and show how to determine a threshold value for the shared ordering cost, which assures the existence of a fair cost allocation.</description><author>Dror, Moshe; Hartman, Bruce C.</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Operations systems with discretionary task completion</title><link>http://www.example.com/articles/1</link><description>Hopp, Wallace J.; Iravani, Seyed M. R.; Yuen, Gigi Y.
Most performance evaluation models in the operations management literature implicitly assume that tasks possess standardized completion criteria. However, in many systems, particularly service and professional work, judgment is frequently required to determine how much time to allocate to a task. In this paper, we show that introducing discretion in task completion adds a fourth variability buffer, quality, to the well-known buffers of capacity, inventory and time. To gain insight into the managerial implications of this difference, we model the work of one- and two-worker systems with discretionary task completion as controlled queues. After characterizing the optimal control policy and identifying some practical heuristics, we use this model to examine the differences between discretionary and noncliscretionary work. We show that in systems with discretionary task completion, (i) adding capacity may actually increase congestion, and (ii) task variability in service time can improve system performance. This implies that it may be suboptimal to expect shorter delays as a result of a capacity increase, and that task variability reduction may not be an appropriate goal in systems with discretionary task completion. We also find that the benefit of queue pooling is smaller in systems with discretionary task completion than in systems with nondiscretionary task completion.</description><author>Hopp, Wallace J.; Iravani, Seyed M. R.; Yuen, Gigi Y.</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal advertising and promotion budgets in dynamic markets with brand equity as a mediating variable</title><link>http://www.example.com/articles/1</link><description>Sriram, S.; Kalwani, Manohar U.
We study the optimal levels of advertising and promotion budgets in dynamic markets with brand equity as a mediating variable. To this end, we develop and estimate a state-space model based on the Kalman filter that captures the dynamics of brand equity as influenced by its drivers, such as the brand's advertising and sales promotion expenditures. By integrating the Kalman filter with the random coefficients logit demand model, our estimation allows us to capture the dynamics of brand equity as well as to model consumer heterogeneity using store-level data. Using these demand model estimates, we determine the Markov perfect equilibrium advertising and promotion strategies. Our empirical analysis is based on store-level scanner data in the orange juice category, which comprises two major brands-Tropicana and Minute Maid. As expected, we find that sales promotions have a significant positive effect on consumers' utility and induce consumers to switch to the promoted brand. However, there is also a negative effect of promotions on brand equity that carries over from period to period. Overall, we find that while sales promotions have a net positive impact both in the short term and in the long term, the implied total profit elasticity including the long-term effect is smaller than the short-term profit elasticity. Correspondingly, we expect myopic decision makers to allocate higher than optimal expenditures to sales promotions. Our results from the supply-side analysis reveal that the actual promotion levels for both brands are indeed higher than the optimal budgets for the forward-looking (long-term orientation) as well as the two-year planning horizon scenarios. Hence, it may be profitable for both brands to reduce their promotion levels. Further, we find that although the forward-looking promotional spending levels are higher for the smaller brand, Minute Maid, it is market leader Tropicana that spends more on sales promotions. Turning to optimal advertising budgets, we find that the equilibrium forward-looking advertising levels are higher for Tropicana, the brand that has higher brand equity and a higher responsiveness to advertising. Further, as expected, the optimal forward-looking advertising levels are higher than the myopic levels and the two-year planning horizon levels for both brands. However, the forward-looking advertising levels are lower than the actual advertising expenditures for both brands. This implies that even when we consider the long-term effects of advertising, the brands are overspending on advertising.</description><author>Sriram, S.; Kalwani, Manohar U.</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stable farsighted coalitions in competitive markets</title><link>http://www.example.com/articles/1</link><description>Nagarajan, Mahesh; Sosic, Greys
In this paper, we study dynamic alliance formation among agents in competitive markets. We look at n agents selling substitutable products competing in a market. In this setting, we examine models with deterministic and stochastic demand, and we use a two-stage approach. In Stage 1, agents form alliances (coalitions), and in Stage 2, coalitions make decisions (price and inventory) and compete against one another. To analyze the stability of coalition structures in Stage 1, we use two notions from cooperative games-the largest consistent set (LCS) and the equilibrium process of coalition formation (EPCF)-which allow players to be farsighted. Thus, in forming alliances, players consider two key phenomena: First, players trade off the size of the total profit of the system versus their allocation of this total pie, and second, they weigh the possibility that an immediate beneficial defection can trigger further counter defections that in the end may prove to be worse than the status quo. In particular, one such example is that of the grand coalition-which we show to be stable in the farsighted sense-even though players benefit myopically by defecting from it. We also provide conditions under which a situation of a few lone players competing against a large coalition is stable. We examine the impact of the size of the market (n), the degree of competition, the effect of cost parameters, and the variability of the demand process on the prices, inventory levels, and structure of the market. We discuss the possible strategic implications of our results to firms in a competitive market and for new entrants.</description><author>Nagarajan, Mahesh; Sosic, Greys</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Past success and creativity over time: A study of inventors in the hard disk drive industry</title><link>http://www.example.com/articles/1</link><description>Audia, Pino G.; Goncalo, Jack A.
We integrate psychological theories of individual creativity with organizational theories of exploration versus exploitation to examine the relationship between past success and creativity over time. A key prediction derived from this theoretical integration is that successful people should be more likely to generate new ideas, but these ideas will tend to be less divergent as they favor the exploitation of familiar knowledge at the expense of the exploration of new domains. This prediction departs from the often-held view that people who generate more ideas will also generate ideas that are more divergent. Analyses of patenting in the hard disk drive industry support our prediction and indicate that collaboration with other inventors and organizational norms for exploration attenuate the tendency for successful individuals to generate increasingly incremental ideas.</description><author>Audia, Pino G.; Goncalo, Jack A.</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Vicarious learning in new product introductions in the early years of a converging market</title><link>http://www.example.com/articles/1</link><description>Srinivasan, Raji; Haunschild, Pamela; Grewal, Rajdeep
Technological developments combine previously distinct technologies that result in converging markets. In converging markets, firms from different industries compete against each other, often for the first time. We propose that firms introducing new products in converging markets will learn vicariously from other firms in the market. Further, we propose that this learning will vary across the dual-technology frontier (DTF), where the high-technology frontier (HTF) and low-technology frontier (LTF) map onto innovative activities driven by technological opportunity and user needs. We propose that at the HTF, local search will dominate and firms will be influenced by HTF product introductions of similarly sized, successful firms. At the LTF, learning will occur across the DTF, vary by origin industry of the firm, and be affected by complementarities in routines and capabilities and market competition among firms. We test the proposed model of vicarious learning using panel data on new product introductions of 67 firms in the U.S. digital camera market in the 1990s. Findings generally support our proposed model of vicarious learning in this market. They show heterogeneity in vicarious learning across the technology frontier and firm characteristics-including the origin industry of target firms. Our results show that vicarious learning in new product introductions in converging markets-which includes both mimetic and nonmimetic learning-is similar in some ways, but different from more traditional markets. We conclude with a discussion of the implications of our findings for theories of organizational learning, new product development, and converging markets.</description><author>Srinivasan, Raji; Haunschild, Pamela; Grewal, Rajdeep</author><pubDate>Mon, 01 Jan 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The horizontal scope of the firm: Organizational tradeoffs vs. buyer-supplier relationships</title><link>http://www.example.com/articles/1</link><description>Chatain, Olivier; Zemsky, Peter
Horizontal scope-the set of products and services offered-is an important dimension of firm strategy and a potentially significant source of competitive advantage. On one hand, the ability to build close buyer-supplier relationships over multiple transactions can give an advantage to broad firms that offer buyers "one-stop shopping." On the other hand, the existence of organizational tradeoffs can give an advantage to firms that specialize in a narrower range of products or services. We develop a biform game that incorporates this tension and show how the use of three generic scope strategies-specialist, generalist, and hybrid-depends on organizational tradeoffs, client-specific scope economies, barriers to entry heterogeneity in buyer task requirements, and the bargaining power of suppliers relative to buyers. We then use the model to study a variety of issues in supply chain management, including the gains to coordinating suppliers, the optimal level of buyer power, and the desirability of subsidizing suppliers. One of our objectives is to show how biform games, which introduce unstructured negotiations into game theory analysis, can be used to develop applied theory relevant to strategy. Generalizing from our stylized model, we identify a class of biform games involving buyers and suppliers that is useful for strategy analysis. Games in this class have the attractive property of each supplier's share of industry total surplus being the product of its added value and its relative bargaining power.</description><author>Chatain, Olivier; Zemsky, Peter</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Biform games</title><link>http://www.example.com/articles/1</link><description>Brandenburger, Adam; Stuart, Harborne
Both noncooperative and cooperative game theory have been applied to business strategy. We propose a hybrid noncooperative-cooperative game model, which we call a biform game. This is designed to formalize the notion of business strategy as making moves to try to shape the competitive environment in a favorable way. (The noncooperative component of a biform game models the strategic moves. The cooperative component models the resulting competitive environment.) We give biform models of various well-known business strategies. We prove general results on when a business strategy, modelled as a biform game, will be efficient.</description><author>Brandenburger, Adam; Stuart, Harborne</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Brokers and competitive advantage</title><link>http://www.example.com/articles/1</link><description>Ryall, Michael D.; Sorenson, Olav
The broker profits by intermediating between two (or more) parties. Using a biform game, we examine whether such a position can confer a competitive advantage, as well as whether any such advantage could persist if actors formed relations strategically. Our analysis reveals that, if one considers exogenous the relations between actors, brokers can enjoy an advantage but only if (1) they do not face substitutes either for the connections they offer or the value they can create, (2) they intermediate more than two parties, and (3) interdependence does not lock them into a particular pattern of exchange. If, on the other hand, one allows actors to form relations on the basis of their expectations of the future value of those relations, then profitable positions of intermediation only arise under strict assumptions of unilateral action. We discuss the implications of our analysis for firm strategy and empirical research.</description><author>Ryall, Michael D.; Sorenson, Olav</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Wintel: Cooperation and conflict</title><link>http://www.example.com/articles/1</link><description>Casadesus-Masanell, Ramon; Yoffie, David B.
We study competitive interactions between Intel and Microsoft, two producers of complementary products. In a system of complements, like the PC, the value of the final product depends on how well the different components work together. This, in turn, depends on the firms' investment in complementary R&amp;D. We ask whether Intel and Microsoft will want to cooperate and make the final product as valuable as possible. Contrary to the popular view that two tight complements will generally have well aligned incentives, we demonstrate that natural conflicts emerge over pricing, the timing of new product releases, and who captures the greatest value at different phases of product generations.</description><author>Casadesus-Masanell, Ramon; Yoffie, David B.</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interdependency, competition, and industry dynamics</title><link>http://www.example.com/articles/1</link><description>Lenox, Michael J.; Rockart, Scott F.; Lewin, Arie Y.
A systematic understanding of industry dynamics is critical to strategy research because individual firm performance dynamics both reflect and affect change at the industry level. Descriptive research on industry dynamics has identified a dominant pattern where prices fall, output rises, and the number of firms rises and then falls over time. Several models have been advanced to explain these patterns, with a particular focus on explaining why a shakeout in the number of firms occurs. In the most prominent models, shakeout is generated by rising realized heterogeneity among firms that either is assumed to be unrecognized but determined ex ante or is generated by stochastic innovation outcomes coupled with convex adjustment costs and scale advantages in innovation and learning. In this paper, we develop an alternative model where heterogeneity. develops among firms over time (leading to a shakeout) because firms must make choices about highly interdependent productive activities where the ideal combinations cannot be easily deduced or imitated. By combining two established models (a Cournot model of competition with an NK model of interdependency in production activities), we are able to advance an alternative explanation for the observed patterns of industry behavior, including shakeout. We show that variation in the potential for interdependency in activities among industries is able to explain varying levels of shakeout as well as differing patterns of entry and exit among industries. Notably, the model generates several empirical predictions not apparent in past research and several that directly conflict with the results of prominent alternative models of industry dynamics. Specifically, we show that when the potential for interdependency within an industry is lows entry slows down and incumbent survival is all but assured, whereas in industries where the potential for interdependencies is high, shakeouts are severe and the rates of entry and exit remain high over longer time periods, with decreasing survival rates for incumbents.</description><author>Lenox, Michael J.; Rockart, Scott F.; Lewin, Arie Y.</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Disagreements, spinoffs, and the evolution of Detroit as the capital of the US automobile industry</title><link>http://www.example.com/articles/1</link><description>Klepper, Steven
The agglomeration of the automobile industry around Detroit, Michigan is explained using a theory in which disagreements lead employees of incumbent firms to found spinoffs in the same industry. Predictions of the theory concerning entry and firm survival are tested using data on the origin, location, and years of production of every entrant into the industry from 1895 to 1966. The geographic concentration of the industry is attributed to four early successful entrants and the many successful spinoffs they spawned in the Detroit area and not to conventional agglomeration economies benefiting co-located firms, as featured in modern theories of agglomeration. Implications of the findings regarding firm strategy are discussed.</description><author>Klepper, Steven</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Churn, baby, churn: Strategic dynamics among dominant and fringe firms in a segmented industry</title><link>http://www.example.com/articles/1</link><description>de Figueiredo, John M.; Silverman, Brian S.
This paper integrates and extends the literatures on industry evolution and dominant firms to develop a dynamic theory of dominant and fringe competitive interaction in a segmented industry. It argues that a dominant firm, seeing contraction of growth in its current segment(s), enters new segments in which it can exploit its technological strengths, but that are sufficiently distant to avoid cannibalization. The dominant firm acts as a low-cost Stackelberg leader, driving down prices and triggering a sales takeoff in the new segment. We identify a "churn" effect associated with dominant firm entry: fringe firms that precede the dominant firm into the segment tend to exit the segment, while new fringe firms enter, causing a net increase in the number of firms in the segment. As the segment matures and sales decline in the segment, the process repeats itself. We examine the predictions of the theory with a study of price, quantity, entry, and exit across 24 product classes in the desktop laser printer industry from 1984 to 1996. Using descriptive statistics, hazard rate models, and panel data methods, we find empirical support for the theoretical predictions.</description><author>de Figueiredo, John M.; Silverman, Brian S.</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic management of R&amp;D pipelines with cospecialized investments and technology markets</title><link>http://www.example.com/articles/1</link><description>Chan, Tat; Nickerson, Jack A.; Owan, Hideo
The theoretical literature on managing R&amp;D pipelines is largely based on real option theory making decisions about undertaking, continuing, or terminating projects. The theory typically assumes that each project or causally related set of projects is independent. However, casual observation suggests that firms expend much effort on managing and balancing their R&amp;D pipelines, where managing appears to be related to the choice of R&amp;D selection thresholds, project risk, and whether to buy or sell projects to fill the pipeline. Not only do these policies appear to differ across firms, they also appear to vary over time for the same firm. Changes in management policies suggest that the choice of R&amp;D selection thresholds is a time-varying strategic decision and there may be some type of vertical interdependency among R&amp;D projects in different stages. In this paper we develop a model using dynamic-programming techniques that explain why firms vary in their R&amp;D project-management policies. The novelty and value of our model derives from the central insight that some firms invest in downstream cospecialized activities that would incur substantial adjustment costs if R&amp;D efforts are unsuccessful whereas other firms have no such investment. If transaction costs in technology markets are positive, which implies that accessing the market for projects is costly, these investments lead to state-contingent project-selection rules that create a dynamic and vertical interdependency among R&amp;D activities and product mix. We describe how choices of R&amp;D selection thresholds, preferences over project risk, and use of technology markets for the buying and selling of projects differ by the state of the firm's pipeline, the magnitude of transaction costs in the adjustment market, and the magnitude of technology costs. These results yield interesting managerial and public policy implications.</description><author>Chan, Tat; Nickerson, Jack A.; Owan, Hideo</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Getting big too fast: Strategic dynamics with increasing returns and bounded rationality</title><link>http://www.example.com/articles/1</link><description>Sterman, John D.; Henderson, Rebecca; Beinhocker, Eric D.; Newman, Lee I.
Neoclassical models of strategic behavior have yielded many insights into competitive behavior, despite the fact that they often rely on a number of assumptions-including instantaneous market clearing and perfect foresight-that have been called into question by a broad range of research. Researchers generally argue that these assumptions are "good enough" to predict an industry's probable equilibria, and that disequilibrium adjustments and bounded rationality have limited competitive implications. Here we focus on the case of strategy in the presence of increasing returns to highlight how relaxing these two assumptions can lead to outcomes quite different from those predicted by standard neoclassical models. Prior research suggests that in the presence of increasing returns, tight appropriability and accommodating rivals, in some circumstances early entrants can achieve sustained competitive advantage by pursuing "get big fast" (GBF) strategies: Rapidly expanding capacity and cutting prices to gain market share advantage and exploit positive feedbacks faster than their rivals. Using a simulation of the duopoly case we show that when the industry moves slowly compared to capacity adjustment delays, boundedly rational firms find their way to the equilibria predicted by conventional models. However, when market dynamics are rapid relative to capacity adjustment, forecasting errors lead to excess capacity-overwhelming the advantage conferred by increasing returns. Our results highlight the risks of ignoring the role of disequilibrium dynamics and bounded rationality in shaping competitive outcomes, and demonstrate how both can be incorporated into strategic analysis to form a dynamic, behavioral game theory amenable to rigorous analysis.</description><author>Sterman, John D.; Henderson, Rebecca; Beinhocker, Eric D.; Newman, Lee I.</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The timing of resource development and sustainable competitive advantage</title><link>http://www.example.com/articles/1</link><description>Pacheco-de-Almeida, Goncalo; Zemsky, Peter
We develop a formal model of the timing of resource development by competing firms. Our aim is to deepen and extend resource-level theorizing about sustainable competitive advantage. Our analysis formalizes the notion of barriers to imitation, particularly those based on time compression diseconomies where the faster a firm develops a resource, the greater the cost. Time compression diseconomies are derived from a micromodel of resource development with diminishing returns to effort. We use a continuous time model of the flows of development costs and market revenues, which allows us to integrate strategic and financial analyses of firm investment problems. We examine two dimensions of sustainability: whether the resources underlying a firm's competitive advantage are economically imitable and, if so, how long imitation takes. Surprisingly, we show that sustainable competitive advantage does not necessarily lead to superior performance. We find that imitators sometimes benefit from reductions in their absorptive capacity and that innovators should license either all or none of their knowledge. Despite recent criticisms, we reaffirm the usefulness of a resource level of analysis for strategy research, especially when the focus is on resources developed through internal projects with identifiable stopping times.</description><author>Pacheco-de-Almeida, Goncalo; Zemsky, Peter</author><pubDate>Sun, 01 Apr 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Residual income-based compensation plans for controlling investment decisions under sequential private information</title><link>http://www.example.com/articles/1</link><description>Pfeiffer, Thomas; Schneider, Georg
Earlier literature has pointed to the effectiveness of residual income-type measures based on particular accrual accounting rules such as the relative benefit allocation rule. These performance metrics have been shown to generate desirable managerial incentives when investment decisions are delegated. This paper further attests to the robustness of these measures by extending the result to a sequential adverse selection model with an inherent real option (an option to abandon). In other words, as long as the residual income measures are judiciously constructed, neither private information nor the requirement to selectively exercise an option derails their use in this setting.</description><author>Pfeiffer, Thomas; Schneider, Georg</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cooperative planning, uncertainty, and managerial control in concurrent design</title><link>http://www.example.com/articles/1</link><description>Mitchell, Victoria L.; Nault, Barrie R.
W e examine whether cooperative planning and uncertainty affect the magnitude of rework in concurrent engineering projects with upstream and downstream operations, and explore the impact of such rework on project delays. Using survey data from a sample of 120 business process (BP) redesign and related information technology (IT) development projects in healthcare and telecommunications, our results indicate that upstream (BP) rework and downstream (IT) rework is mediated and mitigated by cooperative planning through upstream/downstream strategy coupling and cross-functional involvement. In addition, uncertainty related to a lack of firm or industry experience with such projects increases the magnitude of upstream rework but not downstream rework or the amount of cooperative planning. After accounting for project scope, implementation horizon and whether delays are anticipated, we find that project delay is primarily influenced by the magnitude of downstream rework and downstream delay: the magnitude of both upstream and downstream rework significantly increases downstream delay, which significantly increases project delay. However, the magnitude of upstream rework does not directly affect project delay. These results suggest that project delay is under managerial control as cooperative planning is a managerial function that reduces downstream rework, while uncertainty from a lack of experience with the design affecting upstream rework is not directly under managerial control.</description><author>Mitchell, Victoria L.; Nault, Barrie R.</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and capacity rationing for rentals with uncertain durations</title><link>http://www.example.com/articles/1</link><description>Gans, Noah; Savin, Sergei
We consider a rental firm with two types of customers. Contract customers pay fixed, prenegotiated rental fees and expect a high quality of service. Walk-in customers have no contractual relations with the firm and are "shopping for price." Given multiple contract and walk-in classes, the rental firm has to decide when to offer service to contract customers and what fees to charge walk-in customers for service. We formulate this rental management problem as a problem in stochastic control and characterize optimal policies for managing contract and walk-in customers. We also consider static, myopic controls that are simpler to implement, and we analytically establish conditions under which these policies perform optimally. Complementary numerical tests provide a sense of the range of systems for which myopic policies are effective.</description><author>Gans, Noah; Savin, Sergei</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Obtaining fast service in a queueing system via performance-based allocation of demand</title><link>http://www.example.com/articles/1</link><description>Cachon, Gerard P.; Zhang, Fuqiang
Any buyer that depends on suppliers for the delivery of a service or the production of a make-to-order component should pay close attention to the suppliers' service or delivery lead times. This paper studies a queueing model in which two strategic servers choose their capacities/processing rates and faster service is costly. The buyer allocates demand to the servers based on their performance; the faster a server works, the more demand the server is allocated. The buyer's objective is to minimize the average lead time received from the servers. There are two important attributes to consider in the design of an allocation policy: the degree to which the allocation policy effectively utilizes the servers' capacities and the strength of the incentives the allocation policy provides for the servers to work quickly. Previous research suggests that there exists a trade-off between efficiency and incentives, i.e., in the choice between two allocation policies a buyer may prefer the less efficient one because it provides stronger incentives. We find considerable variation in the performance of allocation policies: Some intuitively reasonable policies generate essentially no competition among servers to work quickly, whereas others generate too much competition, thereby causing some servers to refuse to work with the buyer. Nevertheless, the trade-off between efficiency and incentives need not exist: It is possible to design an allocation policy that is efficient and also induces the servers to work quickly We conclude that performance-based allocation can be an effective procurement strategy for a buyer as long as the buyer explicitly accounts for the servers' strategic behavior.</description><author>Cachon, Gerard P.; Zhang, Fuqiang</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pushing quality improvement along supply chains</title><link>http://www.example.com/articles/1</link><description>Zhu, Kaijie; Zhang, Rachel Q.; Tsung, Fugee
In this paper, we consider a buyer who designs a product and owns the brand, yet outsources the production to a supplier. Both the buyer and the supplier incur quality-related costs, e.g., costs of customer goodwill and future market share loss by the buyer and warranty-related costs shared by both the buyer and the supplier whenever a nonconforming item is sold to a customer. Therefore, both parties have an incentive to invest in quality-improvement efforts. This paper explores the roles of different parties in a supply chain in quality improvement. We show that the buyer's involvement can have a significant impact on the profits of both parties and of the supply chain as a whole, and he cannot cede the responsibility of quality improvement to the supplier in many cases. We also investigate how quality-improvement decisions interact with operational decisions such as the buyer's order quantity and the supplier's production lot size.</description><author>Zhu, Kaijie; Zhang, Rachel Q.; Tsung, Fugee</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Method and psychological effects on learning behaviors and knowledge creation in quality improvement projects</title><link>http://www.example.com/articles/1</link><description>Choo, Adrian S.; Linderman, Kevin W.; Schroeder, Roger G.
This study investigates two mechanisms of knowledge creation-one that is method driven and the other that is psychologically driven. Results show that the two mechanisms have different effects on the learning behaviors and knowledge created in Six Sigma projects. The method mechanism directly influences learning behaviors, while the psychological mechanism directly affects knowledge creation. The effects of both mechanisms on knowledge creation are complementary, yet independent. Findings suggest that the value of adhering to a method may lie in modifying the learning behaviors that subsequently create knowledge. When a firm adopts a quality program such as Six Sigma, the method and the degree of its adherence can shape how the firm innovates and creates knowledge.</description><author>Choo, Adrian S.; Linderman, Kevin W.; Schroeder, Roger G.</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A spatiotemporal analysis of the global diffusion of ISO 9000 and ISO 14000 certification</title><link>http://www.example.com/articles/1</link><description>Albuquerque, Paulo; Bronnenberg, Bart J.; Corbett, Charles J.
We study the global diffusion of ISO 9000 and ISO 14000 certification using a network diffusion framework. We start by investigating the presence and nature of contagion effects by defining alternative cross-country networks and testing their relative strength. Second, we study how the rate of diffusion differs between the two standards and between early- and later-adopting countries. Third, we identify which countries had more influence on diffusion than others. Empirically, we build a diffusion model which includes several possible cross-country contagion effects and then use Bayesian methods for estimation and model selection. Using country by year data for 56 countries and nine years, we find that accounting for cross-country influences improves both the fit and the prediction accuracy of our models. However, the specific cross-country contagion mechanism is different across the two standards. Diffusion of ISO 9000 is driven primarily by geography and bilateral trade relations, whereas that of ISO 14000 is driven primarily by geography and cultural similarity. We also find that the diffusion rate of ISO standards is higher for later-adopting countries and for the later ISO 14000 standard. We discuss several implications of our findings for the global diffusion of management standards.</description><author>Albuquerque, Paulo; Bronnenberg, Bart J.; Corbett, Charles J.</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Resolving inconsistencies in utility measurement under risk: Tests of generalizations of expected utility</title><link>http://www.example.com/articles/1</link><description>Bleichrodt, Han; Abellan-Perpinan, Jose Maria; Pinto-Prades, Jose Luis; Mendez-Martinez, Ildefonso
This paper explores inconsistencies that occur in utility measurement under risk when expected utility theory is assumed and the contribution that prospect theory and some other generalizations of expected utility can make to the resolution of these inconsistencies. We used five methods to measure utilities under risk and found clear violations of expected utility. Of the theories studied, prospect theory was the most consistent with our data. The main improvement of prospect theory over expected utility was in comparisons between a riskless and a risky prospect (riskless-risk methods). We observed no improvement over expected utility in comparisons between two risky prospects (risk-risk methods). An explanation for the latter observation may be that there was less distortion in probability weighting in the interval [0.10, 0.20] than has commonly been observed.</description><author>Bleichrodt, Han; Abellan-Perpinan, Jose Maria; Pinto-Prades, Jose Luis; Mendez-Martinez, Ildefonso</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Proper conditioning for coherent VaR in portfolio management</title><link>http://www.example.com/articles/1</link><description>Garcia, Rene; Renault, Eric; Tsafack, Georges
Value at risk (VaR) is a central concept in risk management. As stressed by Artzner et al. (1999, Coherent measures of risk, Math. Finance 9(3) 203-228), VaR may not possess the subadditivity property required to be a coherent measure of risk. The key idea of this paper is that, when tail thickness is responsible for violation of subadditivity, eliciting proper conditioning information may restore VaR rationale for decentralized risk management. The argument is threefold. First, since individual traders are hired because they possess a richer information on their specific market segment than senior management, they just have to follow consistently the prudential targets set by senior management to ensure that decentralized VaR control will work in a coherent way. The intuition is that if one could build a fictitious conditioning information set merging all individual pieces of information, it would be rich enough to restore VaR subadditivity. Second, in this decentralization context, we show that if senior management has access ex post to the portfolio shares of the individual traders, it amounts to recovering some of their private information. These shares can be used to improve backtesting to check that the prudential targets have been enforced by the traders. Finally, we stress that tail thickness required to violate subadditivity even for small probabilities, remains an extreme situation because it corresponds to such poor conditioning information that expected loss appears to be infinite. We then conclude that lack of coherence of decentralized VaR management, that is VaR nonsubadditivity at the richest level of information, should be an exception rather than a rule.</description><author>Garcia, Rene; Renault, Eric; Tsafack, Georges</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Generalized Cox-Ross-Rubinstein binomial models</title><link>http://www.example.com/articles/1</link><description>Chung, San-Lin; Shih, Pai-Ta
This paper generalizes the seminal Cox-Ross-Rubinstein (CRR) binomial model by adding a stretch parameter. The generalized CRR (GCRR) model allows us to fine-tune (via the stretch parameter) the lattice structure so as to efficiently price a range of options, such as barrier options. Our analysis provides insights into the fine structure of convergence of the general binomial model to the Black-Scholes formula. We also discuss how to improve the rate of convergence or the oscillatory behavior of the GCRR model. The numerical results suggest that the GCRR models with various modifications are efficient for pricing a range of options.</description><author>Chung, San-Lin; Shih, Pai-Ta</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Determinants of country-level investment in information technology</title><link>http://www.example.com/articles/1</link><description>Shih, Eric; Kraemer, Kenneth L.; Dedrick, Jason
Investment in information technology (IT) is an important driver of economic growth and productivity in the United States and other developed countries, but as yet it is not shown to be a significant driver in developing countries. Previous research suggests that IT investment and complementary assets are insufficient for developing countries to realize economic benefits. This research note examines the factors that influence IT investment in developed and developing countries to determine how greater investment might be stimulated to achieve productivity gains. We use the flexible accelerator model of investment and find that it is a good predictor of country-level IT investment. We also extend the model to include country-level variables, and find a negative relationship between IT investment and interest rates, but positive and significant relationships between investment, openness to trade, and telecommunications infrastructure. When we include interaction effects between national income levels and country variables, we find that the impacts of interest rates, size of the financial sector, teledensity, and intellectual property rights are strongest in shaping IT investment for developed countries. In contrast, we find that the impact of openness to trade is greater for developing countries, as is the size of government and education levels.</description><author>Shih, Eric; Kraemer, Kenneth L.; Dedrick, Jason</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Asymmetric new product development alliances: Win-win or win-lose partnerships?</title><link>http://www.example.com/articles/1</link><description>Kalaignanam, Kartik; Shankar, Venkatesh; Varadarajan, Rajan
Interorganizational alliances are widely recognized as critical to product innovation, particularly in high-technology markets. Many new product development (NPD) alliances tend to be asymmetric, that is, they are formed between a larger firm and a smaller firm. As is the case with alliances in general, asymmetric alliances also typically result in changes in the shareholder values of the partner firms. Are the changes in shareholder values of the partner firms significant? Are asymmetric NPD alliances win-win or win-lose partnerships? Are the gains or losses symmetric for the larger and smaller partner firms? What factors drive the changes in shareholder values of the partner firms? These important questions remain largely unexplored as evidenced by the dearth of empirical research on the effect of asymmetric NPD alliances on shareholder value and on the apportionment of this value between the partner firms. We develop and empirically test a model of short-term changes in shareholder values of larger and smaller firms involved in NPD alliances, using the event study methodology on data covering 167 asymmetric alliances in the information technology and communication industries. In this model, we examine alliance, firm, and partner characteristics as potential determinants of the changes in shareholder values of the partner firms due to an NPD alliance announcement. Our model accounts for selection correction, potential cross-correlation across the residuals from the models of firm value changes for the larger and smaller firms, and unobserved heterogeneity. The results suggest that both the partners experience significant short-term financial gains, but there are considerable asymmetries between the larger and smaller firms with regard to the effects of alliance, partner, and firm characteristics on the gains of the partner firms. The results relating to alliance characteristics suggest that while a broad scope alliance enhances the financial gains for the larger firm, a scale R&amp;D alliance (relative to a link alliance) contributes positively to the financial gains for the smaller firm. With regard to partner characteristics, while partner alliance experience positively influences the financial gains for the larger firm, it has no significant effect on the financial returns for the smaller firm. Further, partner innovativeness is positively associated with the financial gains for the larger firm, but partner reputation is unrelated to the financial gains of the smaller firm. Regarding firm characteristics, the magnitude of the financial gains accruing from a firm's own alliance experience is considerably higher for the smaller firm than it is for the larger firm. We outline the implications of the research findings for future research and management practice.</description><author>Kalaignanam, Kartik; Shankar, Venkatesh; Varadarajan, Rajan</author><pubDate>Thu, 01 Mar 2007 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note - Sell first, fix later: Impact of patching on software quality</title><link>http://www.example.com/articles/1</link><description>Arora, A.; Caulkins, J. P.; Telang, R.
We present a model of fixing or patching a software problem after the product has been released in the market. Specifically, we model a software firm's trade-off in releasing a buggy product early and investments in fixing it later. just as the marginal cost of producing software can be effectively zero, so can the marginal cost of repairing multiple copies of defective software by issuing patches. We show that due to the fixed cost nature of investments in patching, a software vendor has incentives to release a buggier product early and patch it later in a larger market. Thus, a software monopolist releases a product with fewer bugs but later than what is socially optimal. We contrast this result with physical good markets where market size does not play any role in quality provision. We also show that for comparable costs, a software monopolist releases the product with more bugs but invests more in post-patching support later than the physical good monopolist.</description><author>Arora, A.; Caulkins, J. P.; Telang, R.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Environmental volatility, development decisions, and software volatility: A longitudinal analysis</title><link>http://www.example.com/articles/1</link><description>Barry, E. J.; Kemerer, C. E.; Slaughter, S. A.
Although product development research often focuses on activities prior to product launch, for long-lived, adaptable products like software, development can continue over the entire product life cycle. For managers of these products the challenges are to predict when and how much the products will change and to understand how their development decisions influence the timing and magnitude of future change activities. We develop a two-stage model that relates environmental volatility to product development decisions and product development decisions to software volatility. The model is evaluated using a data archive that captures changes over 20 years to a firm's environment, its managers' development choices, and its software products. In Stage 1 we find that higher environmental volatility leads to greater use of process technology and standard component designs but less team member rotation. Earlier development decisions strongly influence current development choices, especially for product design and process technology. In Stage 2 we find that increased use of standard component designs dampens future software volatility by decreasing the average rate and magnitude of change. Adding new team members increases product enhancements at a faster pace than more intense use of process technology but adds repairs at almost the same rate as enhancements.</description><author>Barry, E. J.; Kemerer, C. E.; Slaughter, S. A.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conjoint optimization: An exact branch-and-bound algorithm for the share-of-choice problem</title><link>http://www.example.com/articles/1</link><description>Camm, J. D.; Cochran, J. J.; Curry, D. J.; Kaman, S.
Conjoint analysis is a statistical technique used to elicit partworth utilities for product attributes from consumers to aid in the evaluation of market potential for new products. The objective of the share-of-choice problem (a common approach to new product design) is to find the design that maximizes the number of respondents for whom the new product's utility exceeds a specific hurdle (reservation utility). We present an exact branch-and-bound algorithm to solve the share-of-choice problem. Our empirical results, based on several large commercial data sets and simulated data from a controlled experiment, suggest that the approach is useful for finding provably optimal solutions to realistically sized problems, including cases where partworths contain estimation error.</description><author>Camm, J. D.; Cochran, J. J.; Curry, D. J.; Kaman, S.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Behavioral estimation of mathematical programming objective function coefficients</title><link>http://www.example.com/articles/1</link><description>Troutt, M. D.; Pang, W. K.; Hou, S. H.
We propose a parameter estimation method based on what we call the minimum decisional regret principle. We focus on mathematical programming models with objective functions that depend linearly on costs or other parameters. The approach is illustrated for cost estimation in production planning using linear programming models. The method uses past planning data to estimate costs that are otherwise difficult to estimate. We define a monetary measure of distance between observed plans and optimal ones, called decisional regret. The proposed estimation algorithm finds parameter values for which the associated optimal plans are as near as possible to the observed ones on average. Such techniques may be called behavioral estimation because they are based on the observed planning or decision-making behavior of managers or firms. Two numerical illustrations are given. A supporting hyperplane algorithm is used to solve the estimation model. A method is proposed for obtaining range estimates of the parameters when multiple alternative estimates exist. We also propose a new validation approach for this estimation principle, which we call the target-mode agreement criterion.</description><author>Troutt, M. D.; Pang, W. K.; Hou, S. H.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How does residual income affect investment? The role of prior performance measures</title><link>http://www.example.com/articles/1</link><description>Balachandran, S. V.
This paper examines whether "you get what you pay for" in firms that implement residual income (RI)-based compensation. Specifically, this paper explores differences in investment patterns of firms that implement RI-based compensation plans conditional on whether the firms switched from earnings or return on investment (ROI)-based compensation. I find that the pattern of investment for firms switching to RI from earnings-based compensation is opposite to that of firms switching from ROI-based compensation. Changes in investment within each individual subgroup yield weaker, mixed results. In addition, this paper documents that delivered RI increases in firms that implement RI. My paper contributes to the literature on the investment effects of RI by examining the relevance of a set of arguments that have been made in management accounting textbooks since 1965. These arguments are still found in current textbooks and are commonly taught to students in graduate level managerial accounting classes. The arguments help us to examine a natural experiment in which we can better specify the conditions under which RI use is expected to be associated with changes in investment.</description><author>Balachandran, S. V.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Improving preference assessment: Limiting the effect of context through pre-exposure to attribute levels</title><link>http://www.example.com/articles/1</link><description>Carlson, K. A.; Bond, S. D.
This paper introduces a technique for improving preference assessment by reducing the influence of context on preferential choices. We propose that a decision maker who is exposed to relevant attribute levels will form spontaneous valuations, which will then insulate the decision maker from the effects of context during subsequent preference assessment. Results from three studies supported this hypothesis. Pre-exposure to product attribute levels undermined the impact of attribute priming, decision framing, and asymmetric dominance on preferential choices. A fourth study demonstrated that similar results can be obtained by allowing decision makers to pregenerate lists of attribute levels on their own.</description><author>Carlson, K. A.; Bond, S. D.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effectiveness of simple decision heuristics: Forecasting commercial success for early-stage ventures</title><link>http://www.example.com/articles/1</link><description>Astebro, T.; Elhedhli, S.
We investigate the decision heuristics used by experts to forecast that early-stage ventures are subsequently commercialized. Experts evaluate 37 project characteristics and subjectively combine data on all cues by examining both critical flaws and positive factors to arrive at a forecast. A conjunctive model is used to describe their process, which sums "good" and "bad" cue counts separately. This model achieves a 91.8% forecasting accuracy of the experts' correct forecasts. The model correctly predicts 86.0% of outcomes in out-of-sample, out-of-time tests. Results indicate that reasonably simple decision heuristics can perform well in a natural and very difficult decision-making context.</description><author>Astebro, T.; Elhedhli, S.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Turning visitors into customers: A usability-centric perspective on purchase behavior in electronic channels</title><link>http://www.example.com/articles/1</link><description>Venkatesh, V.; Agarwal, R.
We develop a theoretical model for predicting purchase behavior in electronic channels. The model suggests that website use (i.e., technology use), a key indicator of the degree to which a site is "sticky," is a significant antecedent of purchase behavior. Furthermore, we relate the usability of a website to use behavior and purchase behavior. Specifically, individual characteristics and product type are argued to differentially influence the weights that customers place on five different categories of usability. The weighted ratings of the five categories together determine use behavior and purchase behavior, after controlling for purchase need, experience with similar sites, and previous purchase on the specific sites. The model was tested in a longitudinal field study among 757 customers who provided usability assessments for multiple websites from four different industries-i.e., airlines, online bookstores, automobile manufacturers, and car rental agencies. Six months later, 370 of these individuals provided responses to help understand the transition from visitor to customer, i.e., whether they actually transacted with a specific website. Results provided strong support for the model and yield important theoretical and practical implications.</description><author>Venkatesh, V.; Agarwal, R.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational learning curves for customer dissatisfaction: Heterogeneity across airlines</title><link>http://www.example.com/articles/1</link><description>Lapre, M. A.; Tsikriktsis, N.
In the extensive literature on learning curves, scholars have ignored outcome measures of organizational performance evaluated by customers. We explore whether customer dissatisfaction follows a learning-curve pattern. Do organizations learn to reduce customer dissatisfaction? Customer dissatisfaction occurs when customers' ex ante expectations about a product or service exceed ex post perceptions about the product or service. Because customers can increase expectations over time, customer dissatisfaction may not decline even when the product or service improves. Consequently, it is an open question whether customer dissatisfaction follows a learning-curve pattern. Drawing from the literatures on learning curves and organizational learning, we hypothesize that customer dissatisfaction follows a U-shaped function of operating experience (Hypothesis 1), that focused airlines learn faster to reduce customer dissatisfaction than full-service airlines (Hypothesis 2), and that organizational learning curves for customer dissatisfaction are heterogeneous across airlines (Hypothesis 3). We test these hypotheses with quarterly data covering 1987 to 1998 on consumer complaints against the 10 largest U.S. airlines. We find strong support for Hypothesis 1 and Hypothesis 3. Hypothesis 2 is not supported in the sense that the average focused airline did not learn faster than the average full-service airline. However, we do find that the best focused airline learns faster than the best full-service airline. We explore this result by extending a knowledge-based view of managing productivity learning curves in factories to complaint learning curves in airlines.</description><author>Lapre, M. A.; Tsikriktsis, N.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal component stocking policy for assemble-to-order systems with lead-time-dependent component and product pricing</title><link>http://www.example.com/articles/1</link><description>Hsu, V. N.; Lee, C. Y.; So, K. C.
Short delivery time and the efficient management of component inventories are two crucial elements that determine the competitiveness of many contract assembly manufacturers, especially in the electronics industry. In this paper, we develop and analyze an optimization model to determine the optimal stocking quantities for components of an assemble-to-order product in an environment where demand is uncertain and the price for the final product and the costs of components depend on their delivery lead times. We provide an efficient solution procedure to solve the problem in which the manufacturer must deliver the full order quantity possibly in multiple shipments. We further extend our model to the situation where the manufacturer has the option of not delivering the full quantity but instead takes the penalty for a delivery shortage. We derive some analytical results that illustrate how different model parameters affect the optimal solution and provide useful insights for managing components in the assemble-to-order environment.</description><author>Hsu, V. N.; Lee, C. Y.; So, K. C.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The optimizer's curse: Skepticism and postdecision surprise in decision analysis</title><link>http://www.example.com/articles/1</link><description>Smith, J. E.; Winkler, R. L.
Decision analysis produces measures of value such as expected net present values or expected utilities and ranks alternatives by these value estimates. Other optimization-based processes operate in a similar manner. With uncertainty and limited resources, an analysis is never perfect, so these value estimates are subject to error. We show that if we take these value estimates at face value and select accordingly, we should expect the value of the chosen alternative to be less than its estimate, even if the value estimates are unbiased. Thus, when comparing actual outcomes to value estimates, we should expect to be disappointed on average, not because of any inherent bias in the estimates themselves, but because of the optimization-based selection process. We call this phenomenon the optimizer's curse and argue that it is not well understood or appreciated in the decision analysis and management science communities. This curse may be a factor in creating skepticism in decision makers who review the results of an analysis. In this paper, we study the optimizer's curse and show that the resulting expected disappointment may be substantial. We then propose the use of Bayesian methods to adjust value estimates. These Bayesian methods can be viewed as disciplined skepticism and provide a method for avoiding this postdecision disappointment.</description><author>Smith, J. E.; Winkler, R. L.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Behavioral causes of the bullwhip effect and the observed value of inventory information</title><link>http://www.example.com/articles/1</link><description>Croson, R.; Donohue, K.
The tendency of orders to increase in variability as one moves up a supply chain is commonly known as the bullwhip effect. We study this phenomenon from a behavioral perspective in the context of a simple, serial, supply chain subject to information lags and stochastic demand. We conduct two experiments on two different sets of participants. In the first, we find the bullwhip effect still exists when normal operational causes (e.g., batching, price fluctuations, demand estimation, etc.) are removed. The persistence of the bullwhip effect is explained to some extent by evidence that decision makers consistently underweight the supply line when making order decisions. In the second experiment, we find that the bullwhip, and the underlying tendency of underweighting, remains when information on inventory levels is shared. However, we observe that inventory information helps somewhat to alleviate the bullwhip effect by helping upstream chain members better anticipate and prepare for fluctuations in inventory needs downstream. These experimental results support the theoretically suggested notion that upstream chain members stand to gain the most from information-sharing initiatives.</description><author>Croson, R.; Donohue, K.</author><pubDate>Wed, 01 Mar 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>