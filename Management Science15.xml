<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Management Science15</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:07:42 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Commercializing knowledge: University science, knowledge capture, and firm performance in biotechnology</title><link>http://www.example.com/articles/1</link><description>Zucker, L. G.; Darby, M. R.; Armstrong, J. S.
Commercializing knowledge involves transfer from discovering scientists to those who will develop it commercially. New codes and formulae describing discoveries develop slowly-with little incentive if value is low and many competing opportunities if high. Hence new knowledge remains naturally excludable and appropriable. Team production allows more knowledge capture of tacit, complex discoveries by firm scientists. A robust indicator of a firm's tacit knowledge capture (and strong predictor of its success) is the number of research articles written jointly by firm scientists and discovering, "star" scientists, nearly all working at top universities. An operationally attractive generalization of our star measure-collaborative research articles between firm scientists and top research university scientists-replicates the impact on firm success. In panel analyses, publications by firm scientists with stars and/or top 112 university scientists increase the number and citation rate for firm patents. Further, star articles increase these rates significantly more than other top 112 university scientists' articles. Cross-sectional analyses of products and employment show a similar pattern of positive effects on firms' success of collaborations with stars or top university scientists, but estimates of differential effects are nonrobust due to multicollinearity. Venture capital funding has significant, usually positive effects on firm success.</description><author>Zucker, L. G.; Darby, M. R.; Armstrong, J. S.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selling university technology: Patterns from MIT</title><link>http://www.example.com/articles/1</link><description>Shane, S.
any research universities engage in efforts to license inventions developed by university-affiliated inventors. However, no systematic explanation of the conditions under which university inventions will be licensed or commercialized has been provided. Drawing on transaction cost economics, I provide a conceptual framework to explain which university inventions are most likely to be licensed, commercialized, and generate royalties, and who will undertake that commercialization. I test this framework on data on the 1,397 patents assigned to the Massachusetts Institute of Technology during the 1980-1996 period. The results show that (1) university inventions are more likely to be licensed when patents are effective; (2) when patents are effective, university technology is generally licensed to noninventors; (3) when patents are effective, licensing back to inventors increases the likelihood of license termination and reduces the likelihood of invention commercialization; and (4) the effectiveness of patents increases royalties earned for inventions licensed to noninventors. The implications of these findings for innovation management and strategy, entrepreneurship, and university technology commercialization are discussed.</description><author>Shane, S.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Equity and the technology transfer strategies of American research universities</title><link>http://www.example.com/articles/1</link><description>Feldman, M.; Feller, I.; Bercovitz, J.; Burton, R.
American universities are experimenting with new mechanisms for promoting the commercialization of academic research and generating revenue from university intellectual property. This paper discusses mechanisms available to universities in managing the commercialization of intellectual property, considering equity as a technology transfer mechanism that offers advantages for both generating revenue and aligning the interests of universities, industry and faculty. Employing data from a national survey of Carnegie I and Carnegie II institutions, we document the recent rise in university equity holdings. We present and estimate a model that considers the university's use of equity to be a function of behavioral factors related to the university's prior experiences with licensing, success relative to other institutions, and the organization of the technology transfer office, as well as structural characteristics related to university type.</description><author>Feldman, M.; Feller, I.; Bercovitz, J.; Burton, R.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Who is selling the Ivory Tower? Sources of growth in university licensing</title><link>http://www.example.com/articles/1</link><description>Thursby, J. G.; Thursby, M. C.
Historically, commercial use of university research has been viewed in terms of spillovers. Recently, there has been a dramatic increase in technology transfer through licensing as universities attempt to appropriate the returns from faculty research. This change has prompted concerns regarding the source of this growth-specifically, whether it suggests a change in the nature of university research. We develop an intermediate input model to examine the extent to which the growth in licensing is due to the productivity of observable inputs or driven by a change in the propensity of faculty and administrators to engage in commercializing university research. We model licensing as a three-stage process, each involving multiple inputs. Nonparametric programming techniques are applied to survey data from 64 universities to calculate total factor productivity (TFP) growth in each stage. To examine the sources of TFP growth, the productivity analysis is augmented by survey evidence from businesses who license-in university inventions. Results suggest that increased licensing is due primarily to an increased willingness of faculty and administrators to license and increased business reliance on external R&amp;D rather than a shift in faculty research.</description><author>Thursby, J. G.; Thursby, M. C.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning to patent: Institutional experience, learning, and the characteristics of US university patents after the Bayh-Dole Act, 1981-1992</title><link>http://www.example.com/articles/1</link><description>Mowery, D. C.; Sampat, B. N.; Ziedonis, A. A.
nan</description><author>Mowery, D. C.; Sampat, B. N.; Ziedonis, A. A.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How do university inventions get into practice?</title><link>http://www.example.com/articles/1</link><description>Colyvas, J.; Crow, M.; Gelijns, A.; Mazzoleni, R.; Nelson, R. R.; Rosenberg, N.; Sampat, B. N.
nan</description><author>Colyvas, J.; Crow, M.; Gelijns, A.; Mazzoleni, R.; Nelson, R. R.; Rosenberg, N.; Sampat, B. N.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Putting patents in context: Exploring knowledge transfer from MIT</title><link>http://www.example.com/articles/1</link><description>Agrawal, A.; Henderson, R.
In this paper we explore the degree to which patents are representative of the magnitude, direction, and impact of the knowledge spilling out of the university by focusing on the Massachusetts Institute of Technology (MIT), and in particular, on the Departments of Mechanical and Electrical Engineering. Drawing on both qualitative and quantitative data, we show that patenting is a minority activity: a majority of the faculty in our sample never patent, and publication rates far outstrip patenting rates. Most faculty members estimate that patents account for less than 10% of the knowledge that transfers from their labs. Our results also suggest that in two important ways patenting is not representative of the patterns of knowledge generation and transfer from MIT: patent volume does not predict publication volume, and those firms that cite MIT papers are in general not the same firms as those that cite MIT patents. However, patent volume is positively correlated with paper citations, suggesting that patent counts may be reasonable measures of research impact. We close by speculating on the implications of our results for the difficult but important question of whether, in this setting, patenting acts as a substitute or a complement to the process of fundamental research.</description><author>Agrawal, A.; Henderson, R.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comparison of US and European university-industry relations in the life sciences</title><link>http://www.example.com/articles/1</link><description>Owen-Smith, J.; Riccaboni, M.; Pammolli, F.; Powell, W. W.
We draw on diverse data sets to compare the institutional organization of upstream life science research across the United States and Europe. Understanding cross-national differences in the organization of innovative labor in the life sciences requires attention to the structure and evolution of biomedical networks involving public research organizations (universities, government laboratories, nonprofit research institutes, and research hospitals), science-based biotechnology firms, and multinational pharmaceutical corporations. We use network visualization methods and correspondence analyses to demonstrate that innovative research in biomedicine has its origins in regional clusters in the United States and in European nations. But the scientific and organizational composition of these regions varies in consequential ways. In the United States, public research organizations and small firms conduct R&amp;D across multiple therapeutic areas and stages of the development process. Ties within and across these regions link small firms and diverse public institutions, contributing to the development of a robust national network. In contrast, the European story is one of regional specialization with a less diverse group of public research organizations working in a smaller number of therapeutic areas. European institutes develop local connections to small firms working on similar scientific problems, while cross-national linkages of European regional clusters typically involve large pharmaceutical corporations. We show that the roles of large and small firms differ in the United States and Europe, arguing that the greater heterogeneity of the U.S. system is based on much closer integration of basic science and clinical development.</description><author>Owen-Smith, J.; Riccaboni, M.; Pammolli, F.; Powell, W. W.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on university entrepreneurship and technology transfer</title><link>http://www.example.com/articles/1</link><description>Mowery, D. C.; Shane, S.
nan</description><author>Mowery, D. C.; Shane, S.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Links and impacts: The influence of public research on industrial R&amp;D</title><link>http://www.example.com/articles/1</link><description>Cohen, W. M.; Nelson, R. R.; Walsh, J. P.
In this paper, we use data from the Carnegie Mellon Survey on industrial R&amp;D to evaluate for the U.S. manufacturing sector the influence of "public" (i.e., university and government R&amp;D lab) research on industrial R&amp;D, the role that public research plays in industrial R&amp;D, and the pathways through which that effect is exercised. We find that public research is critical to industrial R&amp;D in a small number of industries and importantly affects industrial R&amp;D across much of the manufacturing sector. Contrary to the notion that university research largely generates new ideas for industrial R&amp;D projects, the survey responses demonstrate that public research both suggests new R&amp;D projects and contributes to the completion of existing projects in roughly equal measure overall. The results also indicate that the key channels through which university research impacts industrial R&amp;D include published papers and reports, public conferences and meetings, informal information exchange, and consulting. We also find that, after controlling for industry, the influence of public research on industrial R&amp;D is disproportionately greater for larger firms as well as start-ups.</description><author>Cohen, W. M.; Nelson, R. R.; Walsh, J. P.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customer loyalty and supplier quality competition</title><link>http://www.example.com/articles/1</link><description>Gans, N.
We develop a model of customer choice in response to random variation in quality The choice model yields closed-form expressions which reflect the effect of competing suppliers' service quality on the long-run fraction of purchases a customer makes at the various competitors. We then use the expressions as the basis of simple normative models for suppliers seeking to maximize their long-run average profits. The results provide insight into the effect of switching behavior on the service levels offered by competing suppliers.</description><author>Gans, N.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incumbent entry into new market niches: The role of experience and managerial choice in the creation of dynamic capabilities</title><link>http://www.example.com/articles/1</link><description>King, A. A.; Tucci, C. L.
Increasingly, technological innovation creates markets for new products and services. To survive, firms must respond to these new markets. How do firms develop the capabilities necessary to succeed in such changing conditions? Some suggest that experience with previous entry builds such capabilities. Others suggest that capabilities arise from experience producing and selling to existing markets. The role of managers is also debated. Some argue that experience with existing markets causes managers to miss entry opportunities. Others argue that managers enter new markets when their firm possesses the experience needed to compete effectively. In this paper, we explore these issues by investigating entry patterns in the disk-drive industry. We investigate the effect of experience in existing markets and experience with previous market entry. We find that experience in previous markets increased the probability that a firm would enter a new market. We show that this experience had greater value if the firm entered the new market. We infer that managers chose to enter these markets to obtain this increase in value.</description><author>King, A. A.; Tucci, C. L.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Generalized column generation for linear programming</title><link>http://www.example.com/articles/1</link><description>Oguz, O.
Column generation is a well-known and widely practiced technique for solving linear programs with too many variables or constraints to include in the initial formulation explicitly. Instead, the required column information is generated at each iteration of the simplex algorithm. This paper shows that, even if the number of variables is low enough for explicit inclusion in the model with the available technology, it may still be more efficient to resort to column generation for some class of problems.</description><author>Oguz, O.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A mean-variance analysis of self-financing portfolios</title><link>http://www.example.com/articles/1</link><description>Korkie, B.; Turtle, H. J.
T his paper develops the analytics and geometry of the investment opportunity set (IOS) and the test statistics for self-financing portfolios. A self-financing portfolio is a set of long and short investments such that the sum of their investment weights, or net investment, is zero. This contrasts with a standard portfolio that has investment weights summing to one. Examples of self-financing portfolios are hedges, overlays, arbitrage portfolios, swaps, and long/short portfolios. A standard portfolio plus the IOS of self-financing portfolios form a restricted IOS hyperbola with restricted efficient set constants that differ from the usual constants. The restrictions affect statistical tests of portfolio efficiency, which are developed for the self-financing restrictions. As an application, we consider the self-financing portfolios formed by Fama and French (1992, 1993, 1995), based on market capitalization and value. In contrast to Fama and French (1992, 1993, 1995), we find that their restricted IOS is significantly different from the unrestricted IOS with the implication that the Fama-French tests are misspecified.</description><author>Korkie, B.; Turtle, H. J.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A multi-echelon inventory system with information exchange</title><link>http://www.example.com/articles/1</link><description>Moinzadeh, K.
In this paper, we consider a supply-chain model consisting of a single product, one supplier, and multiple retailers. Demand at the retailers is random, but stationary. Each retailer places her orders to the supplier according to the well-known (Q, R) policy. We assume that the supplier has online information about the demand, as well as inventory activities of the product at each retailer, and uses this information when making order/replenishment decisions. We first propose a replenishment policy for the supplier, which incorporates information about the inventory position of the retailers. Then, we provide an exact analysis of the operating measures of such systems. Assuming the inventory/replenishment decisions are made centrally for the system, we compare the performance of our model with those that do not use information in their decision making, namely, systems that use installation stock policies via a numerical experiment. Based on our numerical results, we identify the parameter settings under which information sharing is most beneficial.</description><author>Moinzadeh, K.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Simultaneous capacity and production management of short-life-cycle, produce-to-stock goods under stochastic demand</title><link>http://www.example.com/articles/1</link><description>Angelus, A.; Porteus, E. L.
This paper derives the optimal simultaneous capacity and production plan for a short-life-cycle, produce-to-stock good under stochastic demand. Capacity can be reduced as well as added, at exogenously set unit prices. In both cases studied, with and without carry-over of unsold units, a target interval policy is optimal: There is a (usually different) target interval for each period such that capacity should be changed as little as possible to bring the level available into that interval. Our contribution in the case of no carry-over, is a detailed characterization of the target intervals, assuming demands increase stochastically at the beginning of the life cycle and decrease thereafter. In the case of carry-over, we establish the general result and show that capacity and inventory are economic substitutes: The target intervals decrease in the initial stock level and the optimal unconstrained base stock level decreases in the capacity level. In both cases, optimal service rates are not necessarily constant over time. A numerical example illustrates the results.</description><author>Angelus, A.; Porteus, E. L.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Complementary product integration by high-technology new ventures: The role of initial technology strategy</title><link>http://www.example.com/articles/1</link><description>Nambisan, S.
In this paper, we investigate the relationship between complementary product integration and the initial technology strategy of a high-technology new venture. With customers placing considerable emphasis on cross-product integration, the success of a new venture is dependent as much on its ability to integrate its product with relevant complementary products as on the core product functionality itself. We identify three types of complementary product integration: value-added internal, add-on module, and data interface. We argue that the adoption of proactive initial technology strategy critically determines the ability of a high-technology new venture to rapidly and efficiently integrate its product with new and emerging complementary products. More specifically, we offer hypotheses that relate initial design and development strategies to the number and the type of complementary product integrations achieved by a new venture in the initial years. The hypotheses are tested using data from a set of U.S.-based software new ventures. The results support our arguments and imply the need for high-technology new ventures to adopt an explicit complementary product focus during initial product design.</description><author>Nambisan, S.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Network ties, reputation, and the financing of new ventures</title><link>http://www.example.com/articles/1</link><description>Shane, S.; Cable, D.
Explaining how entrepreneurs overcome information asymmetry between themselves and potential investors to obtain financing is an important issue for entrepreneurship research. Our premise is that economic explanations for venture finance, which do not consider how social ties influence this process, are undersocialized and incomplete. However, we also argue that organization theoretic arguments, which draw on the concept of social obligation, are oversocialized. Drawing on the organizational theory literature, and in-depth fieldwork with 50 high-technology ventures, we examine the effects of direct and indirect ties between entrepreneurs and 202 seed-stage investors on venture finance decisions. We show that these ties influence the selection of ventures to fund through a process of information transfer.</description><author>Shane, S.; Cable, D.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Editorial objectives - Design and operations management</title><link>http://www.example.com/articles/1</link><description>Hopp, W. J.; Lovejoy, W. S.; Ulrich, K.
In recent years the field of operations management has expanded to embrace a larger set of problems than that which historically dominated our academic literature. There is now an increased presence of upper management issues (for example issues of design, investment, and coordination) at both the demand and supply end of our research activities. In recognition of this, the Manufacturing, Distribution and Service Operations department of Management Science will be renamed the Design and Operations Management department, with a revised editorial mission (below) focused on promoting and addressing these higher level issues. The intent is not to discount the importance of further progress on more tactical managerial issues, but rather to reduce the overlap among the editorial missions of the top INFORMS journals and at the same time promote new work in exciting and industrially relevant areas. We invite a careful reading of our editorial mission and enthusiastically encourage all appropriate contributions.</description><author>Hopp, W. J.; Lovejoy, W. S.; Ulrich, K.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Structuring the new product development pipeline</title><link>http://www.example.com/articles/1</link><description>Ding, M.; Eliashberg, J.
dIn many new product development (NPD) situations, the development process is characterized by uncertainty, and no single development approach will necessarily lead to a successful product. To increase the likelihood of having at least one successful product, multiple approaches may be simultaneously funded at the various NPD stages. The managerial challenge is to construct ex ante an appropriate NPD pipeline by choosing the right number of approaches to be funded at each stage. This so-called pipeline problem is also present in, among others, advertising copy selection and new products test markets problems. We describe here a normative model for structuring pipelines for such situations. The optimal structure of the pipeline is driven by the cost of the development approach, its probability of survival, and the expected profitability. We illustrate the workability and implications of the model by applying it to some real-world scenarios in the pharmaceutical industry, and by comparing its normative pipeline recommendations against actual pipelines. Our results suggest that, for the cases we studied, firms tend to use narrower pipelines for their new drug development than they should, and thereby they underspend on research and development. We also present general qualitative insights for one- and two-stage NPD optimal pipeline structures.</description><author>Ding, M.; Eliashberg, J.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Job rotation as a learning mechanism (vol 47, pg 1361, 2001)</title><link>http://www.example.com/articles/1</link><description>Ortega, J.
nan</description><author>Ortega, J.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Editorial objectives - Business strategy</title><link>http://www.example.com/articles/1</link><description>Levinthal, D.
nan</description><author>Levinthal, D.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technological opportunities and new firm creation</title><link>http://www.example.com/articles/1</link><description>Shane, S.
Research on the creation of new high-technology companies has typically focused either on industry-level factors such as market structure and technology regime or on individual-level factors such as the work experience of entrepreneurs. This study complements these approaches by examining the effect of technological opportunities on firm formation. Ln particular, the study shows that the probability that an invention will be commercialized through firm formation is influenced by its importance, radicalness, and patent scope.</description><author>Shane, S.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploiting a cost advantage and coping with a cost disadvantage</title><link>http://www.example.com/articles/1</link><description>Besanko, D.; Dranove, D.; Shanley, M.
This paper provides an empirical investigation of how firms with cost advantages (cost disadvantages) exploit (cope with) their advantages (disadvantages) through their pricing behavior. Guided by microeconomic theory and insights from the industrial organization literature, we develop testable implications about the effect of industry structure and firmspecific characteristics on the pass-through elasticity: The rate at which changes in a firm's cost relative to competitors translates into changes in the firm's price relative to competitors. We test these implications using data from the PIMS Competitive Strategy database. The results indicate that a firm's pass-through elasticity systematically depends on whether the firm operates in a commodity or noncommodity industry, the firm's capacity utilization, and its cost and quality position in its industry. The pass-through elasticity is also shown to depend in a nonlinear way on market concentration.</description><author>Besanko, D.; Dranove, D.; Shanley, M.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Systemic risk in financial systems</title><link>http://www.example.com/articles/1</link><description>Eisenberg, L.; Noe, T. H.
We consider default by firms that are part of a single clearing mechanism. The obligations of all firms within the system are determined simultaneously in a fashion consistent with the priority of debt claims and the limited liability of equity We first show, via a fixed-point argument, that there always exists a "clearing payment vector" that clears the obligations of the members of the clearing system; under mild regularity conditions, this clearing vector is unique. Next, we develop an algorithm that both clears the financial system in a computationally efficient fashion and provides information on the systemic risk faced by the individual system firms. Finally, we produce qualitative comparative statics for financial systems. These comparative statics imply that, in contrast to single-firm results, even unsystematic, nondissipative shocks to the system will lower the total value of the system and may lower the value of the equity of some of the individual system firms.</description><author>Eisenberg, L.; Noe, T. H.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Entrepreneurs, contracts, and the failure of young firms</title><link>http://www.example.com/articles/1</link><description>Azoulay, P.; Shane, S.
Although economic theory has emphasized that moral hazard and hold-up problems influence the design of contracts, very little is known about the process by which explicit contracts are established and the effect of contractual arrangements on firm performance. This paper attempts to demonstrate that firms are selected for survival on the basis of contracting efficiency. Based on a statistical analysis of 170 new franchise contracts and interviews with the founders of 16 of these new franchise systems, we show that new franchise chains that adopt exclusive territories are more Likely to survive over time than chains that do not. Moreover, successful and failed entrepreneurs possess different information about how to design contracts. These entrepreneurs undertake "contractual experiments" based on the information they possess. Those whose experiments prove to be more consistent with economic theory are rewarded for their superior information with survival.</description><author>Azoulay, P.; Shane, S.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The long-run stock price performance of firms with effective TQM programs</title><link>http://www.example.com/articles/1</link><description>Hendricks, K. B.; Singhal, V. R.
This paper documents the long-run stock price performance of firms with effective Total Quality Management (TQM) programs. The winning of quality awards is used as a proxy for effective TQM implementation. We compare stock price performance of award winners against various matched control groups for a five-year implementation period and a five-year postimplementation period. During the implementation period there is no difference in the stock price performance, but during the postimplementation period award winners significantly outperform firms in the various control groups. Depending on the control group used, the mean outperformance ranges from 38% to 46%. Our results clearly indicate that effective implementation of TQM principles and philosophies leads to significant wealth creation. Furthermore, our results should alleviate many of the concerns regarding the value of quality award systems. Overall, these systems are valuable in terms of recognizing TQM firms and promoting awareness of TQM.</description><author>Hendricks, K. B.; Singhal, V. R.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>March Madness and the office pool</title><link>http://www.example.com/articles/1</link><description>Kaplan, E. H.; Garstka, S.
March brings March Madness, the annual conclusion to the U.S. men's college basketball season with two single elimination basketball tournaments showcasing the best college teams in the country. Almost as mad is the plethora of office pools across the country where the object is to pick a priori as many game winners as possible in the tournament. More generally, the object in an office pool is to maximize total pool points, where different points are awarded for different correct winning predictions. We consider the structure of single elimination tournaments, and show how to efficiently calculate the mean and the variance of the number of correctly predicted wins (or more generally the total points earned in an office pool) for a given slate of predicted winners. We apply these results to both random and Markov tournaments. We then show how to determine optimal office pool predictions that maximize the expected number of points earned in the pool. Considering various Markov probability models for predicting game winners based on regular season performance, professional sports rankings, and Las Vegas betting odds, we compare our predictions with what actually happened in past NCAA and NIT tournaments. These models perform similarly, achieving overall prediction accuracies of about 58%, but do not surpass the simple strategy of picking the seeds when the goal is to pick as many game winners as possible. For a more sophisticated point structure, however, our models do outperform the strategy of picking the seeds.</description><author>Kaplan, E. H.; Garstka, S.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing discrete barrier and hindsight options with the tridiagonal probability algorithm</title><link>http://www.example.com/articles/1</link><description>Tse, W. M.; Li, L. K.; Ng, K. W.
This paper develops an algorithm to calculate the Brownian multivariate normal probability subject to any preset error tolerance criteria. The algorithm is founded upon the computational simplicity of the tridiagonal structure of the inverse of the Brownian correlation matrix. Compared with existing pricing technologies without the "barrier too close" problem, our calculation method can produce a more accurate and efficient analytic evaluation of barrier options monitored at discrete instants with well- or ill-behaved barrier levels, or discrete hindsight options, for a reasonably large number of monitorings.</description><author>Tse, W. M.; Li, L. K.; Ng, K. W.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>One size does not fit all projects: Exploring classical contingency domains</title><link>http://www.example.com/articles/1</link><description>Shenhar, A. J.
Not many authors have attempted to classify projects according to any specific scheme, and those who have tried rarely offered extensive empirical evidence. From a theoretical perspective, a traditional distinction between radical and incremental innovation has often been used in the Literature of innovation, and has created the basis for many classical contingency studies. Similar concepts, however, did not become standard in the literature of projects, and it seems that theory development in project management is still in its early years. As a result, most project management literature still assumes that all projects are fundamentally similar and that "one size fits all." The purpose of this exploratory research is to show how different types of projects are managed in different ways, and to explore the domain of traditional contingency theory in the more modern world of projects. This two-step research is using a combination of qualitative and quantitative methods and two data sets to suggest a conceptual, two-dimensional construct model for the classification of technical projects and for the investigation of project contingencies. Within this framework, projects are classified into four levels of technological uncertainty, and into three levels of system complexity, according to a hierarchy of systems and subsystems. The study provides two types of implications, For project leadership it shows why and how management should adapt a more project-specific style. For theory development, it offers a collection of insights that seem relevant to the world of projects as temporary organizations, but are, at times, different, from classical structural contingency theory paradigms in enduring organizations. While still exploratory in nature, this study attempts to suggest new inroads to the future study of modern project domains.</description><author>Shenhar, A. J.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A periodic review inventory system with emergency replenishments</title><link>http://www.example.com/articles/1</link><description>Tagaras, G.; Vlachos, D.
This paper proposes and analyzes a periodic review inventory system with two replenishment modes. Regular orders are placed periodically following a base stock policy on inventory position, and arrive at the stocking location after a deterministic lead time. The location also has the option of placing emergency orders, characterized by a shorter lead time but higher acquisition cost, in case of imminent stockouts. Thus, at some appropriate time in the replenishment cycle, the necessity and size of an emergency order is determined according to a base stock policy on net stock. The timing of the emergency order is such that this order arrives and can be used to satisfy the demand in the time period just before the arrival of a regular order, when the likelihood of a stockout is highest. An approximate cost model is developed which can easily be optimized with respect to the order-up-to parameters. This model is used as the basis for a heuristic algorithm, which leads to solutions that are very close to the exact optimal solutions determined through simulation. It is shown that the proposed system offers substantial cost savings relative to a system without the emergency replenishment option.</description><author>Tagaras, G.; Vlachos, D.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The dynamic value of hierarchy</title><link>http://www.example.com/articles/1</link><description>Knott, A. M.
This study develops a dual-routines view of the dynamic value of hierarchy, and tests it against the implicit null hypothesis that hierarchy merely provides static advantages over markets. The view holds that hierarchical managers perform two roles that create value for firms in perpetuity-an administrative role of enforcing operational routine, and an entrepreneurial role of executing a metaroutine that continually revises operational routine to keep pace with changes in the environment. The test consists of a natural experiment comparing the behavior and performance of establishments that leave a franchise, "lose their hierarchical managers," with those that remain. I find support for the view. in the absence of the franchisor, establishment behavior drifts from the operational routine, and establishments fail to adopt innovation. Both responses lead to significant decay in performance. Thus hierarchical managers are necessary to actively enforce routine, even after the routine been assimilated, and to introduce innovation, even in this unique setting of perfect incentives.</description><author>Knott, A. M.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Comparisons with a standard in simulation experiments</title><link>http://www.example.com/articles/1</link><description>Nelson, B. L.; Goldsman, D.
We consider the problem of comparing a finite number of stochastic systems with respect to a single system (designated as the "standard") via simulation experiments. The comparison is based on expected performance, and the goal is to determine if any system has larger expected performance than the standard, and if so to identify the best of the alternatives. In this paper we provide two-stage experiment design and analysis procedures to solve the problem for a variety of scenarios, including those in which we encounter unequal variances across systems, as well as those in which we use the variance reduction technique of common random numbers and it is appropriate to do so. The emphasis is added because in some cases common random numbers can be counterproductive when performing comparisons with a standard. We also provide methods for estimating the critical constants required by our procedures, present a portion of an extensive empirical study, and demonstrate one of the procedures via a numerical example.</description><author>Nelson, B. L.; Goldsman, D.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing inventory with multiple products, lags in delivery, resource constraints, and lost sales: A mathematical programming approach</title><link>http://www.example.com/articles/1</link><description>Downs, B.; Metters, R.; Semple, J.
This paper develops an order-up-to S inventory model that is designed to handle multiple items, resource constraints, lags in delivery, and lost sales without sacrificing computational simplicity. Mild conditions are shown to ensure that the expected average holding cost and the expected average shortage cost are separable convex functions of the order-up-to levels. We develop nonparametric estimates of these costs and use them in conjunction with Linear programming to produce what is termed the "LP policy." The LP policy has two major advantages over traditional methods: first, it can be computed in complex environments such as the one described above; and second, it does not require an explicit functional form of demand, something that is difficult to specify accurately in practice. In two numerical experiments designed so that optimal policies could be computed, the LP policy fared well, differing from the optimal profit by an average of 2.20% and 1.84%, respectively. These results compare quite favorably with the errors incurred in traditional methods when a correctly specified distribution uses estimated parameters. Our findings support the effectiveness of this mathematical programming technique for approximating complex, real-world inventory control problems.</description><author>Downs, B.; Metters, R.; Semple, J.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sequencing JIT mixed-model assembly lines under station-load and part-usage constraints</title><link>http://www.example.com/articles/1</link><description>Drexl, A.; Kimms, A.
This paper deals with two most important problems, from both practical and theoretical standpoints, arising in sequencing mixed-model assembly lines. Such lines have become core components of modern repetitive manufacturing, and just-in-time (JIT) manufacturing in particular. One problem is to keep the usage rate of all parts fed into the final assembly as constant as possible (the "level-scheduling problem"), while the other is to keep the Line's workstation loads as constant as possible (the "car-sequencing problem"). In this paper the combined problem is formulated as a single-integer programming model. The LP-relaxation of this model is solved by column-generation techniques. The results of an experimental evaluation show that the lower bounds are tight.</description><author>Drexl, A.; Kimms, A.</author><pubDate>Thu, 01 Mar 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Design for the environment: A quality-based model for green product development</title><link>http://www.example.com/articles/1</link><description>Chen, C. L.
Green product development, which addresses environmental issues through product design and innovation as opposed to the traditional end-of-pipe-control approach, is receiving significant attention from customers, industries, and governments around the world. in this paper we develop a quality-based model for analyzing the strategic and policy issues concerning the development of products with conflicting traditional and environmental attributes. On the demand side of the problem, we use the framework of conjoint analysis to structure the preferences of the ordinary and green customers. On the supply side, we apply the theories in optimal product design and market segmentation to analyze the producer's strategic decisions regarding the number of products introduced and their prices and qualities. On the policy side, we evaluate the effects of environmental standards on the economic and environmental consequences of green product development. By jointly considering the interactions among the customers' preferences, the producer's product strategies, and the environmental standards imposed by governments, we present some interesting findings that can be used to manage and regulate the development of green products. Two major findings show that green product development and stricter environmental standards might not necessarily benefit the environment.</description><author>Chen, C. L.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Scheduling and reliable lead-time quotation for orders with availability intervals and lead-time sensitive revenues</title><link>http://www.example.com/articles/1</link><description>Keskinocak, P.; Ravi, R.; Tayur, S.
Motivated by applications in the manufacturing and service industries, we consider two models for coordinating scheduling with lead-time quotation: a basic model. with a single customer type, and an enhanced model where an additional second customer type expects immediate service or production. In both models, revenues Obtained from the customers are sensitive to the lead time, there is a threshold of lead time above which the customer does not place an order, and the quoted lead times are 100% reliable. These models are related to well-known scheduling problems, which have been studied in both offline and online settings. We introduce the immediate quotation case and study it with the (traditional) online version. We provide complexity results for the offline case, and perform competitive analysis for the online cases. A natural question of bridging the gap between the online and quotation models leads us to the delayed quotation model, which we study briefly. The analysis of these models provides useful qualitative insights as well.</description><author>Keskinocak, P.; Ravi, R.; Tayur, S.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient timing of communication in multiperiod agencies</title><link>http://www.example.com/articles/1</link><description>Christensen, P. O.; Feltham, G. A.
This paper examines communication in a two-period principal/agent model in which the agent receives a private signal about the second outcome before the first outcome is realized. No communication is compared with communication at three possible dates: before the first outcome (early), at the first outcome/consumption date (normal), and between the initial consumption date and the second outcome (delayed). Delayed communication is shown to have no value if the agent's information is perfect, but can have value if it is imperfect. Early and normal communication can be used to "smooth" compensation across periods and, hence, generally have incremental value over delayed communication if the agent cannot borrow or save. However, the "smoothing" benefits disappear if he can borrow and save. Early and normal communication are equivalent if the agent has domain-additive exponential preferences and the private signal is uninformative about the first outcome. If the private signal is informative about the first outcome, the incremental value of early compared with normal communication attains its maximum for "medium" informativeness. A unifying example is used throughout.</description><author>Christensen, P. O.; Feltham, G. A.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Generating scenario trees for multistage decision problems</title><link>http://www.example.com/articles/1</link><description>Hoyland, K.; Wallace, S. W.
In models of decision making under uncertainty we often are faced with the problem of representing the uncertainties in a form suitable for quantitative models. If the uncertainties are expressed in terms of multivariate continuous distributions, or a discrete distribution with far too many outcomes, we normally face two possibilities: either creating a decision model with internal sampling, or trying to find a simple discrete approximation of the given distribution that serves as input to the model. This paper presents a method based on nonlinear programming that can be used to generate a limited number of discrete outcomes that satisfy specified statistical properties. Users are free to specify any statistical properties they find relevant, and the method can handle inconsistencies in the specifications. The basic idea is to minimize some measure of distance between the statistical properties of the generated outcomes and the specified properties. We illustrate the method by single- and multiple-period problems. The results are encouraging in that a limited number of generated outcomes indeed have statistical properties that are close to or equal to the specifications. We discuss how to verify that the relevant statistical properties are captured in these specifications, and argue that what are the relevant properties, will be problem dependent.</description><author>Hoyland, K.; Wallace, S. W.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Project management under risk: Using the real options approach to evaluate flexibility in R&amp;D</title><link>http://www.example.com/articles/1</link><description>Huchzermeier, A.; Loch, C. H.
Managerial flexibility has value in the context of uncertain R&amp;D projects, as management can repeatedly gather information about uncertain project and market characteristics and, based on this information, change its course of action. This value is now well accepted and referred to as "real option value." We introduce, in addition to the familiar real option of abandonment, the option of corrective action that management can take during the project. The intuition from options pricing theory is that higher uncertainty in project pay offs increases the real option value of managerial decision flexibility. However, R&amp;D managers face uncertainty not only in payoffs, but also from many other sources. We identify five example types of R&amp;D uncertainty, in market payoffs, project budgets, product performance, market requirements, and project schedules. How do they influence the value from managerial flexibility? We find that if uncertainty is resolved or costs/revenues occur after all decisions have been made, more variability may "smear out" contingencies and thus reduce the value of flexibility In addition, variability may reduce the probability of flexibility ever being exercised, which also reduces its value. This result runs counter to established option pricing theory intuition and contributes to a better risk management in R&amp;D projects. Our model builds intuition for R&amp;D managers as to when it is and when it is not worthwhile to delay commitments-for example, by postponing a design freeze, thus maintaining flexibility in R&amp;D projects.</description><author>Huchzermeier, A.; Loch, C. H.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An extreme-value model of concept testing</title><link>http://www.example.com/articles/1</link><description>Dahan, E.; Mendelson, H.
We model concept testing in new product development as a search for the most profitable solution to a design problem. When allocating resources, developers must balance the cost of testing multiple designs against the potential profits that may result. We propose extreme-value theory as a mathematical abstraction of the concept-testing process. We investigate the trade-off between the benefits and costs of parallel concept testing and derive closed-form solutions for the case of profits that follow extreme-value distributions. We analyze the roles of the scale and tail-shape parameters of the profit distribution as well as the cost of testing in determining the optimal number of tests and total budget for the concept phase of NPD. Using an example, we illustrate how to estimate and interpret the scale and tail-shape parameters. We find that the impact of declining concept-testing costs on expected profits, the number of concepts tested, and total spending depend on the scale/cost ratio and tail-shape parameter of the profit distribution.</description><author>Dahan, E.; Mendelson, H.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Recombinant uncertainty in technological search</title><link>http://www.example.com/articles/1</link><description>Fleming, L.
While the Course of technological change is widely accepted to be highly uncertain and unpredictable, little work has identified or studied the ultimate sources and causes of that uncertainty. This paper proposes that purely technological uncertainty derives from inventors' search processes with unfamiliar components and component combinations. Experimentation with new components and new combinations leads to Less useful inventions on average, but it also implies an increase in the variability that can result in both failure and breakthrough. Negative binomial count and dispersion models with patent citation data demonstrate that new combinations are indeed more variable. Ln contrast to predictions, however, the reuse of components has a nonmonotonic and eventually positive effect on variability.</description><author>Fleming, L.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Developing products on "Internet time": The anatomy of a flexible development process</title><link>http://www.example.com/articles/1</link><description>MacCormack, A.; Verganti, R.; Iansiti, M.
Uncertain and dynamic environments present fundamental challenges to managers of the new product development process. Between successive product generations, significant evolutions can occur in both the customer needs a product must address and the technologies it employs to satisfy these needs. Even within a single development project, firms must respond to new information, or risk developing a product that is obsolete the day it is launched. This paper examines the characteristics of an effective development process in one such environment-the Internet software industry. Using data on 29 completed development projects we show that in this industry, constructs that support a more flexible development process are associated with better-performing projects. This flexible process is characterized by the ability to generate and respond to new information for a longer proportion of a development cycle. The constructs that support such a process are greater investments in architectural design, earlier feedback on product performance from the market, and the use of a development team with greater amounts of "generational" experience. Our results suggest that investments in architectural design play a dual role in a flexible process: First through the need to select an architecture that maximizes product performance and, second, through the need to select an architecture that facilitates development process flexibility. We provide examples from our fieldwork to support this view.</description><author>MacCormack, A.; Verganti, R.; Iansiti, M.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrating operations and marketing perspectives of product innovation: The influence of organizational process factors and capabilities on development performance</title><link>http://www.example.com/articles/1</link><description>Tatikonda, M. V.; Montoya-Weiss, M. M.
This paper adopts a multidisciplinary view of innovation by integrating operations and marketing perspectives of product development. The conceptual framework builds on the resource-based view of the firm and organizational information-processing theory to characterize relationships among organizational process factors, product development capabilities, critical uncertainties, and operational/market performance in product development projects. Data from a cross-sectional sample of 120 completed development projects for assembled goods is analyzed via a two-stage hierarchical moderated regression approach. The findings show that: (1) the organizational process factors studied are associated with achievement of operational outcome targets for product quality, unit cost, and time-to-market; (2) achievement of operational outcomes aids the achievement of market outcomes, in turn suggesting that development capabilities are indeed valuable firm resources; and (3) these relationships are robust under conditions of technological, market, and environmental uncertainty. This article provides practical insight into how product development projects can be better managed for operational and market success. Additionally, this article sets a theoretical and empirical basis for future research on the influence of organizational process factors and capabilities on diverse product-innovation outcomes.</description><author>Tatikonda, M. V.; Montoya-Weiss, M. M.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance measurement and design in supply chains</title><link>http://www.example.com/articles/1</link><description>Baiman, S.; Fischer, P. E.; Rajan, M. V.
This paper examines the relationship between product architecture, supply-chain performance metrics, and supply-chain efficiency. We model the contracting relationship between a supplier and a buyer. The supplier is privately informed about the outcome of his design/production investment. The buyer both appraises the supplier's component and does further processing/component production of his own. If the final product produced by the buyer;exhibits decoupling and no function sharing with respect to the components (termed separable architecture), the first-best outcome is attained if both internal and external failures are contractible. When only one type of failure can be contracted on, we derive conditions under which contracting on internal failure is superior to contracting on external failure, and vice versa. If the buyer's final product has a nonseparable architecture with respect to the components, first-best cannot be achieved even if both internal and external failures are contractible. The value of contracting on internal failure alone is unaffected by the architecture design, while that of external failure declines relative to the separable setting; the net result is often to make the former the uniformly dominant performance metric. Our results highlight the interaction between the performance metrics used for contracting within the supply chain, the architecture of the product produced by the supply chain, and the incentive efficiency of the chain.</description><author>Baiman, S.; Fischer, P. E.; Rajan, M. V.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sourcing by design: Product complexity and the supply chain</title><link>http://www.example.com/articles/1</link><description>Novak, S.; Eppinger, S. D.
This paper focuses on the connection between product complexity and vertical integration I using original empirical evidence from the auto industry. A rich literature has addressed the choice between internal production and external sourcing of components in the auto industry. More recent literature has developed the concept of product architecture as another choice variable that may be one of the important contributors to product complexity. In this paper, we connect these two important decisions and study them jointly. We use the property rights approach to argue that complexity in product design and vertical integration of production are complements: that in-house production is more attractive when product complexity is high, as firms seek to capture the; benefits of their investment in the skills needed to coordinate development of complex designs. We test this hypothesis with a simultaneous equations model applied to data from the luxury-performance segment of the auto industry. We find a significant and positive relationship between product complexity and vertical integration. This has implications for optimal incentive structures within firms, as well as for interpreting firm performance.</description><author>Novak, S.; Eppinger, S. D.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The idea itself and the circumstances of its emergence as predictors of new product success.</title><link>http://www.example.com/articles/1</link><description>Goldenberg, J.; Lehmann, D. R.; Mazursky, D.
In view of the distressingly low rate of success in new product introduction, it is important to identify predictive guidelines early in:the new product development process so that better choices can be made and unnecessary costs avoided. In this paper, we propose a framework for early analysis based on the success potential embodied in the product-idea itself and the circumstances of its emergence. Based on two studies reporting actual introductions, we identified several determinants (such as how the ideas originated, their specific configurations, and the level of technology required for their implementation) that significantly distinguish successful from unsuccessful new products in the marketplace. We suggest that these factors, together with already known factors of success/failure, may aid in the estimation of the potential of a concept early in its development.</description><author>Goldenberg, J.; Lehmann, D. R.; Mazursky, D.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Appropriateness and impact of platform-based product development</title><link>http://www.example.com/articles/1</link><description>Krishnan, V.; Gupta, S.
In their quest to manage the complexity of offering greater product variety, firms in many industries are considering platform-based product development. Product platforms, which are component and subsystem assets shared across a product-family, enable a firm to better leverage investments in product design and development. While the platform approach offers a number of benefits, it also imposes certain additional costs that have not received adequate research attention. In this paper, we use an industrial example both to illustrate some of the costs and benefits of platform-based product development and to motivate the development of a mathematical model. The model is formulated to better understand the appropriateness of product platforms and their impact on product-planning decisions. Our results indicate that platforms are not appropriate for extreme levels of market diversity or high levels of nonplatform scale economies. Also, a firm's product positioning and introduction sequence decisions made during the product-planning phase are significantly impacted by the presence of platforms. Specifically, a platform increases the separation among products and offers a multitude of product introduction strategies. We translate our model findings into a managerial framework.</description><author>Krishnan, V.; Gupta, S.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product differentiation and commonality in design: Balancing revenue and cost drivers</title><link>http://www.example.com/articles/1</link><description>Desai, P.; Kekre, S.; Radhakrishnan, S.; Srinivasan, K.
Product design decisions substantially affect the cost and revenue drivers. A design configuration with commonality can lower manufacturing cost. However, such a design may hinder the ability to extract price premiums through product differentiation. We explicitly investigate the marketing-manufacturing trade-off and derive analytical implications for three possible design configurations: unique, premium-common, and basic-common. Our model considers two distinct segments of consumers. Some of the implications of our analysis are not readily apparent. For example, when the high-quality component is made common, the average quality of the products offered to the two segments increases. One may infer that with higher average quality, higher prices or higher total revenues might ensue. However, this may not be the case, as detailed in the paper. Finally, our analysis provides a useful framework to develop an index that can rank order components in terms of their attractiveness for commonality.</description><author>Desai, P.; Kekre, S.; Radhakrishnan, S.; Srinivasan, K.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sequential testing in product development</title><link>http://www.example.com/articles/1</link><description>Thomke, S.; Bell, D. E.
A fundamental problem in managing product development is the optimal timing, frequency, and fidelity of sequential testing activities that are carried out to evaluate novel product concerts and designs. In this paper, we develop a mathematical model that treats testing as an activity that generates information about technical and customer-need related problems An analysis of the model results in several important findings. First, optimal testing strategies need to balance the tension between several variables, including the increasing cost of redesign, the cost of a test as function of fidelity and the correlation between sequential tests. Second, a Simple form of our model results in an EOQ-like result: The optimal number of tests (called the Economic Testing Frequency or ETF) is the square root of the ratio of avoidable cost and the cost of a test. Third, the relationship between sequential tests can have an impact on optimal testing strategies. If sequential tests are increasing refinements of one another, managers should invest their budgets in a few high-fidelity tests, whereas if the tests identify problems independently of one another it may be more effective if developers carry out a higher number of lower-fidelity tests. Using examples, the implications for managerial practice are discussed and suggestions for further research undertakings are provided.</description><author>Thomke, S.; Bell, D. E.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling a phone center: Analysis of a multichannel, multiresource processor shared loss system</title><link>http://www.example.com/articles/1</link><description>Aksin, O. Z.; Harker, P. T.
This payer presents a model for the study of operations at an inbound call center. The call center is modeled as a multiclass processor shared loss system, where the interacting effects of human, telecommunication, and information technology resources are explicitly incorporated. Product form solutions and approximations for this type of system are provided along with expressions for performance measures like blocking and reneging. Some structural properties of system throughput are analyzed in an effort to pave the way for future optimization studies dealing with the design and management of phone centers.</description><author>Aksin, O. Z.; Harker, P. T.</author><pubDate>Thu, 01 Feb 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A cross-functional approach to evaluating multiple line extensions for assembled products</title><link>http://www.example.com/articles/1</link><description>Ramdas, K.; Sawhney, M. S.
Assembled product manufacturers often introduce line extensions that share components with existing products, or among themselves, resulting in cost interactions among products because of shared costs, and revenue interactions because of cannibalization. We present a cross-functional approach to evaluating multiple line extensions that simultaneously considers revenue implications of component sharing at the product level and cost implications at the component level. We develop a source-of-volume model and a measurement procedure to decompose the life-cycle sales volume from a line extension into sales from cannibalization, competitive draw, and demand expansion. We develop an activity-based costing procedure for estimating the life-cycle costs of line extensions that share components. We develop an optimization model that uses these revenue and cost estimates to identify a subset of line extensions that maximizes incremental profits. We implement our approach at a quartz wristwatch manufacturer. Results suggest that our approach would have improved profits for the firm by over 5%, while actually launching fewer line extensions. We also find that the drivers of cannibalization are counterintuitive. In simulation studies, our approach outperforms three managerial heuristics. We demonstrate that this approach is most valuable when cannibalization dominates competitive draw as a source of volume, and discuss its relative merits under low and high parts-sharing.</description><author>Ramdas, K.; Sawhney, M. S.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product development decisions: A review of the literature</title><link>http://www.example.com/articles/1</link><description>Krishnan, V.; Ulrich, K. T.
his paper is a review of research in product development, which we define as the transformation of a market opportunity into a product available for sale. Our review is broad, encompassing work in the academic fields of marketing, operations management, and engineering design. The value of this breadth is in conveying the shape of the entire-research landscape. We focus on product development projects within a single firm. We also devote our attention to the development of physical goods, although much of the work we describe applies to products of all kinds. We look inside the "black box" of product development at the fundamental decisions that are made by intention or default. In doing so,:we adopt the perspective of product development as a deliberate business process involving hundreds of decisions, many of which can be usefully supported by knowledge and tools. We contrast this approach to prior reviews of the literature, which tend to examine the importance of environmental and contextual variables, such as market growth rate, the competitive environment, or the level of top-management support.</description><author>Krishnan, V.; Ulrich, K. T.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on design and development</title><link>http://www.example.com/articles/1</link><description>Ulrich, K. T.
nan</description><author>Ulrich, K. T.</author><pubDate>Mon, 01 Jan 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A note on stock replenishment and shipment scheduling for vendor-managed inventory systems</title><link>http://www.example.com/articles/1</link><description>Axsater, S.
In a recent paper, Cetinkaya and Lee (2000) model integrated inventory control and shipment scheduling in connection with vendor-managed inventory (VMI). The model is optimized by an approximate technique. This note provides a simple procedure for exact optimization, and illustrates that the errors when using the suggested approximate technique may be very large for certain types of problems. We also suggest a new approximation and an adjustment that can be used to improve both the original and new heuristic.</description><author>Axsater, S.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An optimization model for the simultaneous operational flight and pilot scheduling problem</title><link>http://www.example.com/articles/1</link><description>Stojkovic, M.; Soumis, F.
This paper describes and solves the operational pilot scheduling problem for one day of operations. The problem consists in simultaneously modifying, as necessary, the existing flight departure schedules and planned individual work days (duties) while keeping planned aircraft itineraries unchanged. It requires the covering of all flights from one day of operations with available pilots while minimizing changes in both the flight schedule and the next day's planned duties. The newly constructed personalized duties must not exceed the maximum duty duration. Flight precedence constraints, coming from existing fixed aircraft itineraries, must be respected as well. The problem is mathematically formulated as an integer nonlinear multicommodity network flow model with time windows and additional constraints. To solve the problem, a Dantzig-Wolfe decomposition combined with a branch-and-bound method has been used. The master problem comprises the flight-covering constraints and a new set of flight precedence constraints. Subproblems consisting of time-constrained shortest-path problems with linear time costs are solved by a specialized dynamic-programming algorithm. The proposed optimization approach has been tested on several input data sets. All of them have been successfully solved in very short computational time.</description><author>Stojkovic, M.; Soumis, F.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Warranty signalling and reputation</title><link>http://www.example.com/articles/1</link><description>Balachander, S.
In this paper, we present a signalling-based explanation for the empirical phenomenon that a longer warranty may be offered by a product with lower quality. Our explanation hinges on differences in consumer knowledge about reliability of established and newer products. In a product market where a new entrant competes with an established product, we show that signalling behavior leads to an outcome where the less reliable product may carry the longer warranty.</description><author>Balachander, S.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis of a forecasting-production-inventory system with stationary demand</title><link>http://www.example.com/articles/1</link><description>Toktay, L. B.; Wein, L. M.
We consider a production stage that produces a single item in a make-to-stock manner. Demand for finished goods is stationary. In each time period, an updated vector of demand forecasts over the forecast horizon becomes available for use in production decisions. We model the sequence of forecast update vectors using the Martingale model of forecast evolution developed by Graves et al. (1986, 1998) and Heath and Jackson (1994). The production stage is modeled as a single-server, discrete-time, continuous-state queue. We focus on a modified base-stock policy incorporating forecast information and use an approximate analysis rooted in heavy traffic theory and random walk theory to obtain a closed-form expression for the (forecast-corrected) base-stock level that minimizes the expected steady-state inventory holding and backorder costs. This expression, which is shown to be accurate under certain conditions in a simulation study, sheds some light on the interrelationships among safety stock, stochastic correlated demand, inaccurate forecasts, and random and capacitated production in forecasting-production-inventory systems.</description><author>Toktay, L. B.; Wein, L. M.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Gale-Shapley stable marriage problem revisited: Strategic issues and applications</title><link>http://www.example.com/articles/1</link><description>Teo, C. P.; Sethuraman, J.; Tan, W. P.
We study strategic issues in the Gale-Shapley stable marriage model. Iri the first part of the paper, we derive the optimal cheating strategy and show that it is not always possible for a woman to recover her women-optimal stable partner from the men-optimal stable matching mechanism when she can only cheat by permuting her preferences. In fact, we show, using simulation, that the chances that a woman can benefit from cheating are slim. In the second part of the paper, we consider a two-sided matching market found in Singapore. We study the matching mechanism used by the Ministry of Education (MOE) in the placement of primary six students in secondary schools, and discuss why the current method has limited success in accommodating the preferences of the students, and the specific needs of the schools (in terms of the "mix" of admitted students). Using insights from the first part of the paper, we show that stable matching mechanisms are more appropriate in this matching market and explain why the strategic behavior of the students need not be a major concern.</description><author>Teo, C. P.; Sethuraman, J.; Tan, W. P.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Representing and solving decision problems with limited information</title><link>http://www.example.com/articles/1</link><description>Lauritzen, S. L.; Nilsson, D.
We introduce the notion of LImited Memory Influence Diagram (LIMID) to describe multistage decision problems in which the traditional assumption of no forgetting is relaxed. This can be relevant in situations with multiple decision makers or when decisions must be prescribed under memory constraints, such as in partially observed Markov decision processes (POMDPs). We give an algorithm for improving any given strategy by local computation of single policy updates and investigate conditions for the resulting strategy to be optimal.</description><author>Lauritzen, S. L.; Nilsson, D.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Channel coordination under price protection, midlife returns, and end-of-life returns in dynamic markets</title><link>http://www.example.com/articles/1</link><description>Taylor, T. A.
This paper examines three channel policies that are used in declining price environments: Price protection (P) is a mechanism under which the manufacturer pays the retailer a credit applying to the retailer's unsold inventory when the wholesale price drops during the life cycle; midlife returns (M) allow the retailer to return units partway through the life cycle at some rebate; and end-of-life returns (E) allow the retailer to return unsold units at the end of the life cycle. Under declining retail prices, if the wholesale prices and the return rebates are set properly, then EM (i.e., midlife and end-of-life returns) achieves channel coordination. However, such a policy may not be implementable because it may require the manufacturer to be worse off as a result of coordination. If P is used in addition to EM and the terms are set properly, then PEM guarantees both coordination and a win-win outcome. If the retail price is constant over time, then EM is sufficient to guarantee both coordination and a win-win outcome.</description><author>Taylor, T. A.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An adaptive, distribution-free algorithm for the newsvendor problem with censored demands, with applications to inventory and distribution</title><link>http://www.example.com/articles/1</link><description>Godfrey, G. A.; Powell, W. B.
We consider the problem of optimizing inventories for problems where the demand distribution is unknown, and where it does not necessarily follow a standard form such as the normal. We address problems where the process of deciding the inventory, and then realizing the demand, occurs repeatedly. The only information we use is the amount of inventory left over. Rather than attempting to estimate the demand distribution, we directly estimate the value function using a technique called the Concave, Adaptive Value Estimation (CAVE) algorithm. CAVE constructs a sequence of concave piecewise linear approximations using sample gradients of the recourse function at different points in the domain. Since it is a sampling-based method, CAVE does not require knowledge of the underlying sample distribution. The result is a nonlinear approximation that is more responsive than traditional linear stochastic quasi-gradient methods and more flexible than analytical techniques that require distribution information. In addition, we demonstrate near-optimal behavior of the CAVE approximation in experiments involving two different types of stochastic programs the newsvendor stochastic inventory problem and two-stage distribution problems.</description><author>Godfrey, G. A.; Powell, W. B.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exchange rates and the choice of ownership structure of production facilities</title><link>http://www.example.com/articles/1</link><description>Kouvelis, P.; Axarloglou, K.; Sinha, V.
The aim of this research is to study the effects of real exchange rates on the long-term ownership strategies of production facilities of firms entering foreign markets. Among the strategies considered are exporting (EXP), joint ventures with local partners (JV), and wholly owned production facilities (WOS) in the foreign country. Our research takes a first step in modeling the influence of exchange rates on the choice and dynamic adjustment of such strategies. The insights obtained from our modeling analysis are then translated into testable hypotheses and empirically verified with the use of firm level data from U.S. multinational corporations (both at the firm and a more aggregate level). An insightful result of our model is the identification of a hysteresis phenomenon that characterizes switching behavior between strategies in the presence of switchover cost. The magnitude of the hysteresis band, which is a measure of the inertia associated with keeping the current ownership structure, is affected by a multiplicity of factors such as exchange rate volatility and market power of the entering firm. Analytical and numerical results on the effects of such factors on the hysteresis band are provided. The four testable hypotheses generated from our modeling analysis are rigorously tested with the use of a multinomial logit model on data obtained from the Harvard Multinational Enterprise database, and a data set maintained by the Bureau of Economic Analysis, the U.S. Department of Commerce. The empirical results strongly support our insights that relatively depreciated real exchange rates (i.e., weak home currency) favor (a) the JV over the WOS and (b) EXP mode over the WOS or JV Finally, we summarize our results into useful guidelines for global production managers.</description><author>Kouvelis, P.; Axarloglou, K.; Sinha, V.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contingent labor contracting under demand and supply uncertainty</title><link>http://www.example.com/articles/1</link><description>Milner, J. M.; Pinker, E. J.
Firms increasingly use contingent labor to flexibly respond to demand in many environments. Labor supply agencies are growing to fill this need. As a result, firms and agencies are engaging in long-term contracts for labor supply. We develop mathematical models of the interaction between firms and labor supply agencies when demand and supply are uncertain. We consider two models of labor supply uncertainty, termed productivity and availability uncertainty, and study how each affects the nature of the contracts formed. These models reflect two major roles played by the labor supply agency. In the case of productivity uncertainty we find that it is possible to construct a contract that coordinates the firm and agency hiring in an optimal way. In contrast, we show that in environments characterized by availability uncertainty, optimal contracts are not possible. However, there is a large range of contract parameters for which both parties would benefit from a contract. We analyze these and discuss the trade-offs that should be considered in contract negotiation.</description><author>Milner, J. M.; Pinker, E. J.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of incentive schemes and organizational arrangements on the new product development process</title><link>http://www.example.com/articles/1</link><description>Natter, M.; Mild, A.; Feurstein, M.; Dorffner, G.; Taudes, A.
This paper proposes a new model for studying the new product development process in an artificial environment. We show how connectionist models can be used to simulate the adaptive nature of agents' learning exhibiting similar behavior as practically experienced learning curves. We study the impact of incentive schemes (local, hybrid, and global) on the new product development process for different types of organizations. Sequential organizational structures are compared to two different types of team-based organizations, incorporating methods of quality function deployment such as the house of quality. A key finding of this analysis is that the firms' organizational structure and agents' incentive system significantly interact. We show that the house of quality is less affected by the incentive scheme than firms using a trial and error approach. This becomes an important factor for new product success when the agents' performance measures are conflicting.</description><author>Natter, M.; Mild, A.; Feurstein, M.; Dorffner, G.; Taudes, A.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal investment in knowledge within a firm using a market mechanism</title><link>http://www.example.com/articles/1</link><description>Ba, S. L.; Stallaert, J.; Whinston, A. B.
There has been an extensive research literature on auctions, but recent developments in technology have resulted in new interest in auction mechanisms as a practical way of allocating resources. This paper presents a new double-auction mechanism to handle resource allocation for public goods when complementarity exists. The mechanism is placed in the context of an organization's internal knowledge investment. Knowledge goods have two distinct Characteristics. First, knowledge within an organization can be considered a public good, so it is subject to the free-rider problem. Second, knowledge is interrelated and interdependent; that is, there is complementarity among knowledge components. The value of knowledge often derives from a bundle of knowledge components, rather than from its individual pieces. These two characteristics present a serious challenge to allocating organizational resources for knowledge goods. We introduce an internal market in which knowledge providers offer knowledge projects and knowledge consumers place bids to acquire them. The mechanism is a Groves-Clarke-type double auction that allows bundled knowledge goods to be traded so as to recognize complementarities between knowledge projects. The market mechanism we propose is incentive compatible; i.e., it induces people to reveal their true valuation. In addition, it allows trades of knowledge bundles to determine which knowledge components are most valuable from the organization's viewpoint. Under mild assumptions, the mechanism is a computationally tractable solution to operating a market of bundled public goods. We further show how "imputed prices" can be calculated for subsets of knowledge components and prove that a market mechanism that does not allow bundle orders or does not address the free-rider problem yields a systematic underinvestment in knowledge.</description><author>Ba, S. L.; Stallaert, J.; Whinston, A. B.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The relationship between initial quality perceptions and maintenance behavior: The case of the automotive industry</title><link>http://www.example.com/articles/1</link><description>Conlon, E.; Devaraj, S.; Matta, K. F.
W e examine the relationship between quality, represented by consumer ratings, and quality-related activities by the customer, represented by maintenance activities in the automotive industry. Based on several converging theoretical perspectives, we present and test a model relating vehicle initial quality ratings to consumers' routine maintenance. Three types of data were collected for the study: (1) vehicle service records at a local dealership, (2) primary data from a survey of vehicle owners, and (3) Consumer Reports data on quality ratings and initial purchase prices. The results of a structural equation analysis of the proposed model indicate a significant link between quality and customers' quality behavior. This link has important strategic implications for both automotive manufacturers and distributors, particularly as "leasing" becomes more prevalent in the industry.</description><author>Conlon, E.; Devaraj, S.; Matta, K. F.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technology regimes and new firm formation</title><link>http://www.example.com/articles/1</link><description>Shane, S.
At least since Schumpeter (1934 and 1942), researchers have been interested in identifying the dimensions of technology regimes that facilitate new firm formation as a mode of technology exploitation. Using data on 1,397 patents assigned to the Massachusetts Institute of Technology during the 1980-1996. period, I show that four hypothesized dimensions of the technology regime-the age of the technical field, the tendency of the market toward segmentation, the effectiveness of patents, and the importance of complementary assets in marketing and distribution-influence the likelihood that new technology will be exploited through firm formation.</description><author>Shane, S.</author><pubDate>Sat, 01 Sep 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implementable mechanisms to coordinate horizontal alliances (vol 47, pg 787, 2001)</title><link>http://www.example.com/articles/1</link><description>Nault, B. R.; Tyagi, R. K.
nan</description><author>Nault, B. R.; Tyagi, R. K.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Server assignment policies for maximizing the steady-state throughput of finite queueing systems</title><link>http://www.example.com/articles/1</link><description>Andradottir, S.; Ayhan, H.; Down, D. G.
For a system of finite queues, we study how servers should be assigned dynamically to station's in order to obtain optimal (or near-optimal) long-run average throughput. We assume that travel times between different service facilities are negligible, that each server can work on only one job at a time, and that several servers can work together on one job. We show that when the service rates depend only on either the server or the station (and not both), then all nonidling server assignment policies are optimal. Moreover, for a Markovian system with two stations in tandem and two servers, we show that the optimal policy assigns one server to each station unless that station is blocked or starved (in which case the server helps at the other station), and we specify the criterion used for assigning servers to stations. Finally, we propose a simple server assignment policy for tandem systems in which the number of stations equals the number of servers, and we present numerical results that show that our policy appears to yield near-optimal throughput under general conditions.</description><author>Andradottir, S.; Ayhan, H.; Down, D. G.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The latest arrival hub location problem</title><link>http://www.example.com/articles/1</link><description>Kara, B. Y.; Tansel, B. C.
The traditionally studied hub location problems in the literature pay attention to flight times but not to transient times spent at hubs for unloading, loading, and sorting operations. The transient times may constitute a significant portion of the total delivery time for cargo delivery systems. We focus on the minimization of the arrival time of the last arrived item in cargo delivery systems and develop a model that correctly computes the arrival times by taking into account both the flight times and the transient times. Nonlinear and linear integer formulations are given and computational results are provided. The effects of delays on the system performance are analyzed.</description><author>Kara, B. Y.; Tansel, B. C.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Improving discrete model representations via symmetry considerations</title><link>http://www.example.com/articles/1</link><description>Sherali, H. D.; Smith, J. C.
In this paper, we focus on a useful modeling concept that is frequently ignored while formulating discrete optimization problems. Very often, there exists a natural symmetry inherent in the problem itself that, if propagated to the model, can hopelessly mire a branch-and-bound solver by burdening it to explore and eliminate such alternative symmetric solutions. We discuss three applications where such a symmetry arises: a telecommunications network design problem, a noise pollution problem, and a machine procurement and operation problem. For each case, we identify the indistinguishable objects in the model that create the problem symmetry and show how imposing certain decision hierarchies within the model significantly enhances its solvability, while using a popular modern-day commercial branch-and-cut software (CPLEX 6.5).</description><author>Sherali, H. D.; Smith, J. C.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A dynamic lot-sizing model with demand time windows</title><link>http://www.example.com/articles/1</link><description>Lee, C. Y.; Cetinkaya, S.; Wagelmans, A. P. M.
One of the basic assumptions of the classical dynamic lot-sizing model is that the aggregate demand of a given period must be satisfied in that period. Under this assumption, if backlogging is not allowed, then the demand of a given period cannot be delivered earlier or later than the period. If backlogging is allowed, the demand of a given period cannot be delivered earlier than the period, but it can be delivered later at the expense of a backordering cost. Like most mathematical models, the classical dynamic lot-sizing model is a simplified paraphrase of what might actually happen in real life. In most real-life applications, the customer offers a grace period-we call it a demand time window-during which a particular demand can be satisfied with no penalty. That is, in association with each demand, the customer specifies an acceptable earliest and a latest delivery time. The time interval characterized by the earliest and latest delivery dates of a demand represents the corresponding time window. This paper studies the dynamic lot-sizing problem with demand time windows and provides polynomial time algorithms for computing its solution. If backlogging is not allowed, the complexity of the proposed algorithm is O(T-2) where T is the length of the planning horizon. When backlogging is allowed, the complexity of the proposed algorithm is O(T-3).</description><author>Lee, C. Y.; Cetinkaya, S.; Wagelmans, A. P. M.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stocking decisions for low-usage items in a multilocation inventory system</title><link>http://www.example.com/articles/1</link><description>Kukreja, A.; Schmidt, C. P.; Miller, D. M.
This research shows that organizations with a number of "sister" plants, warehouses, or other stocking points can profit from the concept of proactive use of transshipments as an element of their inventory control policy. For certain types of parts, employing transshipments can significantly reduce the total inventory needed throughout the entire collection of stocking points. The study is motivated by a real-life situation involving a large utility company having 29 power-generating plants in five Southeastern states where there are thousands of parts that are commonly used at multiple plants. At present, each plant operates independently and maintains enough stock to meet its own requirements. Transshipments take place between plants whenever there is an emergency requirement for a part, but no explicit consideration is given to this effect while deciding on stocking levels at different plants. In the case we examined, by setting stocking levels to explicitly take account of transshipments, total system cost could be reduced by about 70% over the company's decentralized policy. In general, this work considers a single-echelon, N-location, continuous-review inventory system in which complete pooling of stock is permitted among the locations. A model is developed for slow-moving, expensive, and consumable parts that are common to two or more locations. A one-for-one ordering policy and queueing theory allow development and solution of a system of equations for the probability distribution of net inventory at each location. A heuristic procedure is also developed to determine cost-effective stocking levels. Actual data from the utility company are used to demonstrate the applicability of the model. Savings achieved as a result of pooling are reported.</description><author>Kukreja, A.; Schmidt, C. P.; Miller, D. M.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Job rotation as a learning mechanism</title><link>http://www.example.com/articles/1</link><description>Ortega, J.
This article analyzes the costs and benefits of job rotation as a mechanism with which the firm can learn about the employees' productivities and the profitability of different jobs or activities. I compare job rotation to an assignment policy where employees specialize in one job along their career. The gains from adopting a job rotation policy are larger when there is more prior uncertainty about employees and activities. I argue that this firm learning theory fits the existing evidence on rotation better than alternative explanations based on employee motivation and employee learning.</description><author>Ortega, J.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrating replenishment decisions with advance demand information</title><link>http://www.example.com/articles/1</link><description>Gallego, G.; Ozer, O.
There is a growing consensus that a portfolio of customers with different demand lead times can lead to higher, more regular revenues and better capacity utilization. Customers with positive demand lead times place orders in advance of their needs, resulting in advance demand information. This gives rise to the problem of finding effective inventory control policies under advance demand information. We show that state-dependent (s, S) and base-stock policies are optimal for stochastic inventory systems with and without fixed costs. The state of the system reflects our knowledge of advance demand information. We also determine conditions under which advance demand information has no operational value. A numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information.</description><author>Gallego, G.; Ozer, O.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of collaborative forecasting on supply chain performance</title><link>http://www.example.com/articles/1</link><description>Aviv, Y.
We consider a cooperative, two-stage supply chain consisting of two members: a retailer and a supplier. In our first model, called local forecasting, each member updates the forecasts of future demands periodically, and is able to integrate the adjusted forecasts into his replenishment process. Forecast adjustments made at both levels of the supply chain can be correlated. The supply chain has a decentralized information structure, so that day-to-day inventory and forecast information are known locally only. In our second model, named collaborative forecasting, the supply chain members jointly maintain and update a single forecasting process in the system. Hence, forecasting information becomes centralized. Finally, we consider as a benchmark the special case in which forecasts are not integrated into the replenishment processes at all. We propose a unified framework that allows us to study and compare the three types of settings. This study comes at a time when various types of collaborative forecasting partnerships are being experimented within industry, and when the drivers for success or failure of such initiatives are not yet fully understood. In addition to providing some managerial insights into questions that arise in this context, our set of models is tailored to serve as building blocks for future work in this emerging area of research.</description><author>Aviv, Y.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capacity acquisition, subcontracting, and lot sizing</title><link>http://www.example.com/articles/1</link><description>Atamturk, A.; Hochbaum, D. S.
The fundamental question encountered in acquiring capacity to meet nonstationary demand over a multiperiod horizon is how to balance the trade-off between having insufficient capacity in some periods and excess capacity in others. In the former situation, part of the demand is subcontracted while, in the latter, capacity that has been paid for is rendered idle. Capacity and subcontracting decisions arise in many economic activities ranging from production capacity planning in semiconductor fabs to leasing communication networks, from transportation contracts to staffing of call centers. In this paper, we investigate the trade-offs between acquiring capacity, subcontracting, production, and holding inventory to satisfy nonstationary demand over a finite horizon. We present capacity acquisition models with holding and without holding inventory and identify forecast-robust properties of the models that restrict the dependence of optimal capacity decisions on the demand forecasts. We develop algorithms for numerous practical cost structures involving variable and fixed charges and prove that they all have polynomial time complexity. For models with inventory, we solve a sequence of constant capacity lot-sizing and subcontracting subproblems, which is also of independent interest.</description><author>Atamturk, A.; Hochbaum, D. S.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On maximizing the net present value of a project under renewable resource constraints</title><link>http://www.example.com/articles/1</link><description>Vanhoucke, M.; Demeulemeester, E.; Herroelen, W.
In this paper we study the resource-constrained project-scheduling problem with discounted cash flows. Each activity of this resource-constrained project-scheduling problem has certain resource requirements and a known deterministic cash flow that can be either positive or negative. Deterministic cash flows are assumed to occur over the duration of the activities. Progress payments and cash outflows occur at the completion of activities. The objective is to schedule the activities subject to a fixed deadline to maximize the net present value subject to the precedence and resource constraints. With these features the financial aspects of project management are taken into account. We introduce a depth-first branch-and-bound algorithm that makes use of extra precedence relations to resolve a number of resource conflicts and a fast recursive search algorithm for the max-npv problem to compute upper bounds. The recursive search algorithm exploits the idea that positive cash flows should be scheduled as early as possible while negative cash flows should be scheduled as late as possible within the precedence constraints. The procedure has been coded in Visual C++, Version 4.0 under Windows NT, and has been validated on two problem sets.</description><author>Vanhoucke, M.; Demeulemeester, E.; Herroelen, W.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Outcomes-adjusted reimbursement in a health-care delivery system</title><link>http://www.example.com/articles/1</link><description>Fuloria, P. C.; Zenios, S. A.
This paper considers a health-care delivery system with two noncooperative parties: a purchaser of medical services and a specialized provider. A dynamic principal-agent model that captures the interaction between the two parties is developed. In this model, patients arrive exogenously, receive periodic treatment from the provider, suffer costly complications that require hospital care, and eventually exit the system in death. The provider chooses the intensity of treatment in each period, incurs an associated cost, and is reimbursed by the purchaser according to observed patient outcomes. The purchaser's problem is to determine a payment system that will induce treatment choices maximizing total social welfare. The optimal payment system, referred to as the outcomes-adjusted payment system, is identified. It consists of a prospective payment per patient and a retrospective payment adjustment based on adverse short-term patient outcomes. This system induces the most efficient delivery of medical services by combining the immediate "threat" of a retrospective payment adjustment with the future reward of prospective payments generated by surviving patients. A numerical example is provided in the context of Medicare's End-Stage Renal Disease program. The example compares the optimal system to systems that are currently in place. The results suggest that the purchaser can achieve significant gains in patient life expectancy by switching to the outcomes-adjusted payment system, but this requires accurate information about treatment technology, patient characteristics, and provider preferences. The life-expectancy gains do not involve increased medical expenditures.</description><author>Fuloria, P. C.; Zenios, S. A.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational differences in rates of learning: Evidence from the adoption of minimally invasive cardiac surgery</title><link>http://www.example.com/articles/1</link><description>Pisano, G. P.; Bohmer, R. M. J.; Edmondson, A. C.
This paper examines learning curves in the health care setting to determine whether organizations achieve performance improvements from cumulative experience at different rates. Although extensive research has shown that cumulative experience leads to performance improvement across numerous contexts, the question of how much of this improvement is due to mere experience and how much is due to collective learning processes has received little attention. We argue that organizational learning processes may allow some organizations to benefit more than others from equivalent levels of experience. We thus propose that learning curves can vary across organizations engaged in the same "learning task," due to organizational learning effects. To investigate this proposition, we investigate cardiac surgery departments implementing a new technology for minimally invasive cardiac surgery. Data on operative procedure times from a sample of 660 patients who underwent the new operation at 16 different institutions are analyzed. The results confirm that cumulative experience is a significant predictor of learning, and further reveal that the slope of the learning curve varies significantly across organizations. Theoretical and practical implications of the work are discussed.</description><author>Pisano, G. P.; Bohmer, R. M. J.; Edmondson, A. C.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Asymptotic distribution of the EMS option price estimator</title><link>http://www.example.com/articles/1</link><description>Duan, J. C.; Gauthier, G.; Simonato, J. G.
Monte Carlo simulation is commonly used for computing prices of derivative securities when an analytical solution does not exist. Recently, a new simulation technique known as empirical martingale simulation (EMS) has been proposed by Duan and Simonato (1998) as a way of improving simulation accuracy. EMS has one drawback however. Because of the dependency among sample paths created by the EMS adjustment, the standard error of the price estimate is not readily available from using one simulation sample. In this paper, we develop a scheme to estimate the EMS accuracy. The EMS price estimator is first shown to have an asymptotically normal distribution. Through a simulation study, we then find that the asymptotic normal distribution serves as a good approximation for samples consisting of as few as 500 simulation paths.</description><author>Duan, J. C.; Gauthier, G.; Simonato, J. G.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Procurement planning to maintain both short-term adaptiveness and long-term perspective</title><link>http://www.example.com/articles/1</link><description>Bonser, J. S.; Wu, S. D.
We study the fuel procurement problem for electrical utilities under uncertain demand and market price. Long-term contractual supply commitments are made at a set price with fuel suppliers at the beginning of each year. Each month the procurement planner can use fuel from these contracts or purchase fuel at the current market price. Motivated by practical insights from this market, we propose a two-phase dynamic procedure to determine a procurement plan. In the first phase, the minimum contract purchases for each month are determined at the beginning of the year. In the second phase, given the minimum contract purchases, the more detailed procurement decisions are determined at the beginning of each month with the most up-to-date information. We perform intensive computational experiments that show that this procedure produces high-quality solutions comparable to a rolling-horizon stochastic-programming heuristic, is easier to maintain and generalize, is computationally faster, and is robust to random fluctuations in demand requirements, spot market prices, and other sources of uncertainty.</description><author>Bonser, J. S.; Wu, S. D.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A nested decomposition approach to a three-stage, two-dimensional cutting-stock problem</title><link>http://www.example.com/articles/1</link><description>Vanderbeck, F.
We consider the cutting of rectangular order pieces into stock pieces of specified width and length. The cutting process involves three stages of orthogonal guillotine cutting: Stock pieces are cut into sections that are cut into slits that are cut into order pieces. Restrictions imposed on the cutting process make the combinatorial structure of the problem more complex, but limit the scope of solution space. The objective of the problem is mainly to minimize waste, but our model also accounts for other issues such as aging stock pieces, urgent or optional orders, and fixed setup costs. Our solution approach involves a nested decomposition of the problem and the recursive use of the column-generation technique: We use a column-generation formulation of the problem (Gilmore and Gomory 1965) and the cutting-pattern-generation subproblem is itself solved using a column-generation algorithm. LP-based lower bounds on the minimum cost are computed and, by rounding the LP solution, a feasible solution and associated upper bound is obtained. This approach could in principle be used in a branch-and-bound search to solve the problem to optimality. We report computational results for industrial instances. The algorithm is being used in industry as a production-planning tool.</description><author>Vanderbeck, F.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A tabu-search heuristic for the capacitated lot-sizing problem with set-up carryover</title><link>http://www.example.com/articles/1</link><description>Gopalakrishnan, M.; Ding, K.; Bourjolly, J. M.; Mohan, S.
This paper presents a tabu-search heuristic for the capacitated lot-sizing problem (CLSP) with set-up carryover. This production-planning problems allows multiple items to be produced within a time period, and setups for items to be carried over from one period to the next. Two interrelated decisions, sequencing and lot sizing, are present in this problem. Our tabu-search heuristic consists of five basic move types - three for the sequencing decisions and two for the lot-sizing decisions. We allow infeasible solutions to be generated at a penalty during the course of the search. We use several search strategies, such as dynamic tabu list, adaptive memory, and self-adjusting penalties, to strengthen our heuristic. We also propose a lower-bounding procedure to estimate the quality of our heuristic solution. We have also modified our heuristic to produce good solutions for the CLSP without set-up carryover. The computational study, conducted on a set of 540 test problems, indicates that on average our heuristic solutions are within 12% of a bound on optimality. In addition, for the set of test problems our results indicate an 8% reduction in total cost through set-up carryover.</description><author>Gopalakrishnan, M.; Ding, K.; Bourjolly, J. M.; Mohan, S.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A polyhedral approach to simplified crew scheduling and vehicle scheduling problems</title><link>http://www.example.com/articles/1</link><description>Fischetti, M.; Lodi, A.; Martello, S.; Toth, P.
Crew and vehicle scheduling are fundamental issues in public transit management. Informally, they can be described as the problem of determining the optimal duties for a set of crews (e.g., bus drivers) or vehicles (e.g., buses) so as to cover a given set of timetabled trips, satisfying a number of constraints laid down by the union contract and company regulations. We consider the simplified but still NP-hard case in which several depots are specified, and limits on both the total time between the start and the end of any duty (spread time) and the total duty operational time (working time) are imposed. We give a 0-1 linear programming formulation based on binary variables associated with trip transitions, which applies to both crew and vehicle scheduling. The model is enhanced by means of new families of valid inequalities, for which exact and heuristic separation procedures are proposed. These techniques are embedded into an exact branch-and-cut algorithm, which also incorporates heuristic procedures. The performance of two implementations of the method (for vehicle scheduling and crew scheduling, respectively) are evaluated through computational testing on both random and real-world test problems from the literature.</description><author>Fischetti, M.; Lodi, A.; Martello, S.; Toth, P.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The nonstationary staff-planning problem with business cycle and learning effects</title><link>http://www.example.com/articles/1</link><description>Anderson, E. G.
Managing highly skilled employees is extremely complex because of the need to balance the costs and time lags associated with their training against the need to meet demand as quickly as possible. Unlike previous approaches to this problem in the staffing literature, this paper develops an optimal staffing policy at the strategic level to cope with nonstationary stochastic demand for a staff characterized by unproductive apprentice employees and fully productive experienced employees. The paper then explores the implications of this policy in different industries, using empirical data. Aside from the optimal policy, this paper's primary results include: (1) demand volatility reduces average productivity, most especially under conditions of low (or slightly negative) growth and - nonintuitively - low employee turnover or knowledge obsolescence rates; (2) there is a trade-off between meeting demand and high productivity; (3) firms with longer business cycles should smooth their hiring and firing policies; and (4) firms in industries with longer training times should smooth their hiring and firing policies. The paper also explores the possible rewards from reducing training times and turnover rates. Finally, it discusses managerial implications and possible future directions in research.</description><author>Anderson, E. G.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A multiple attribute utility theory approach to ranking and selection</title><link>http://www.example.com/articles/1</link><description>Butler, J.; Morrice, D. J.; Mullarkey, P. W.
Managers of large industrial projects often measure performance by multiple attributes. For example, our paper is motivated by the simulation of a large industrial project called a land seismic survey, in which project performance is based on duration, cost, and resource utilization. To address these types of problems, we develop a ranking and selection procedure for making comparisons of systems (e.g., project configurations) that have multiple performance measures. The procedure combines multiple attribute utility theory with statistical ranking and selection to select the best configuration from a set of possible configurations using the indifference-zone approach. We apply our procedure to results generated by the simulator for a land seismic survey that has six performance measures, and describe a particular type of sensitivity analysis that can be used as a robustness check.</description><author>Butler, J.; Morrice, D. J.; Mullarkey, P. W.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Implementable mechanisms to coordinate horizontal alliances</title><link>http://www.example.com/articles/1</link><description>Nault, B. R.; Tyagi, R. K.
Unprecedented changes in the economics of interaction, mainly as a result of advances in information and telecommunication technologies such as the Internet, are causing a shift toward more networked forms of organizations such as horizontal alliances - that is, alliances among firms in similar businesses that have positive externalities between them. Because the success of such horizontal alliances depends crucially on aligning individual alliance-member incentives with those of the alliance as a whole, it is important to find coordination mechanisms that achieve this alignment and are simple-to-implement. In this paper, we examine two simple coordination mechanisms for a horizontal alliance characterized by the following features: (i) firms in the alliance can exert effort only in their "local" markets to increase customer demand for the alliance; (ii) customers are mobile and a customer living in a given alliance member's local area may have a need to buy from some other alliance member; and (iii) the coordination rules followed by the alliance determine which firms from a large pool of potential member-firms join the alliance, and how much effort each firm joining the alliance exerts in its local market. In this horizontal alliance setup, we consider the use of two coordination mechanisms: (i) a linear transfer of fees between members if demand from one member's local customer is served by another member, and (ii) ownership of an equal share of the alliance profits generated from a royalty on each member's sales. We derive conditions on the distribution of demand externalities among alliance members to determine when each coordination mechanism should be used separately, and when the mechanisms should be used together.</description><author>Nault, B. R.; Tyagi, R. K.</author><pubDate>Fri, 01 Jun 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Comparative reliability of verdicts</title><link>http://www.example.com/articles/1</link><description>Klausner, A.; Pollak, M.
We consider the problem of evaluating the reliability of a verdict given by a panel of judges. Given no information other than the number of panelists for and against, we address the question of when is a verdict that was obtained by a majority of k(1) vs. j(1) more or less reliable than one reached by k(2) vs j(2) We define criteria and investigate which verdicts are comparable and which are not. Consequences of this study may have bearing on choice of panel size and decision rule for decision-making bodies, such as courts, juries, committees, and boards. As implied by the above, our perspective is a posterior view of reliability though it also entails prior concern regarding how the reliability of a verdict will be perceived after being delivered. As an example, we apply our results to comparing the reliability of different verdicts handed down by the Supreme Court of the State of Israel and assessing the option of expanding a hearing on a case from a bench of three judges to a larger panel.</description><author>Klausner, A.; Pollak, M.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal pricing that coordinates queues with customer-chosen service requirements</title><link>http://www.example.com/articles/1</link><description>Ha, A. Y.
This article considers the problem of coordinating the admission rates and service requirements of a multiclass queue when these decisions are made on a decentralized basis. The customer classes are characterized by different demand patterns, delay costs, and service costs. Customers make individual decisions on whether to join the queue and, if so, their service requirements. Their class identities and service requirements are private information not known to the system manager. We develop a two-stage decision framework to analyze the problem and characterize the optimal admission rates and service requirements under both centralized and decentralized assumptions. We distinguish admission and service externality costs that lead to suboptimal performance under decentralized control. For a given service discipline, we derive optimal class-specific pricing schemes that can coordinate the system when only service requirements but not class identities are unobservable. When customer class identities are also unobservable, we consider two common service disciplines that offer undifferentiated service: processor sharing and first-come-first-served. Based on the general framework, for the M/G/s processor sharing queue, we show that a single variable fee (payment per unit of time in the system) can induce the optimal admission rates and service requirements for all customer classes. For the M/G/1 first-come-first-served queue, we show that a single pricing scheme that is quadratic in time in service can induce the optimal admission rates and service requirements for all customer classes. Our result demonstrates that, under suitable conditions, simple and undifferentiated pricing can coordinate complex queueing systems with heterogeneous customer classes.</description><author>Ha, A. Y.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cutting corners and working overtime: Quality erosion in the service industry</title><link>http://www.example.com/articles/1</link><description>Oliva, R.; Sterman, J. D.
The erosion of service quality throughout the economy is a frequent concern in the popular press. The American Customer Satisfaction Index for services fell in 2000 to 69.4%, down 5 percentage points from 1994. We hypothesize that the characteristics of services-inseparability intangibility and labor intensity-interact with management practices to bias service providers toward reducing the level of service they deliver, often locking entire industries into a vicious cycle of eroding service standards. To explore this proposition we develop a formal model that integrates the structural elements of service delivery. We use econometric estimation, interviews, observations, and archival data to calibrate the model for a consumer-lending service center in a major bank in the United Kingdom. We find that temporary imbalances between service capacity and demand interact with decision rules for effort allocation, capacity management, overtime, and quality aspirations to yield permanent erosion of the service standards and loss of revenue. We explore policies to improve performance and implications for organizational design in the service sector.</description><author>Oliva, R.; Sterman, J. D.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Shared-savings contracts for indirect materials in supply chains: Channel profits and environmental impacts</title><link>http://www.example.com/articles/1</link><description>Corbett, C. J.; DeCroix, G. A.
There are many materials for which the quantity needed by a firm is at best indirectly related to the quantity of final product produced by that firm, such as solvents in manufacturing processes or office supplies. For any such "indirect" materials, an inescapable incentive conflict exists: The buyer wishes to minimize consumption of these indirect materials, while the supplier's profits depend on increasing volume. Both buyer and supplier can exert effort to reduce consumption, hence making the overall supply chain more efficient. However, no supplier will voluntarily participate unless contract terms are fundamentally revised. This can be done through a variety of "shared-savings" contracts, where both parties profit from a consumption reduction. This paper analyzes several such contracts currently in use for chemicals purchasing. We show that such contracts can always increase supply-chain profits but need not lead to reduced consumption. We analyze equilibrium effort levels, consumption, and total profits, and show how these change with the contract parameters. We find that the goals of maximizing joint profits and minimizing consumption are generally not aligned. Also, surprisingly, a decrease in a cost parameter can lead to a decrease in profits; it may be necessary (but is always possible) to renegotiate the shared-savings contract to reap the benefits of a cost decrease.</description><author>Corbett, C. J.; DeCroix, G. A.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Hybridising tabu search with optimisation techniques for irregular stock cutting</title><link>http://www.example.com/articles/1</link><description>Bennell, J. A.; Dowsland, K. A.
Sequential meta-heuristic implementations for the irregular stock-cutting problem have highlighted a number of common problems. The literature suggests a consensus that it is more efficient to allow configurations with overlapping pieces in the solution space and to penalise these in the evaluation function. However, depending on the severity of the penalty this relaxation results in a tendency to converge toward infeasible solutions or to seek out feasible solutions at the expense of overall quality. A further problem is encountered in defining a neighbourhood search strategy that can deal with the infinite solution space inherent in the irregular stock-cutting problem. The implementation in this paper adopts a hybrid tabu search approach that incorporates two very different optimisation routines that utilise alternative neighbourhoods to address the described problems.</description><author>Bennell, J. A.; Dowsland, K. A.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Regression, correlation, and the time interval: Additive-multiplicative framework</title><link>http://www.example.com/articles/1</link><description>Levy, H.; Guttman, I.; Tkatch, I.
Then two random variables are both additive or multiplicative, the effect of the way one "slices" the available period to subperiods (time intervals) is well documented in the literature. In this paper, we investigate the time interval effect when one of the variables is additive and one is multiplicative. We prove that the squared multiperiod correlation coefficient (rho (2)(n)) decreases monotonically as n increases, and approaches zero when n goes n to infinity. However, for relevant data corresponding to the U.S. stock market index, when shifting from weekly parameters to quarterly parameters the decrease in rho (2)(n) is negligible. n The effect on the regression coefficient is much more dramatic and even a shift from weekly data to quarterly data affects the regression coefficient substantially. The regression slope generally approaches zero, minus infinity or plus infinity, as the number of periods increases. Montonicity, however, exists only in certain cases.</description><author>Levy, H.; Guttman, I.; Tkatch, I.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>New procedures to select the best simulated system using common random numbers</title><link>http://www.example.com/articles/1</link><description>Chick, S. E.; Inoue, K.
Although simulation is widely used to select the best of several alternative system designs, and common random numbers is an important tool for reducing the computation effort of simulation experiments, there are surprisingly few tools available to help a simulation practitioner select the best system when common random numbers are employed. This paper presents new two-stage procedures that use common random numbers to help identify the best simulated system. The procedures allow for screening and attempt to allocate additional replications to improve the value of information obtained during the second stage, rather than determining the number of replications required to provide a given probability of correct selection guarantee. The procedures allow decision makers to reduce either the expected opportunity cost associated with potentially selecting an inferior system, or the probability of incorrect selection. A small empirical study indicates that the new procedures outperform several procedures with respect to several criteria, and identifies potential areas for further improvement.</description><author>Chick, S. E.; Inoue, K.</author><pubDate>Wed, 01 Aug 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and hedging path-dependent options under the CEV process</title><link>http://www.example.com/articles/1</link><description>Davydov, D.; Linetsky, V.
Much of the work on path-dependent options assumes that the underlying asset price follows geometric Brownian motion with constant volatility This paper uses a more general assumption for the asset price process that provides a better fit to the empirical observations. We use the so-called constant elasticity of variance (CEV) diffusion model where the volatility is a function of the underlying asset price. We derive analytical formulae for the prices of important types of path-dependent options under this assumption. We demonstrate that the prices of options, which depend on extrema, such as barrier and lookback options, can be much more sensitive to the specification of the underlying price process than standard call and put options and show that a financial institution that uses the standard geometric Brownian motion assumption is exposed to significant pricing and hedging errors when dealing in path-dependent options.</description><author>Davydov, D.; Linetsky, V.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition and structure in serial supply chains with deterministic demand</title><link>http://www.example.com/articles/1</link><description>Corbett, C. J.; Karmarkar, U. S.
Supply chains often consist of several tiers, with different numbers of firms competing at each tier. A major determinant of the structure of supply chains is the cost structure associated with the underlying manufacturing process. In this paper, we examine the impact of fixed and variable costs on the structure and competitiveness of supply chains with a serial structure and price-sensitive linear deterministic demand. The entry stage is modeled as a simultaneous game, where the players take the outcomes of the subsequent post-entry (Cournot) competition into account in making their entry decisions. We derive expressions for prices and production quantities as functions of the number of entrants at each tier of a multitier chain. We characterize viability and stability of supply-chain structures and show, using lattice arguments, that there is always an equilibrium structure in pure strategies in the entry game. Finally, we examine the effects of vertical integration in the two-tier case. Altogether, the paper provides a framework for comparing a variety of supply-chain structures and for studying how they are affected by cost structures and by the number of entrants throughout the chain.</description><author>Corbett, C. J.; Karmarkar, U. S.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Future capacity procurements under unknown demand and increasing costs</title><link>http://www.example.com/articles/1</link><description>Burnetas, A.; Gilbert, S.
In this paper we study a situation in which a broker must manage the procurement of a short-life-cycle product. As the broker observes demand for the item, she learns about the demand process. However, as is often the case in practice, it becomes either more difficult or more expensive to procure the item as the selling season advances. Thus, the broker must trade off higher procurement costs against the benefit of making ordering decisions with better information about demand. Problems of this type arise, for example, in the travel industry, where a travel agent's cost of procuring airline and hotel reservations increases as the date of a vacation package approaches. We develop a newsvendor-like characterization of the optimal procurement policy. In a numerical analysis, we demonstrate how broker procurements tend to cluster just before price increases and how brokers can benefit from explicitly considering the effects of information about demand in their ordering policies.</description><author>Burnetas, A.; Gilbert, S.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modelling practical lot-sizing problems as mixed-integer programs</title><link>http://www.example.com/articles/1</link><description>Belvaux, G.; Wolsey, L. A.
In spite of the remarkable improvements in the quality of general purpose mixed-integer programming software, the effective solution of a variety of lot-sizing problems depends crucially on the development of tight formulations for the special problem features occurring in practice. After reviewing some of the basic preprocessing techniques for handling safety stocks and multilevel problems, we discuss a variety of aspects arising particularly in small and large bucket (time period) models such as start-ups, changeovers, minimum batch sizes, choice of one or two set-ups per period, etc. A set of applications is described that contains one or more of these special features, and some indicative computational results are presented. Finally, to show another technique that is useful, a slightly different (supply chain) application is presented, for which the a priori addition of some simple mixed-integer inequalities, based on aggregation, leads to important improvements in the results.</description><author>Belvaux, G.; Wolsey, L. A.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Solving the cell suppression problem on tabular data with linear constraints</title><link>http://www.example.com/articles/1</link><description>Fischetti, M.; Salazar, J. J.
Cell suppression is a widely used technique for protecting sensitive information in statistical data presented in tabular form. Previous works on the subject mainly concentrate on 2- and 3-dimensional tables whose entries are subject to marginal totals. In this paper we address the problem of protecting sensitive data in a statistical table whose entries are linked by a generic system of linear constraints. This very general setting covers, among others, k-dimensional tables with marginals as well as the so-called hierarchical and linked tables that are very often used nowadays for disseminating statistical data. In particular, we address the optimization problem known in the literature as the (secondary) Cell Suppression Problem, in which the information loss due to suppression has to be minimized. We introduce a new integer linear programming model and outline an enumerative algorithm for its exact solution. The algorithm can also be used as a heuristic procedure to find near-optimal solutions. Extensive computational results on a test-bed of 1,160 real-world and randomly generated instances are presented, showing the effectiveness of the approach. In particular, we were able to solve to proven optimality 4-dimensional tables with marginals as well as linked tables of reasonable size (to our knowledge, tables of this kind were never solved optimally by previous authors).</description><author>Fischetti, M.; Salazar, J. J.</author><pubDate>Sun, 01 Jul 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Creating and transferring knowledge for productivity improvement in factories</title><link>http://www.example.com/articles/1</link><description>Lapre, M. A.; Van Wassenhove, L. N.
Can a firm accelerate its learning curve if knowledge about the production function is incomplete? This article identifies a production line specifically set up to create technological knowledge about its production function through scientific experimentation (formal learning) as opposed to learning by doing. The organizational structure of this line was very successful in creating technological knowledge. Formal learning resulted in huge productivity improvements. Replication of this organizational structure on three production lines in other plants within the same firm fell short of expectations. Formal learning did not result in similar productivity improvements. Our research suggests two factors that may facilitate creation and transfer of technological knowledge: management buy-in and knowledge diversity to solve interdepartmental problems.</description><author>Lapre, M. A.; Van Wassenhove, L. N.</author><pubDate>Mon, 01 Oct 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Prospect theory, mental accounting, and differences in aggregated and segregated evaluation of lottery portfolios</title><link>http://www.example.com/articles/1</link><description>Langer, T.; Weber, M.
If individuals have to evaluate a sequence of lotteries, their judgment is influenced by the presentation mode. Experimental studies have found significantly higher acceptance rates for a sequence of lotteries if the overall distribution was displayed instead of the set of lotteries itself. Mental accounting and loss aversion provide an easy and intuitive explanation for this phenomenon. Ln this paper we offer an explanation that incorporates further evaluation concepts of Prospect Theory. Our formal analysis of the difference in aggregated and segregated portfolio evaluation demonstrates that the higher attractiveness of the aggregated presentation mode is not a general phenomenon (as suggested in the Literature) but depends on specific parameters of the lotteries. The theoretical findings are supported by an experimental study In contrast to the existing evidence and in line with our theoretical results, we find for specific types of lotteries an even lower acceptance rate if the overall distribution is displayed.</description><author>Langer, T.; Weber, M.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Producer-supplier contracts with incomplete information</title><link>http://www.example.com/articles/1</link><description>Lim, W. S.
This paper investigates the contract design problem of a producer when he purchases parts from a supplier, and there is incomplete information regarding the quality of the parts. This is the first game-theoretic model of quality control tl;at captures this informational asymmetry. We focus on two compensation schemes embedded in the contract, namely, price rebate (when inspection is done upon receipt of the parts) and warranty. We show that when a full-price rebate is not possible and the producer and the supplier have to share the damage costs, an optimal contract is such that the:supplier compensates the producer by the same amount, regardless of his quality type. However, a supplier with low quality is more likely to be offered a contract with an inspection scheme, while a supplier with high quality is constrained with a warranty scheme. We also show that when the producer need not share the cost in exactly one of the compensation schemes, he may still offer the other compensation scheme to a supplier type depending on the relative costs involved, the maximum compensation cost acceptable by all supplier types, and his ex ante beliefs about the quality level of the supplier.</description><author>Lim, W. S.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordination mechanisms for a distribution system with one supplier and multiple retailers</title><link>http://www.example.com/articles/1</link><description>Chen, F. R.; Federgruen, A.; Zheng, Y. S.
We address a fundamental two-echelon distribution system in which the sales volumes of the retailers are endogenously determined on the basis of known demand functions. Specifically, this paper studies a distribution channel where a supplier distributes a single product to retailers, who in turn sell the product to consumers. The demand in each retail market arrives continuously at a constant rate that is a general decreasing function of the retail price in the market. We have characterized an optimal strategy, maximizing total systemwide profits in a centralized system. We have also shown that the same optimum level of channelwide profits can be achieved-in a decentralized system, but only if coordination is achieved via periodically charged, fixed fees, and a nontraditional discount pricing scheme under which the discount given to a retailer is the sum of three discount components based on the retailer's (i) annual-sales volume, (ii) order quantity, and (iii) order frequency, respectively. Moreover, we show that no (traditional) discount scheme, based on order quantities only, suffices to optimize channelwide profits when there-are multiple nonidentical retailers. The paper also considers a scenario where the channel members fail to coordinate their decisions and provides numerical examples that illustrate the value of coordination. We extend our results to settings in which the retailers' holding cost rates depend on the wholesale price.</description><author>Chen, F. R.; Federgruen, A.; Zheng, Y. S.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Demand heterogeneity and technology evolution: Implications for product and process innovation</title><link>http://www.example.com/articles/1</link><description>Adner, R.; Levinthal, D.
The evolution of technology has been a central issue in the strategy and organizations Literature. However, the focus of much of this work has been on what is essentially the "supply side" of technical change-the evolution of firm capabilities. We present a demand-based view of technology evolution that is focused on the interaction between technology development and the demand environment in which the technology is ultimately evaluated. We develop a formal computer simulation model that explicitly considers the influence of heterogeneity in market demand-the presence of consumers with different needs and requirements-on firms' innovation choices. The model is used to examine the dynamics of product and process innovation (Utterback and Abernathy 1975). The analysis reveals that demand heterogeneity offers an alternative to supply-side explanations of the technology Life cycle. Further, by considering the implications of decreasing marginal utility from performance improvements, the model highlights the role of "technologically satisfied" consumers in shaping innovation incentives, and suggests a rationale for a new stage in the technology life cycle characterized by increasing performance at a stable price. The stage has not yet been treated formally in the Literature, but is widely observed, most prominently in digital and information-based technologies.</description><author>Adner, R.; Levinthal, D.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contracting to assure supply: How to share demand forecasts in a supply chain</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.; Lariviere, M. A.
Forecast sharing is studied in a supply chain with a manufacturer that faces stochastic demand for a single product and a supplier that is the sole source for a critical component. The following sequence of events occurs: the manufacturer provides her initial forecast to the supplier along with a contract,the supplier constructs-capacity (if he accepts the Contract), the manufacturer receives an updated forecast and submits a final order. Two contract compliance regimes are considered. If the supplier accepts the contract under forced compliance then he has little flexibility with respect to his capacity choice; under voluntary compliance, however, he maintains substantial flexibility. Optimal supply chain performance requires the manufacturer to share her initial forecast truthfully, but she has an incentive to inflate her forecast to induce the supplier to build more capacity. The supplier is aware of this bias, and so may not trust the manufacturer's forecast, harming supply chain performance. We study contracts that allow the supply chain to share demand forecasts credibly under either compliance regime.</description><author>Cachon, G. P.; Lariviere, M. A.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managerial allocation of time and effort: The effects of interruptions</title><link>http://www.example.com/articles/1</link><description>Seshadri, S.; Shapira, Z.
Time is one of the more salient constraints on managerial behavior. This constraint may be very taxing in high-velocity environments where managers have to attend to many;tasks simultaneously. Earlier work by Radner (1976) proposed models based on notions of the thermostat or "putting out fires" to guide managerial time and effort allocation among tasks. We link these ideas to the issue of the level of complexity of the tasks to be attended to while alluding to the sequential versus parallel modes of processing. We develop a stochastic model to analyze the behavior of a manager who has to attend to a few short-term proesses while attempting to devote as much time as possible to the pursuit of a long-term project. A major aspect of this problem is how the manager deals with interruptions. Different rules of attention allocation are proposed, and their implications to managerial behavior are discussed.</description><author>Seshadri, S.; Shapira, Z.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Parallel and sequential testing of design alternatives</title><link>http://www.example.com/articles/1</link><description>Loch, C. H.; Terwiesch, C.; Thomke, S.
An important managerial problem in product design in the,extent to which testing activities are carried out in parallel or in series. Parallel testing has the advantage of proceeding more rapidly than serial testing but does not take advantage of the potential for learning between tests, thus resulting in a larger number of tests. We model this trade-off in the form of a dynamic program and derive the optimal testing strategy (or mix of parallel and serial testing) that minimizes both the to;al cost and time of testing. We derive the optimal testing strategy as a function of testing cost, prior knowledge, and testing lead time. Using information theory to measure the test efficiency, we further show that in the case of imperfect testing (due to noise or simulated test conditions), the attractiveness of parallel strategies decreases. Finally, we analyze the relationship between testing strategies and the structure of design hierarchy. We show that a key benefit of modular product architecture lies in the reduction of testing cost.</description><author>Loch, C. H.; Terwiesch, C.; Thomke, S.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Ending inventory valuation in multiperiod production scheduling</title><link>http://www.example.com/articles/1</link><description>Fisher, M.; Ramdas, K.; Zheng, Y. S.
When making lot-sizing decisions, managers often use a model horizon T that is much smaller than any reasonable estimate of the firm's future horizon. This is done because forecast accuracy deteriorates rapidly for longer horizons, while computational burden increases. However, what is optimal over the short horizon may be suboptimal over the long run, resulting in errors known as end-effects. A common end-effect in lot-sizing models is to set end-of-horizon inventory to zero. This policy can result in excessive setup costs or stock-outs in the long run. We present a method to mitigate end-effects in lot sizing by including a valuation term V(I-T) for end-of-horizon inventory I-T, in the objective function of the short-horizon model. We develop this concept within the classical EOQ modeling framework, and then apply it to the dynamic lot-sizing problem (DLSP). If demand in each period of the DLSP equals the long-run average demand rate, then our procedure induces an optimal ordering policy over the short horizon that coincides with the long-run optimal ordering policy. We test our procedure empirically against the Wagner-Whitin algorithm and the Silver Meal heuristic, under several demand patterns, within a rolling horizon framework. With few exceptions, our approach significantly outperforms the other approaches tested, for modest to long model horizons. We discuss applicability to more general lot-sizing problems.</description><author>Fisher, M.; Ramdas, K.; Zheng, Y. S.</author><pubDate>Tue, 01 May 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sharing and lateral transshipment of inventory in a supply chain with expensive low-demand items</title><link>http://www.example.com/articles/1</link><description>Grahovac, J.; Chakravarty, A.
The emergence of carriers that deliver items to geographically dispersed destinations quickly rand at a reasonable cost,: combined with the low cost of sharing information through networked databases, has opened up new opportunities to better manage inventory. We investigate these benefits in the context of a supply chain in which a manufacturer supplies;expensive, low-demand items to vertically integrated or autonomous retailers via one central depot. The manufacturer's lead time is assumed to be due to the geographical distance, from the market or a combination of low:volumes, high variety,land inflexible production processes. We formulate and solve an appropriate mathematical model based on one-for-one inventory policies in which a replenishment order is placed as soon as the customer withdraws an item. We find that sharing and transshipment of items often, but not always, reduces the overall costs of holding, shipping, and waiting for inventory. Unexpectedly, these cost reductions are sometimes achieved through increasing overall inventory levels in the supply chain. Finally while sharing of inventory typically: benefits all the participants in decentralized supply chains, this is not necessarily the case-sharing can hurt the distributor or individual retailers, regardless of their relative power: in the supply chain.</description><author>Grahovac, J.; Chakravarty, A.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The trade-off between efficiency and learning in interorganizational relationships for product development</title><link>http://www.example.com/articles/1</link><description>Sobrero, M.; Roberts, E. B.
his paper analyzes the performance implications of interorganizational relationships in the development of technological innovations, focusing on the characteristics of the tasks partitioned between a manufacturer and its suppliers in the development of new products. We identify two critical dimensions: (1) the design scope and (2) the level of task interdependency. The design scope dimension characterizes the type of problem-solving activities outsourced by the manufacturer. The level of task interdependency dimension characterizes the influence of any given supplier-manufacturer interaction on other activities within an overall innovation process. Data:analyses on 50 supplier-manufacturer relationships drawn from three new product development projects show that the type of problem-solving activities being partitioned and their level of interdependency with the rest of the project are important predictors of performance outcomes of the relationship, controlling for contractual differences. Further, the analyses demonstrate a clear trade-off between short-term efficiency-increasing and longer-term learning-enhancing outcomes.</description><author>Sobrero, M.; Roberts, E. B.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capacitated multi-item inventory systems with random and seasonally fluctuating demands: Implications for postponement strategies</title><link>http://www.example.com/articles/1</link><description>Aviv, Y.; Federgruen, A.
We e address multi-item inventory systems with random and seasonally fluctuating and possibly correlated, demands. The items are produced in two stages, each with its own lead-time; in the first stage a common intermediate product is manufactured. The production volumes in the first stage are bounded by given capacity limits. We develop an accurate lower bound and close-to-optimal heuristic strategies of simple structure. The gap between them, evaluated in an extensive numerical study, is on average only 0.45%. We use the model to investigate the benefits of various delayed product differentiation (postponement) strategies, as well as other strategic questions, including (i) the benefits of flexible versus dedicated production facilities; (ii) the trade-off between capacity and inventory investments, and (iii) the trade-off between capacity investments and service levels.</description><author>Aviv, Y.; Federgruen, A.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning and forgetting: Modeling optimal product sampling over time</title><link>http://www.example.com/articles/1</link><description>Heiman, A.; McWilliams, B.; Shen, Z. H.; Zilberman, D.
Firms use samples to increase the sales of almost all consumable goods, including food, health, and cleaning products. Despite its importance, sampling remains one of the most under-researched areas. There are no theoretical quantitative models of sampling behavior other than the pioneering work of Jain et al. (1995), who modeled sampling as an important factor in the diffusion of new products. In this paper we characterize sampling as having two effects. The first is the change in the probability of a consumer purchasing a product immediately after having sampled the product. The second is an increase in the consumer's cumulative goodwill formation, which results from sampling the product. This distinction differentiates our model from other models of goodwill, in which firm sales are only a function of the existing goodwill level. We determine the optimal dynamic sampling effort of a firm and examine the factors that affect the sampling decision. We find that although the sampling effort will decline over a product's Life cycle, it may continue in mature products. Another finding is that when we have a positive change in the factors that increase sampling productivity steady-state goodwill stock and sales will increase, but equilibrium sampling can either increase or decrease. The change in the sampling level is indeterminate because, while increased sampling productivity means that firms have incentives to increase sampling, the increase in the equilibrium goodwill level indirectly reduces the marginal productivity of sampling, thus reducing the incentives to sample. We discuss managerial implications, and how the model can be used to address various circumstances.</description><author>Heiman, A.; McWilliams, B.; Shen, Z. H.; Zilberman, D.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Fast simulation of Markov chains with small transition probabilities</title><link>http://www.example.com/articles/1</link><description>Juneja, S.; Shahabuddin, P.
Consider: a-finite-state Markov chain where the transition probabilities differ by orders of magnitude. This Markov chain has an "attractor state," i.e., from any state of the Markov chain there exists:a sample path of significant probability-to the-attractor state. There also exists a "rare set," which is accessible from the attractor state only by sample paths of very. small probability. The problem is to estimate the probability that starting from the attractor state, the Maykov-chain hits the rare set before returning to the attractor state. Examples of this setting arise in the case of reliability models with highly reliable components as well as in the case of-queueing networks with low traffic. Importance-sampling is a commonly used simulation technique for the fast estimation of rare-event probabilities. It-involves simulating the Markov chain under a new probability-measure that emphasizes the most likely paths to therare set.-Previous research focused on developing importance-sampling schemes fora special case of Markov chains that did not include "high-probability cycles." We show, through examples that the Markov chains,used to model many commonly encountered systems do have high-probability cycles, and existing importance-sampling schemes can lead to infinite variance in simulating such systems, We then develop the insight that in the:presence. of high-probability cycles care should be taken in allocating:the new transition probabilities so that the variance accumulated over these cycles does not increase without bounds. Based on this observation we develop two importance-sampling techniques that have the bounded; relative error property, i.e., the simulation run-length required to estimate the, rare-event probability to a fixed degree of accuracy remains bounded the event of interest becomes more rare.</description><author>Juneja, S.; Shahabuddin, P.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An empirical examination of dynamic quality-based learning models</title><link>http://www.example.com/articles/1</link><description>Ittner, C. D.; Nagar, V.; Rajan, M. V.
Using detailed data on defect rates and quality costs from twelve plants of a Fortune 500 company, we provide the first direct tests of predictions arising from two-sets of dynamic quality-based learning models. We find-greater support for quality-based learning models that assume learning is a function of both proactive investments in quality improvement and autonomous learning-by-doing, than for models-that assume learning is a function of reactive investments in quality improvement alone. We then extend these two:Sets of models: to examine the impact of individual prevention activities and past nonconformance expenditures on defect rates. We find that-benefits from different types of prevention expenditures vary, and that past nonconformance expenditures provide learning opportunities that allow the organization to more efficiently cope with future failures,, thereby reducing subsequent nonconformance costs. These important implications are absent incurrent-quality-based learning models, providing an opportunity for future theoretical development.</description><author>Ittner, C. D.; Nagar, V.; Rajan, M. V.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A generalized model of operations reversal for fashion goods</title><link>http://www.example.com/articles/1</link><description>Jain, N.; Paul, A.
Operations reversal is a process design principle that involves switching two consecutive stages of the manufacturing process to improve process performance; In tl;is paper we investigate conditions under which operations reversal can be used to reduce the variability-as measured by the variance and standard deviation-of production volumes at the intermediate stage of the manufacturing process. We:generalize the operations reversal model of Lee and Tang (1998) to explicitly incorporate two important characteristics of fashion goods markets: heterogeneity among customers and unpredictability of customer preferences. We also present a new approach to modeling the operations reversal problem.</description><author>Jain, N.; Paul, A.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strong one-switch utility</title><link>http://www.example.com/articles/1</link><description>Bell, D. E.; Fishburn, P. C.
The linear plus exponential utility function has received increasing attention of late as a particularly attractive family for evaluating additive gambles;for wealth. In,addition to its ability to reflect increasing appreciation for money, risk aversion, and decreasing risk. aversion, it is consistent with a risk-return representation in-which return is measured by expected value. In this paper we present a new condition, strong one-switch, that characterizes the Linear plus exponential family.</description><author>Bell, D. E.; Fishburn, P. C.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information sharing in a supply chain: A note on its value when demand is nonstationary</title><link>http://www.example.com/articles/1</link><description>Raghunathan, S.
In a recent paper, Lee, So, and Tang (2000) showed that in a two-level supply chain with non-stationary AR(1) end demand, the manufacturer benefits significantly when the retailer shares point-of-sale (POS) demand data. We show in this paper, analytically and through simulation, that the manufacturer's benefit is insignificant when the parameters of the AR(1) process are known to both parties, as in Lee, So, and Tang (LST). The key-reason for the difference:between our results and those of LST is that:LST assume that the manufacturer also uses an AR(1) process to forecast the retailer order quantity. However, the manufacturer can reduce the variance-of its forecast further by using the entire order history to which it has access. Thus, when intelligent use of already available internal information (order history) suffices, there is no need to invest in interorganizational systems for information sharing.</description><author>Raghunathan, S.</author><pubDate>Sun, 01 Apr 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A coordinated production planning model with capacity expansion and inventory management</title><link>http://www.example.com/articles/1</link><description>Rajagopalan, S.; Swaminathan, J. M.
Motivated by a problem faced by a large manufacturer of a consumer product, we explore the interaction between production planning and capacity acquisition decisions in environments with demand growth. We study a firm producing multiple items in a multiperiod environment where demand for items is known but varies over time with a long-term growth and possible short-term fluctuations. The production equipment is characterized by significant changeover time between the production of different items. While demand growth is gradual, capacity additions are discrete. Therefore, periods immediately following a machine purchase are characterized by excess machine capacity. We develop a mathematical programming model and an effective solution approach to determine the optimal capacity acquisition, production and inventory decisions over time. Through a computational study, we show the effectiveness of the solution approach in terms of solution quality and investigate the impact of product variety, cost of capital, and other important parameters on the capacity and inventory decisions. The computational results bring out some key insights-increasing product variety may not result in excessive inventory and even a substantial increase In set-up times or holding costs may not increase the total cost over the horizon in a significant manner due to the ability to acquire additional capacity We also provide solutions and insights to the real problem that motivated this work.</description><author>Rajagopalan, S.; Swaminathan, J. M.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing for a durable-goods monopolist under rapid sequential innovation</title><link>http://www.example.com/articles/1</link><description>Kornish, L. J.
A durable-goods monopolist who will be introducing new and improved versions of his product must decide how to price his products, keeping in mind the relative attractiveness of the current and future products. Dhebar (1994) has shown that if technology is changing too quickly and the producer cannot credibly commit to future prices and quality, then no equilibrium strategy exists. That is, there is no credible strategy for the future product that the producer can commit to in the first period. We show that an equilibrium pricing strategy exists if the monopolist does not offer upgrade pricing, that is, special pricing to consumers who have bought an earlier version. The author shows the possible purchase patterns in equilibrium and derives the optimal pricing strategy.</description><author>Kornish, L. J.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal operating policies for multiplant stochastic manufacturing systems in a changing environment</title><link>http://www.example.com/articles/1</link><description>Li, L.; Porteus, E. L.; Zhang, H. T.
This paper addresses the horizontal coordination between production units located in different countries within a supply chain in a changing environment. The model incorporates (1) congestion and delay through uncertainties in demand and processing times, (2) a changing production cost environment through uncertainty in an environmental state (such as the exchange rate), and (3) allowing production to stock. The main contribution of the paper is to characterize the optimal production policy, assuming no switchover cost, as a barrier-type control policy for a series of models. For each plant, there is a barrier function defined on the environmental state that gives the total worldwide stock level barrier: This plant should be operated if and only if the stock level is below this barrier. The authors also show that, in a simple setting of two plants in different countries, the barrier functions are not always monotone in the exchange rate even when the production costs are. The authors present a condition under which the control barriers are shown to be monotone. Under this condition and with only two plants, it is optimal to follow a primary/secondary plant policy in which, given a fixed environmental state, it is optimal to operate both plants when stock is low, only the primary plant when stock is moderate, and neither when stock is high. Furthermore, the roles of the plants switch as the environmental state passes through the equal production cost state. In addition, the firm seeks to carry the most stock as a hedge against future environmental state moves, when one plant is significantly cheaper than the other. The firm seeks to carry the least stock when both plants are equally costly to operate.</description><author>Li, L.; Porteus, E. L.; Zhang, H. T.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Validation of trace-driven simulation models: Bootstrap tests</title><link>http://www.example.com/articles/1</link><description>Kleijnen, J. P. C.; Cheng, R. C. H.; Bettonvil, B.
Trace-driven (or correlated inspection) simulation means that the simulated and the real systems have some common inputs (say, historical arrival times) so that the two systems' outputs are cross-correlated. To validate such a simulation, this paper focuses on the difference between the average simulated and real responses. To evaluate this validation statistic, the paper develops a novel bootstrap technique-based on replicated runs. This validation statistic and the bootstrap technique are evaluated in extensive Monte Carlo experiments with specific single-server queues. These experiments show acceptable Type-I and Type-II error probabilities.</description><author>Kleijnen, J. P. C.; Cheng, R. C. H.; Bettonvil, B.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theory of finitely durable goods monopoly with used-goods market and transaction costs</title><link>http://www.example.com/articles/1</link><description>Huang, S.; Yang, Y.; Anderson, K.
We construct a dynamic game to model a monopoly of finitely durable goods. The solution concept is Markov-perfect equilibria with general equilibria embedded in every time period. Our model is flexible enough to simultaneously explain or accommodate many commonly observed phenomena or stylized facts, such as concurrent leasing and selling, active secondary markets for used goods, heterogeneous consumers, endogenous consumption patterns, depreciation, an infinite time horizon, and nontrivial transaction costs. Within our model, consumers have incentives to segment themselves into various consumption classes according to their willingness to pay, and nontrivial transaction costs to sell used goods put strong constraints on consumers' consumption sequences in time. As a direct consequence of the finite durability, the market power of the monopolist remains intact. Leasing manifests itself as a facilitator of price discrimination by debundling the durable good into new and used portions that are naturally bundled together under outright sales. The concurrent leasing and selling reflects the degree of the comparative advantage the monopolist has over consumers in disposing used goods. This comparative advantage, which is partially exploited by the monopolist and partially shared by the consumers, provides a sufficient mechanism to gain Pareto improvement on the market.</description><author>Huang, S.; Yang, Y.; Anderson, K.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Making descriptive use of prospect theory to improve the prescriptive use of expected utility</title><link>http://www.example.com/articles/1</link><description>Bleichrodt, H.; Pinto, J. L.; Wakker, P. P.
This paper proposes a quantitative modification of standard utility elicitation procedures, such as the probability and certainty equivalence methods, to correct for commonly observed violations of expected utility. Traditionally, decision analysis assumes expected utility not only for the prescriptive purpose of calculating optimal decisions but also for the descriptive purpose of eliciting utilities. However, descriptive violations of expected utility bias utility elicitations. That such biases are effective became clear when systematic discrepancies were found between different utility elicitation methods that, under expected utility, should have yielded identical utilities. As it is not clear how to correct for these biases without further knowledge of their size or nature, most utility elicitations still calculate utilities by means of the expected utility formula. This paper speculates on the biases and their sizes by using the quantitative assessments of probability transformation and loss aversion suggested by prospect theory. It presents quantitative corrections for the probability and certainty equivalence methods. If interactive sessions to correct for biases are not possible, then the authors propose to use the corrected utilities rather than the uncorrected ones in prescriptions of optimal decisions. In an experiment, the discrepancies between the probability and certainty equivalence methods are removed by the authors' proposal.</description><author>Bleichrodt, H.; Pinto, J. L.; Wakker, P. P.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note: The newsvendor model with endogenous demand</title><link>http://www.example.com/articles/1</link><description>Dana, J. D.; Petruzzi, N. C.
This paper considers a firm's price and inventory policy when it faces uncertain demand that depends on both price and inventory level. The authors extend the classic newsvendor model by assuming that expected utility maximizing consumers choose between visiting the firm an consuming an exogenous, outside option. The outside option represents the utility the consumer forgoes when she chooses to visit the firm before-knowing whether or not the product will be available. The authors investigate both the case in which the firm's price is exogenous and the case in which price is chosen optimally. The paper makes two contributions. First, the authors show that the firm holds more inventories, provides a higher fill rate, attracts more customers, and earns higher profits when it internalizes the effect of its inventory on demand. Second, the authors show that in the endogenous price case the firm's two-dimensional decision problem can be reduced to two, sequential, single-variable optimizations. As a result, the endogenous-price case is as easy to solve as the exogenous-price case.</description><author>Dana, J. D.; Petruzzi, N. C.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On continuous-time optimal advertising under S-shaped response</title><link>http://www.example.com/articles/1</link><description>Feinberg, F. M.
Continuous-time monopolistic models of advertising expenditure that rely on strict response concavity have been shown to prescribe eventual spending at a constant rate. However, analyses of discrete analogs have suggested that S-shaped response (convexity for low expenditure levels) may allow for the periodic optima encountered in actual practice. Casting the dynamic between advertising and sales in a common format (an autonomous, first-order relationship), the present paper explores extensions along three dimensions: an S-shaped response function, the value of the discount rate, and the possibility of diffusionlike response. Supplementing the treatment by Mahajan and Muller (1986), a flexible class of S-shaped response models is formulated for which it is demonstrated that, in contrast to findings in the literature on discretized advertising models, continuous periodic optima cannot be supported. Further, a set of conditions on the advertising response function are derived, that contains and extends that suggested by Sasieni (1971). Collectively, these results both suggest a set of baseline properties that reasonable models should possess and cast doubt on the ability of first-order models to capture effects of known managerial relevance.</description><author>Feinberg, F. M.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bayesian models for early warning of bank failures</title><link>http://www.example.com/articles/1</link><description>Sarkar, S.; Sriram, R. S.
he focus of this research is to demonstrate how probabilistic models may be used to provide early warnings for bank failures. While prior research in the auditing literature has recognized the applicability of a Bayesian belief revision framework for many audit tasks, empirical evidence has suggested that auditors' cognitive decision processes often violate probability axioms. We believe that some of the well-documented cognitive limitations of a human auditor can be compensated by an automated system. In particular, we demonstrate that a formal belief revision scheme can be incorporated into an automated system to provide reliable probability estimates for early warning of bank failures. The automated system examines financial ratios as predictors of a bank's performance and assesses the posterior probability of a banks financial health (alternatively, financial distress). We examine two different probabilistic models, one that is simpler and makes more assumptions, while the other that is somewhat more complex but makes fewer assumptions. We find that both models are able to make accurate predictions with the help of historical data to estimate the required probabilities. In particular, the more complex model is found to be very well calibrated in its probability estimates. We posit that such a model can serve as a useful decision aid to an auditor's judgment process.</description><author>Sarkar, S.; Sriram, R. S.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Leveraging the customer base: Creating competitive advantage through knowledge management</title><link>http://www.example.com/articles/1</link><description>Ofek, E.; Sarvary, M.
Professional services firms (e.g., consultants, accounting firms, or advertising agencies) generate and sell business solutions to their customers. In doing so, they can leverage the cumulative experience gained from serving their customer base to either reduce their variable costs or increase the quality of their products/services. In other words, their "production technology" exhibits some form of increasing returns to scale. Growth and globalization, coupled with recent advances in information technology, have led many of these firms to introduce sophisticated knowledge management (KM) systems in order to create sustainable competitive advantage. In this paper, the authors analyze how KM is likely to affect competition among such professional services firms. In particular, they first explore what type (supply-side versus demand-side) of economies of scale are likely to be exploited in KM systems. In the former case, KM's role is to reduce the operating costs of the firm, while in the latter case, its role is to create added value to customers by significantly increasing product quality. Second, the authors analyze the competitive dynamics and market structure that emerge as a result of firms competing with KM systems. The results shed light on the current literature exploring the deployment of KM systems by suggesting that in a competitive setting, when firms' ability to leverage their customer base is high, KM should lead to quality improvement rather than cost reductions. In a dynamic setting, it is also shown that when firms use their KM system to improve product quality, higher ability to leverage the customer base may actually hurt profits and lead to industry shakeout. Beyond normative insights, the results also support a number of recent market trends in management consulting, including the increased emphasis on knowledge-creating activities in modern KM systems, the wave of mergers between consulting firms, and the recent emergence of "retail consulting" services.</description><author>Ofek, E.; Sarvary, M.</author><pubDate>Thu, 01 Nov 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance of coupled product development activities with a deadline</title><link>http://www.example.com/articles/1</link><description>Joglekar, N. R.; Yassine, A. A.; Eppinger, S. D.; Whitney, D. E.
This paper explores the performance of coupled development activities by proposing a performance generation model (PGM). The goal of the PGM is to develop insights about optimal strategies (i.e., sequential, concurrent, or overlapped) to manage coupled design activities that share a fixed amount of engineering resources subject to performance and deadline constraints. Model analysis characterizes the solution space for the coupled development problem. The solution space is used to explore the generation of product performance and the associated dynamic forces affecting concurrent development practices. We use these forces to explain conditions under which concurrency is a desirable strategy.</description><author>Joglekar, N. R.; Yassine, A. A.; Eppinger, S. D.; Whitney, D. E.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Comment on "Reducing buyer search costs: Implications for electronic marketplaces"</title><link>http://www.example.com/articles/1</link><description>Harrington, J. E.
nan</description><author>Harrington, J. E.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>What determines the shape of the probability weighting function under uncertainty?</title><link>http://www.example.com/articles/1</link><description>Kilka, M.; Weber, M.
Decision weights are an important component in recent theories of decision making under uncertainty. To better explain these decision weights, a two-stage approach has been proposed: First, the probability of an event is judged and then this probability is transformed by the probability weighting function known from decision making under risk. We extend the two-stage approach by allowing the probability weighting function to depend on the type of uncertainty. Using this more general approach, properties of decision weights can be attributed to properties of probability judgments and/or to properties of probability weighting. We present an empirical study that shows that it is indeed necessary to allow the probability weighting function to be source dependent. The analysis includes an examination of properties of the probability weighting function under uncertainty that have not been considered yet.</description><author>Kilka, M.; Weber, M.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing claims under GARCH-Level dependent interest rate processes</title><link>http://www.example.com/articles/1</link><description>Cvsa, V.; Ritchken, P.
This article considers the pricing of interest-rate-sensitive claims when the underlying interest rate is driven by a two-state-variable GARCH process. Analytical solutions are established for the case when the innovations in the short rate are normal and/or chi-squared random variables and the volatility of rates take on a special GARCH form. GARCH models that nest level-dependent interest rate models, including the Cox, Ingersoll, and Ross model, are also considered. Algorithms are provided that permit the efficient pricing of American-style interest rate claims under a rather broad array of GARCH-Level dependent processes.</description><author>Cvsa, V.; Ritchken, P.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An optimization framework for product design</title><link>http://www.example.com/articles/1</link><description>Shi, L. Y.; Olafsson, S.; Chen, Q.
An important problem in the product design and development process is to use the partworths preferences of potential customers to design a new product such that market share is maximized. The authors present a new optimization framework for this problem, the nested partitions (NP) method. This method is globally convergent and may utilize existing heuristic methods to speed its convergence. We incorporate several known heuristics into this framework and demonstrate through numerical experiments that using the NP method results in superior product designs. Our numerical results suggest that the new framework is particularly useful for designing complex products with many attributes.</description><author>Shi, L. Y.; Olafsson, S.; Chen, Q.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A two-location inventory model with transshipment and local decision making</title><link>http://www.example.com/articles/1</link><description>Rudi, N.; Kapur, S.; Pyke, D. F.
In situations where a seller has surplus stock and another seller is stocked out, it may be desirable to transfer surplus stock from the former to the latter. We examine how the possibility of such transshipments between two independent locations affects the optimal inventory orders at each location. If each location aims to maximize its own profits-we call this local decision making-their inventory choices will not, in general, maximize joint profits. We find transshipment prices which induce the locations to choose inventory levels consistent with joint-profit maximization.</description><author>Rudi, N.; Kapur, S.; Pyke, D. F.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Project contracts and payment schedules: The client's problem</title><link>http://www.example.com/articles/1</link><description>Dayanand, N.; Padman, R.
Contractual agreements have assumed significant complexity in recent times because of the emergence of strategies like outsourcing and partnering in the successful completion of large software development, manufacturing, and construction projects. A client and contractor enter into an agreement for a project either by bidding or negotiation. Effective and efficient bidding, negotiation, and subsequent monitoring are hindered by the lack of appropriate decision support tools for the management of project finances. Progress payments to the contractor are an important issue in project management because of their potential impact on project finances and activity schedules. In this paper, we consider the problem of simultaneously determining the amount, location, and timing of progress payments in projects from a client's perspective. We develop three mixed-integer linear programming models, based on some practical methods of determining payment schedules from different types of project contracts. We discuss properties of the models and draw insights about the characteristics of optimal payment schedules obtained with each model by an experimental study on a sample of 10 small projects. Our analysis shows that, contrary to current practice, the client obtains the greatest benefit by scheduling the project for early completion such that the payments are not made at regular intervals. It is also cost effective for the client to make payments either in the early stages of the project or toward the end, even though this causes considerable variation in the time gap between payments. We also evaluate the impact of the client's preferred payment schedules on the contractor's finances and activity schedules, and draw some conclusions on the interdependence of payment and project parameters on the objectives of both parties entering the contractual agreement.</description><author>Dayanand, N.; Padman, R.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of worker learning, forgetting, and heterogeneity on assembly line productivity</title><link>http://www.example.com/articles/1</link><description>Shafer, S. M.; Nembhard, D. A.; Uzumeri, M. V.
The authors investigate through several simulations how patterns of learning and forgetting affect the operating performance of an assembly line. A unique aspect of this study is that a distribution of learning/forgetting behavior based on an empirical population of workers is used rather than assuming the same learning pattern for all employees. The paper demonstrates that modeling only central tendency and not the variations across workers tends to systematically underestimate overall productivity. The data used to estimate the parameters for the distribution of learning curves were collected from an assembly line that produces car radios. Analysis of the models fit to a population of workers reveals that higher levels of previous experience are positively correlated with higher steady-state productivity levels and negatively correlated with the learning rate. To further motivate the study, a conceptual model with several factors hypothesized to influence assembly line productivity is presented. Among key factors included in the model are the rate of worker learning, the size of the worker pool, task tenure, and the magnitude of worker forgetting. In controlled computer simulation experiments, each of these factors was found to be statistically significant, as were a number of the two-way interaction terms.</description><author>Shafer, S. M.; Nembhard, D. A.; Uzumeri, M. V.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic factor market intelligence: An application of information economics to strategy formulation and competitor intelligence</title><link>http://www.example.com/articles/1</link><description>Makadok, R.; Barney, J. B.
This paper develops a model of information-acquisition decisions by firms that are competing in a "strategic factor market" (Barney 1986) to purchase a scarce resource whose value is unknown and differs across firms. The model builds on the argument that more accurate expectations about the firm-specific value of resources is, other than luck, the only way for firms to obtain the specific resources required for competitive advantage. We address the more specific question of what types of information firms should gather to accomplish this goal. The model generates a series of testable hypotheses about how a firm's optimal mix of different types of information is affected by a number of factors, including the level of uncertainty about the value of the resource being acquired; the rarity, imitability and nonsubstitutability of that resource; the level of inscrutability of firms' pre-existing stocks of resources; and firms' information-gathering and information-processing capacities.</description><author>Makadok, R.; Barney, J. B.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product variety, supply chain structure, and firm performance: Analysis of the US bicycle industry</title><link>http://www.example.com/articles/1</link><description>Randall, T.; Ulrich, K.
Using data from the U.S. bicycle industry, we examine the relation among product variety, supply chain structure, and firm performance. Variety imposes two types of costs on a supply chain: production costs and market mediation costs. Production costs include, among other costs, the incremental fixed investments associated with providing additional product variants. Market mediation costs arise because of uncertainty in product demand created by variety. In the presence of demand uncertainty, precisely matching supply with demand is difficult. Market mediation costs include the variety-related inventory holding costs, product mark-down costs occurring when supply exceeds demand, and the costs of lost sales occurring when demand exceeds supply. We analyze product variety at the product attribute level, noting that the relative impact of variety on production and market mediation costs depends to a large extent on the attribute underlying the variety. That is, some types of variety incur high production costs and some types of variety incur high market mediation costs. We characterize supply chain structure by the degree to which production facilities are scale-efficient and by the distance of the production facility from the target market. We hypothesize that firms with scale-efficient production (i.e., high-volume firms) will offer types of variety associated with high production costs, and firms with local production will offer types of variety associated with high market mediation costs. This hypothesis implies that there is a coherent way to match product variety with supply chain structure. Empirical results suggest that firms which match supply chain structure to the type of product variety they offer outperform firms which fail to match such choices.</description><author>Randall, T.; Ulrich, K.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information technology and trader competition in financial markets: Endogenous liquidity</title><link>http://www.example.com/articles/1</link><description>Dewan, S.; Mendelson, H.
This note integrates the models of Dewan and Mendelson (1998) (DM) and Kyle (1985), extending the DM analysis of time-based competition in financial markets to the case of endogenous liquidity. The results enable us to examine the link between information technology investments, trading strategies, and liquidity.</description><author>Dewan, S.; Mendelson, H.</author><pubDate>Sat, 01 Dec 2001 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Editorial objectives - Manufacturing, distribution, and service operations</title><link>http://www.example.com/articles/1</link><description>Hopp, W. J.; Lovejoy, W. S.
nan</description><author>Hopp, W. J.; Lovejoy, W. S.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on stochastic models and simulation</title><link>http://www.example.com/articles/1</link><description>Glasserman, P.
nan</description><author>Glasserman, P.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Path generation for quasi-Monte Carlo simulation of mortgage-backed securities</title><link>http://www.example.com/articles/1</link><description>Akesson, F.; Lehoczky, J. P.
Monte Carlo simulation is playing an increasingly important role in the pricing and hedging of complex, path dependent financial instruments. Low discrepancy simulation methods offer the potential to provide faster rates of convergence than those of standard Monte Carlo methods; however, in high dimensional problems special methods are required to ensure that the faster convergence rates hold. Indeed, Ninomiya and Tezuka (1996) have shown high-dimensional examples, in which low discrepancy methods perform worse than Monte Carlo methods. The principal component construction introduced by Acworth et al. (1998) provides one solution to this problem. However, the computational effort required to generate each path grows quadratically with the dimension of the problem. This article presents two new methods that offer accuracy equivalent, in terms of explained variability, to the principal components construction with computational requirements that are Linearly related to the problem dimension. One method is to take into account knowledge about the payoff function, which makes it more flexible than the Brownian Bridge construction. Numerical results are presented that show the benefits of such adjustments. The different methods are compared for the case of pricing mortgage backed securities using the Hull-White term structure model.</description><author>Akesson, F.; Lehoczky, J. P.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risk-constrained dynamic active portfolio management</title><link>http://www.example.com/articles/1</link><description>Browne, S.
Active portfolio management is concerned with objectives related to the outperformance of the return of a target benchmark portfolio. In this paper, we consider a dynamic active portfolio management problem where the objective is related to the tradeoff between the achievement of performance goals and the risk of a shortfall. Specifically, we consider an objective that relates the probability of achieving a given performance objective to the time it takes to achieve the objective. This allows a new direct quantitative analysis of the risk/return tradeoff, with risk defined directly in terms of probability of shortfall relative to the benchmark, and return defined in terms of the expected time to reach investment goals relative to the benchmark. The resulting optimal policy is a state-dependent policy that provides new insights. As a special case, our analysis includes the case where the investor wants to minimize the expected time until a given performance goal is reached subject to a constraint on the shortfall probability.</description><author>Browne, S.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Management of antiretroviral therapy for HIV infection: Analyzing when to change therapy</title><link>http://www.example.com/articles/1</link><description>D'Amato, R. M.; D'Aquila, R. T.; Wein, L. M.
We analyze two joint decisions in the management of HIV-infected patients on antiretroviral therapy: how frequently to measure a patient's virus level, and when to switch therapy. The underlying stochastic model captures the initial suppression and eventual rebound of the virus level in the blood of a typical HIV-infected patient undergoing treatment. We consider two classes of policies: a viral load policy, which triggers a change in therapy when the current virus level divided by the smallest level achieved thus far exceeds a prespecified threshold, and a proactive policy, which is similar to the viral load policy but also switches drugs at a prespecified time if no evidence of viral rebound has been seen. We find approximate analytical expressions for the probability of switching before the virus reaches its nadir (minimum value) and the mean delay in detection of viral rebound (i.e., the time interval from when the viral nadir occurs until the switch in therapy). Numerical results show that the proactive policy outperforms (i.e., a smaller detection delay for a given probability of prenadir switching) the viral load policy and recent recommendations by an expert AIDS panel, and may delay the onset of multidrug resistance in a significant proportion of patients who experience drug failure.</description><author>D'Amato, R. M.; D'Aquila, R. T.; Wein, L. M.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Variance reduction via lattice rules</title><link>http://www.example.com/articles/1</link><description>L'Ecuyer, P.; Lemieux, C.
This is a review article on lattice methods for multiple integration over the unit hypercube, with a variance-reduction viewpoint. It also contains some new results and ideas. The aim is to examine the basic principles supporting these methods and how they can be used effectively for the simulation models that are typically encountered in the area of management science. These models can usually be reformulated as integration problems over the unit hypercube with a large (sometimes infinite) number of dimensions. We examine selection criteria for the lattice rules and suggest criteria which take into account the quality of the projections of the lattices over selected low-dimensional subspaces. The criteria are strongly related to those used for selecting linear congruential and multiple recursive random number generators. Numerical examples illustrate the effectiveness of the approach.</description><author>L'Ecuyer, P.; Lemieux, C.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A heavy traffic approximation for workload processes with heavy tailed service requirements</title><link>http://www.example.com/articles/1</link><description>Resnick, S.; Samorodnitsky, G.
A system with heavy tailed service requirements under heavy load having a single server has an equilibrium waiting time distribution which is approximated by the Mittag-Leffler distribution. This fact is understood by a direct analysis of the weak convergence of a sequence of negative drift random walks with heavy right tail and the associated all time maxima of these random walks. This approach complements the recent transform view of Boxma and Cohen (1997).</description><author>Resnick, S.; Samorodnitsky, G.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price and service discrimination in queuing systems: Incentive compatibility of Gc mu scheduling</title><link>http://www.example.com/articles/1</link><description>Van Mieghem, J. A.
This article studies the optimal prices and service quality grades that a queuing system-the "firm"-provides to heterogeneous, utility-maximizing customers who measure quality by their experienced delay distributions. Results are threefold: First, delay cost curves are introduced that allow for a flexible description of a customer's quality sensitivity. Second, a comprehensive executable approach is proposed that analytically specifies scheduling, delay distributions and prices for arbitrary delay sensitivity curves. The tractability of this approach derives from porting heavy-traffic Brownian results into the economic analysis. The generalized c mu (Gc mu) scheduling rule that emerges is dynamic so that, in general, service grades need not correspond to a static priority ranking. A benchmarking example investigates the value of differentiated service. Third, the notions of grade and rate incentive compatibility (IC) are introduced to study this system under asymmetric information and are established for Gc mu scheduling when service times are homogeneous and customers atomistic. Grade IC induces correct grade choice resulting in perfect service discrimination; rate IC additionally induces centralized-optimal rates. Dynamic Gc mu scheduling exhibits negative feedback that, together with time-dependent pricing, can also yield rate incentive compatibility with heterogeneous service times. Finally, multiplan pricing, which offers all customers a menu with a choice of multiple rate plans, is analyzed.</description><author>Van Mieghem, J. A.</author><pubDate>Fri, 01 Sep 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Variance reduction techniques for estimating value-at-risk</title><link>http://www.example.com/articles/1</link><description>Glasserman, P.; Heidelberger, P.; Shahabuddin, P.
This paper describes, analyzes and evaluates an algorithm for estimating portfolio loss probabilities using Monte Carlo simulation. Obtaining accurate estimates of such loss probabilities is essential to calculating value-at-risk, which is a quantile of the loss distribution. The method employs a quadratic (''delta-gamma'') approximation to the change in portfolio value to guide the selection of effective variance reduction techniques; specifically importance sampling and stratified sampling. If the approximation is exact, then the importance sampling is shown to be asymptotically optimal. Numerical results indicate that an appropriate combination of importance sampling and stratified sampling can result in large variance reductions when estimating the probability of large portfolio losses.</description><author>Glasserman, P.; Heidelberger, P.; Shahabuddin, P.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A time-oriented branch-and-bound algorithm for resource-constrained project scheduling with generalised precedence constraints</title><link>http://www.example.com/articles/1</link><description>Dorndorf, U.; Pesch, E.; Phan-Huy, T.
Resource-constrained project scheduling with generalised precedence constraints is a very general scheduling model with applications in areas such as make-to-order production planning. We describe a time-oriented branch-and-bound algorithm that uses constraint-propagation techniques which actively exploit the temporal and resource constraints of the problem in order to reduce the search space. Extensive computational experiments with systematically generated test problems show that the algorithm solves more problems to optimality than other exact solution procedures which have recently been proposed, and that the truncated version of the algorithm is also a very good heuristic.</description><author>Dorndorf, U.; Pesch, E.; Phan-Huy, T.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic economic lot size model with perishable inventory</title><link>http://www.example.com/articles/1</link><description>Hsu, V. N.
This paper considers an economic lot size (ELS) model for perishable products where an inventory stock's deterioration rate and its carrying cost in each period depend on the age of the stock. We discuss situations where the traditional ELS models are not applicable, and propose a new model with general concave production and inventory cost functions, We explore the structural properties of the optimal solutions and use them to develop a dynamic programming algorithm which solves the problem in polynomial time. We also consider special cases of the general model which are solvable with reduced computational complexities.</description><author>Hsu, V. N.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling the impact of an outcome-oriented reimbursement policy on clinic, patients, and pharmaceutical firms</title><link>http://www.example.com/articles/1</link><description>So, K. C.; Tang, C. S.
Tackling the steep increase in drug costs is an especially important issue among many health care providers and insurers. To entice the clinics to become more cost efficient, the U.S. federal government, as well as many HMOs, have developed various cost containment initiatives recently. However, the impact of these initiatives on the patients' well-being, the clinic's profitability, and the pharmaceutical firm's profitability has nor been formally analyzed. Tn this paper we develop a mathematical model that is intended to examine the impact of a reimbursement policy for drug usage. Despite the simplistic structure of our model, the analysis enhances our understanding of the joint impact of the reimbursement policy on the patients, the clinic, and the pharmaceutical firm. Thus, our analysis can provide valuable information for evaluating the effectiveness of implementing such a reimbursement policy. In addition, we utilize the data gathered from a clinic to help support the assumptions and results of the underlying model.</description><author>So, K. C.; Tang, C. S.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Short-term variations and long-term dynamics in commodity prices</title><link>http://www.example.com/articles/1</link><description>Schwartz, E.; Smith, J. E.
In this article, we develop a two-factor model of commodity prices that allows mean-reversion in short-term prices and uncertainty in the equilibrium level to which prices revert. Although these two factors are not directly observable, they may be estimated from spot and futures prices. Intuitively, movements in prices for long-maturity futures contracts provide information about the equilibrium price level, and differences between the prices for the short- and long-term contracts provide information about short-term variations in prices. We show that, although this model does not explicitly consider changes in convenience yields over time, this short-term/long-term model is equivalent to the stochastic convenience yield model developed in Gibson and Schwartz (1990). We estimate the parameters of the model using prices for oil futures contracts and apply the model to some hypothetical oil-linked assets to demonstrate its use and some of its advantages over the Gibson-Schwartz model.</description><author>Schwartz, E.; Smith, J. E.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The inverse newsvendor problem: Choosing an optimal demand portfolio for capacitated resources</title><link>http://www.example.com/articles/1</link><description>Carr, S.; Lovejoy, W.
The classical newsvendor problem is one of optimally choosing a level of capacity to respond to a known demand distribution. The inverse newsvendor problem is one of optimally choosing a demand distribution with fixed capacity. The applications of the inverse problem include industrial settings where demand management is relatively less costly than capacity adjustments. Demand distributions are chosen from an opportunity set, which reflects the set of market opportunities for the firm. We analyze the firm's profit as a function of these demand alternatives, provide solution methods and insights, and identify inefficient and dominated distributions. We provide results when the opportunity set is known or only partially known. We extend the results to cases in which there are multiple prioritized customer classes that share the firm's productive capacity. This paper was motivated by an industrial application in a firm selling a semicommodity product into three prioritized industrial sectors. We review the application of our methods to this setting.</description><author>Carr, S.; Lovejoy, W.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sequential product positioning under differential costs</title><link>http://www.example.com/articles/1</link><description>Tyagi, R. K.
This paper examines the product positioning decisions of firms that enter a market sequentially and that have potentially different cost structures. It shows that if the first mover knows the second mover to have a lower production cost, it positions away from the most attractive location in the market; further, the larger the second-mover's cost advantage, the farther away the first mover positions from the most attractive location. The payer also models uncertainty in the first-mover's mind about the later-entrant's cost structure, and shows that an increase in this uncertainty (in the sense of mean-preserving spread) also makes the first mover position farther from the most attractive location in the market. Overall, this payer suggests that unless the first entrant in a market is certain that the later entrant will not have a superior cost structure, it may be better off leaving the best position in the market vacant and having a niche or fringe product.</description><author>Tyagi, R. K.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Perishable asset revenue management with Markovian time dependent demand intensities</title><link>http://www.example.com/articles/1</link><description>Feng, Y. Y.; Gallego, C.
Many industries face the problem of selling a fixed stock of items over a finite horizon. These industries include airlines selling seats before planes depart, hotels renting rooms before midnight, theaters selling seats before curtain time, and retailers selling seasonal items with long procurement lead times. Given a sunk investment in seats, rooms, or winter coats, the objective for these industries is to maximize revenues in excess of salvage value. When demand is price sensitive and stochastic, pricing is an effective tool to maximize expected revenues. In this paper we address the problem of deciding the optimal timing of price changes within a given menu of allowable, possibly time dependent, price paths each of which is associated with a general Poisson process with Markovian, time dependent, predictable intensities. We show that a set of variational inequalities characterize the value functions and the optimal (possibly random) time changes. In addition, we develop an efficient algorithm to compute the optimal value functions and the optimal pricing policy.</description><author>Feng, Y. Y.; Gallego, C.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Portfolio optimization under a minimax rule</title><link>http://www.example.com/articles/1</link><description>Cai, X. Q.; Teo, K. L.; Yang, X. Q.; Zhou, X. Y.
This paper provides a new portfolio selection rule. The objective is to minimize the maximum individual risk and we use an l(infinity) function as the risk measure. We provide an explicit analytical solution for the model and are thus able to Plot the entire efficient frontier. Our selection rule is very conservative. One of the features of the solution is that it does not explicitly involve the covariance of the asset returns.</description><author>Cai, X. Q.; Teo, K. L.; Yang, X. Q.; Zhou, X. Y.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exact and heuristic solutions for a shipment problem with given frequencies</title><link>http://www.example.com/articles/1</link><description>Bertazzi, L.; Speranza, M. G.; Ukovich, W.
We consider the problem of shipping several products from an origin to a destination when a discrete set of shipping frequencies is available, in such a way that the sum of the transportation and inventory costs is minimized. This problem, which is known to be NP-hard, has applications in transportation planning and in location analysis. In this paper we derive dominance rules for the problem solutions that allow a tightening of the bounds on the problem variables and improve the efficiency of a known branch-and-bound algorithm. Moreover, we present some heuristics and compare them with two different modifications of an EOQ-type algorithm for the solution of the problem with continuous frequencies.</description><author>Bertazzi, L.; Speranza, M. G.; Ukovich, W.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Nonparametric estimation of the cumulative intensity function for a nonhomogeneous Poisson process from overlapping realizations</title><link>http://www.example.com/articles/1</link><description>Arkin, B. L.; Leemis, L. M.
A nonparametric technique for estimating the cumulative intensity function of a nonhomogeneous Poisson process from one or more realizations on an interval is extended here to include realizations that overlap. This technique does not require any arbitrary parameters from the modeler, and the estimated cumulative intensity function can be used to generate a point process for simulation by inversion.</description><author>Arkin, B. L.; Leemis, L. M.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A new algebraic geometry algorithm for integer programming</title><link>http://www.example.com/articles/1</link><description>Bertsimas, D.; Perakis, G.; Tayur, S.
We propose a new algorithm for solving integer programming (IP) problems that is based on ideas from algebraic geometry. The method provides a natural generalization of the Farkas lemma for IP, leads to a way of performing sensitivity analysis, offers a systematic enumeration of all feasible solutions, and gives structural information of the feasible set of a given IP. We provide several examples that offer insights on the algorithm and its properties.</description><author>Bertsimas, D.; Perakis, G.; Tayur, S.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bounds on the opportunity cost of neglecting reoptimization in mathematical programming</title><link>http://www.example.com/articles/1</link><description>Oguz, O.
Postoptimality or sensitivity analysis are well-developed subjects in almost all branches of mathematical programming. In this note, we propose a simple formula which can toe used to get preliminary bounds on the value of this type of analysis for a specific class of mathematical programming problems. We also show that our bounds are tight.</description><author>Oguz, O.</author><pubDate>Sat, 01 Jul 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A structure-exploiting tool in algebraic modeling languages</title><link>http://www.example.com/articles/1</link><description>Fragniere, E.; Gondzio, J.; Sarkissian, R.; Vial, J. P.
A new concept is proposed for linking algebraic modeling languages with structure-exploiting solvers. SPI (Structure-Passing Interface) is a program that retrieves structure new concept is proposed for linking algebraic modeling languages with structure from an anonymous mathematical program built by an algebraic modeling language. SPI passes the special structure of the problem to an SES (Structure-Exploiting Solver). An integration of SPI and SES leads to SET (Structure-Exploiting Tool) and can be used with any algebraic modeling language. This approach relies on the idea that most exploitable block structures can be easily detected from the algebraic formulation of models. It should enable algebraic modeling languages to access the large body of algorithmic techniques which require problem structure.</description><author>Fragniere, E.; Gondzio, J.; Sarkissian, R.; Vial, J. P.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stochastic prediction in multinomial logit models</title><link>http://www.example.com/articles/1</link><description>Hsu, A.; Wilcox, R. T.
It is standard practice to form predictions from multinomial logit models by ignoring the estimation error associated with the parameter estimates and solving for the predicted endogeneous variable (market share) in terms of the exogenous variables and the point estimates of the parameters. It has long been recognized in the econometrics literature that this type of nonstochastic prediction, which ignores the sampling distribution of the parameter estimates, leads to incorrect inferences about the endogenous variable. We offer a simulation-based approach for approximating the exact stochastic prediction. We show that this approach provides very accurate approximations with minimal computation time and would be easy to implement in industrial applications.</description><author>Hsu, A.; Wilcox, R. T.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Methadone maintenance and HIV prevention: A cost-effectiveness analysis</title><link>http://www.example.com/articles/1</link><description>Zaric, G. S.; Brandeau, M. L.; Barnett, P. G.
We assess the cost-effectiveness of maintenance treatment for heroin addiction, with emphasis on its role in preventing HIV infection. The analysis is based on a dynamic compartmental model of the HIV epidemic among a population of adults, ages 18 to 44. The population is divided into nine compartments according to infection status and risk group. The model takes into account disease transmission from drug injection and sexual contacts. The health benefits of methadone maintenance and the resulting HIV infections averted are measured in terms of Life years gained and quality-adjusted life years gained. Costs considered include all health-care costs (including cost of HIV care and other health care) and the cost of methadone maintenance. The analysis shows that expanding existing methadone maintenance programs is a cost-effective health-care intervention that can play an important role in slowing the spread of HIV and improving the length and quality of life for injection drug users (IDUs), and that such expansion is cost-effective even in populations with low HIV prevalence among IDUs. Incremental expansion of methadone maintenance programs was found to have a cost-effectiveness ratio of between $9,700 and $17,200 per life year gained, and between $6,300 and $10,900 per quality-adjusted Life year gained. Although methadone maintenance treatment is provided to IDUs,the analysis shows that significant benefits accrue to non-IDU members of the population. Sensitivity analysis shows that new methadone maintenance treatment slots will be cost-effective even if they are twice as expensive and half as effective in reducing risky behavior as current methadone maintenance programs.</description><author>Zaric, G. S.; Brandeau, M. L.; Barnett, P. G.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain inventory management and the value of shared information</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.; Fisher, M.
In traditional supply chain inventory management, orders are the only information firms exchange, but information technology now allows firms to share demand and inventory data quickly and inexpensively. We study the value of sharing these data in a model with one supplier, N identical retailers, and stationary stochastic consumer demand. There are inventory holding: costs and back-order penalty costs. We compare a traditional information policy that does not use shared information with a full information policy that does exploit shared information. in a numerical study we find that supply chain costs are 2.2% lower on average with the full information policy than with the traditional information policy, and the maximum difference is 12.1%. We also develop a simulation-based lower bound over all feasible policies. The cost difference between the traditional information policy and the lower bound is an upper bound on the value of information sharing: Ln the same study, that difference is 3.4% on average, and no more than 13.8%. We contrast the value of information sharing with two other benefits of information technology, faster and cheaper order processing, which lead to shorter lead times and smaller batch sizes, respectively. In our sample, cutting lead times nearly in half reduces costs by 21% on average, and cutting batches in half reduces costs by 22% on average. For the settings we study, we conclude that implementing information technology to accelerate and smooth the physical flow of goods through a supply chain is significantly more valuable than using information technology to expand the flow of information.</description><author>Cachon, G. P.; Fisher, M.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decentralizing cross-functional decisions: Coordination through internal markets</title><link>http://www.example.com/articles/1</link><description>Kouvelis, P.; Lariviere, M. A.
A firm faces many problems that are inherently cross-functional. To solve them successfully requires the coordinated actions of many functional representatives acting in a decentralized setting. Functional managers, however, respond to their own individual incentives and may consequently fail to maximize the overall profits of the firm. We examine this issue in a setting in which the output of early actions limits the range of later actions, and we propose an incentive scheme that allows the system to be successfully decentralized. Our mechanism is based on linear transfer prices for the intermediate output that are implemented through an internal market; a market maker buys the output from one function and sells it to another. She is not obliged to sell at the same price at which she bought and may set prices solely to provide incentives. We illustrate the flexibility of the scheme by applying it to several models in the operations management literature.</description><author>Kouvelis, P.; Lariviere, M. A.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Do corporate global environmental standards create or destroy market value?</title><link>http://www.example.com/articles/1</link><description>Dowell, G.; Hart, S.; Yeung, B.
Arguments can be made on both sides of the question of whether a stringent global corporate environmental standard represents a competitive asset or Liability for multinational enterprises (MNEs) investing in emerging and developing markets. Analyzing the global environmental standards of a sample of U.S.-based MNEs in relation to their stock market performance, we find that firms adopting a single stringent global environmental standard have much higher market values, as measured by Tobin's q, than firms defaulting to less stringent, or poorly enforced host country standards. Thus, developing countries that use lax environmental regulations to attract foreign direct investment may end up attracting poorer quality, and perhaps less competitive, firms. Our results also suggest that externalities are incorporated to a significant extent in firm valuation. We discuss plausible reasons for this observation.</description><author>Dowell, G.; Hart, S.; Yeung, B.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Market information and firm performance</title><link>http://www.example.com/articles/1</link><description>Raju, J. S.; Roy, A.
he value of new information depends on how accurate the information is, but it may also depend on the characteristics of the firm and the nature of the industry it operates in. We develop a game-theoretic model to understand how firm and industry characteristics moderate the effect of market information on firm profits. Our results suggest that information is more valuable when product substitutability is higher, suggesting that information is of greater value in more competitive industries. Our results also suggest that although industry size does not affect the value of information, information is more valuable for larger firms. We find that, except under some conditions, more precise market information has a greater impact on profits in a Stackelberg mode of conduct than in a Bertrand-Nash mode.</description><author>Raju, J. S.; Roy, A.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An empirical analysis of process industry transformation systems</title><link>http://www.example.com/articles/1</link><description>Dennis, D.; Meredith, J.
recess industries share many characteristics because their transformation systems are designed for nondiscrete materials. Hence, the process industries typically are lumped together in a general group and contrasted from the discrete industries as a whole. The result is a poor understanding of the differences between distinct types of process industries. In this article, 19 different process industry sites are analyzed for the purpose of identifying the key differences between their transformation systems. Using cluster analysis, seven major subtypes of process industries are identified within the sample: (1) process job shop, (2) fast batch, (3) custom blending, (4) stock hybrid, (5) custom hybrid, (6) multistage continuous, and (7) rigid continuous. It is shown how these seven subtypes differ on the composite dimensions of (1) materials diversity, (2) equipment, (3) materials movement, and (4) run time. The research and managerial implications of these results are discussed.</description><author>Dennis, D.; Meredith, J.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Assessing dependence: Some experimental results</title><link>http://www.example.com/articles/1</link><description>Clemen, R. T.; Fischer, G. W.; Winkler, R. L.
Constructing decision- and risk-analysis probability models often requires measures of dependence among variables.;Although data are sometimes available to estimate such measures, in many applications they must be obtained by means of subjective judgment by experts. We discuss two experimental studies that compare the accuracy of six different methods for assessing dependence. Our results lead to several conclusions: First, simply asking experts to report a correlation is a reasonable approach. Direct estimation-is more accurate than the other methods studied, is not prone to mathematically inconsistent responses (as are some other measures), and is judged to be less difficult than alternate methods. In addition, directly assessed correlations showed less variability than the correlations derived from other assessment methods. Our results also show that experience with the variables can improve performance somewhat, as can training in a given assessment method. Finally, if a judge uses several different assessment methods, an average of the resulting estimates can also lead to better performance.</description><author>Clemen, R. T.; Fischer, G. W.; Winkler, R. L.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analytical valuation of American-style Asian options</title><link>http://www.example.com/articles/1</link><description>Hansen, A. T.; Jorgensen, P. L.
This article derives the first analytical pricing formulas for American-style Asian options of the so-called floating strike type. Geometric as well as arithmetic averaging Is considered. The setup is a standard Black-Scholes framework where the price of the underlying security evolves according to a geometric Brownian motion. A decomposition result that splits up the value of the floating strike American option into the price of an otherwise equivalent European option and an early exercise premium is first presented. This decomposition result is then manipulated further for the two separate types of averaging. With geometric averaging we derive an exact pricing formula, whereas with arithmetic averaging we develop an analytical approximation formula that proves to be very precise. Numerical examples are provided.</description><author>Hansen, A. T.; Jorgensen, P. L.</author><pubDate>Tue, 01 Aug 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Assessing the construct validity of risk attitude</title><link>http://www.example.com/articles/1</link><description>Pennings, J. M. E.; Smidts, A.
Two major approaches to measuring risk attitude are compared. One, based on the expected I utility model is derived from responses to lotteries and direct scaling. The other measure is a psychometric approach based on Likert statements that produces a unidimensional risk attitude scale. The data are from computer-assisted interviews of 346 Dutch owner-managers of hog farms, who made decisions about their own businesses. While the measures demonstrate some degree of convergent validity, those measures based on lotteries were better predictors of actual market behavior. In contrast the psychometric scale showed more agreement to self-reported measures of innovativeness, market orientation, and the intention to reduce risk. In light of the higher predictive validity of lottery-based measurements, we recommend elicitation methods based on the expected utility paradigm.</description><author>Pennings, J. M. E.; Smidts, A.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing the delivery of dialysis therapy: A multiclass fluid model analysis</title><link>http://www.example.com/articles/1</link><description>Zenios, S. A.; Fuloria, P. C.
Motivated by the exceptionally high mortality statistics of dialysis patients and the ongoing debate about the adequacy of the current reimbursement for dialysis in the United States, ave pursue a detailed analysis of the dialysis delivery system. The analysis is based on a multiclass fluid model for the dialysis facility, which combines a pharmacokinetics model of dialysis and an empirically validated model of dialysis-specific mortality. Assuming that the facility operates under budget and capacity constraints, our analysis determines the main factors that affect the delivery of dialysis. Numerical results, which are representative of the dialysis environment in the US, demonstrate the accuracy of the model and provide concrete insights about the operations of the dialysis facility. A major finding is that an improvement in the technology of dialysis is likely to have a more substantial impact on the overall Life expectancy of the dialysis population as compared to an increase in the dialysis reimbursement rate.</description><author>Zenios, S. A.; Fuloria, P. C.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competing with new product technologies: A process model of strategy</title><link>http://www.example.com/articles/1</link><description>Das, S. S.; Van de Ven, A. H.
This paper draws upon research in the economics of technical change and in the social construction of technology to develop and test a process model of strategy. We conducted a longitudinal study of leading firms that were sponsoring new and competing product technologies in two industries: the videoplayer industry and the medical diagnostic imaging industry. We built original datasets on the actions of these firms, and then empirically examined the strategy process. Our findings indicate that the nature of the product technology, whether novel or evolved, and the market, whether concentrated or dispersed, influences when firms use technical or institutional strategies to get their new product technology accepted by the market.</description><author>Das, S. S.; Van de Ven, A. H.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Determinants of user innovation and innovation sharing in a local market</title><link>http://www.example.com/articles/1</link><description>Morrison, P. D.; Roberts, J. H.; von Hippel, E.
It is known that end users of products and services sometimes innovate, and that innovations developed by users sometimes become the basis for important new commercial products and services. It has also been argued and to some extent shown that such innovations will be found concentrated in a "lead user" segment of the user community. However, neither the characteristics of innovating users nor the scope of the community that they "lead" has been explored in depth. In this paper, we explore the characteristics of innovation, innovators, and innovation sharing by library users of OPAC information search systems in Australia. This market has capable users, but it is nonetheless clearly a "follower" with respect to worldwide technological advance. We find that 26% of users in this local market nonetheless do modify their OPACs in both major and minor ways, and that OPAC manufacturers judge many of these user modifications to be of commercial interest. We find that we can distinguish modifying from nonmodifying users on the basis of a number of factors, including their "leading-edge status" and their in-house technical capabilities. We find that many innovating users freely share their innovations with others, and find that we can distinguish users that share information about their modifications from users that do not. We conclude by considering some implications of our findings for idea generation practices in marketing.</description><author>Morrison, P. D.; Roberts, J. H.; von Hippel, E.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information and incentive effects of inventory in JIT production</title><link>http://www.example.com/articles/1</link><description>Alles, M.; Amershi, A.; Datar, S.; Sarkar, R.
This paper provides an economic rationale for modern manufacturing control practices such as the minimal inventories in Just in Time (JIT) systems, zero-defect policies, and continuous improvement. The popular and academic literature contains descriptive studies on the mechanics of these systems and their perceived benefits. We use a model of production to analyze both informational and incentive rationales for reduced inventories. A TIT-like environment of low inventory levels is optimal in our model because it helps workers to better observe and understand the production process and to think and act creatively to improve operational reliability and yields. Empirical evidence using data obtained from 116 plants worldwide supports our conclusions about the effect of reduced inventories on process reliability, product quality, and cost.</description><author>Alles, M.; Amershi, A.; Datar, S.; Sarkar, R.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient risk sharing: The last frontier</title><link>http://www.example.com/articles/1</link><description>Pratt, J. W.
When rational risk-averse agents must choose among and share monetary risks, it is known that efficient sharing is typically nonlinear, even with common beliefs. Wherever it is, the sharing rule may affect the choice, randomized choice may allow everyone to gain, and indeed a randomized choice between unacceptable risks may be acceptable. An important exception occurs if the agents' utility functions are all exponential, all logarithmic, or all the same power (HARA). Then choices should accord with a group utility function of the same form independent of the sharing rule, randomization never helps, and ail efficient sharing rules are linear. This self-contained paper simplifies, refines, and completes earlier analyses, identifying all exceptions; they are the linear sharing rules that make the agents' utilities agree. Aside from HARA, this can only occur for precisely one linear sharing rule or, in periodic versions of HARA, countably many.</description><author>Pratt, J. W.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordinating expertise in software development teams</title><link>http://www.example.com/articles/1</link><description>Faraj, S.; Sproull, L.
Like all teams, knowledge teams must acquire and manage critical resources in order to accomplish their work. The most critical resource for knowledge teams is expertise, or specialized skills and knowledge, but the mere presence of expertise on a team is insufficient to produce high-quality work. Expertise must be managed and coordinated in order to leverage its potential. That is, teams must be able to manage their skill and knowledge interdependencies effectively through expertise coordination, which entails knowing where expertise is located, knowing where expertise is needed, and bringing needed expertise to bear. This study investigates the importance of expertise coordination through a cross-sectional investigation of 69 software development teams. The analysis reveals that expertise coordination shows a strong relationship with team performance that remains significant over and above team input characteristics, presence of expertise, and administrative coordination.</description><author>Faraj, S.; Sproull, L.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Decision support for a housing mobility program using a multiobjective optimization model</title><link>http://www.example.com/articles/1</link><description>Johnson, M. P.; Hurter, A. P.
As result of public housing reform and welfare reform, the operating environment of public housing authorities has changed significantly. Given these policy initiatives, housing mobility programs represent viable strategies for providing public housing residents with access to economically healthy, integrated neighborhoods. In this paper we present a decision support methodology to assist the design of housing mobility programs. This methodology incorporates economic models for estimating dollar-valued impacts associated with tenant relocation, and a multiobjective optimization model for generating alternative relocation schemes associated with various objective function weights. Using data for Lake County, Illinois and Chicago, we demonstrate that nondominated allocations represent significant trade-offs between dollar-valued and non-dollar-valued policy objectives; existing distributions of subsidized housing represent suboptimal solutions to the housing relocation problem; and increases in available rental housing can result in housing dispersion schemes that have positive net economic benefits relative to the status quo.</description><author>Johnson, M. P.; Hurter, A. P.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The importance of organizational structure for the adoption of innovations</title><link>http://www.example.com/articles/1</link><description>DeCanio, S. J.; Dibble, C.; Amir-Atefi, K.
Organizational structure affects both the overall behavior of firms and the situations of individuals and subunits within firms. The effect of exogenous changes in the environment (market prices, costs, or regulations) on organizations can be partitioned into the immediate direct effect of the change and the full effect after organizational structure has had time to adjust. This paper develops a computational model of the diffusion of a profitable innovation through a firm, and uses numerical simulations to calculate the relative importance of the direct and structural adjustment components of changes in profitability. One finding is that a failure to recognize the importance of organizational structure on the performance of firms will lead to serious bias in estimation of the costs or benefits of a change in external circumstances. The type of network model developed also has implications for the individuals and divisions that make up the firm. We examine some of the structural characteristics of well-adapted organizations, and show that asymmetries and economic inequalities emerge even when the individual agents' personal characteristics are identical.</description><author>DeCanio, S. J.; Dibble, C.; Amir-Atefi, K.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interactive multiobjective group decision making with interval parameters</title><link>http://www.example.com/articles/1</link><description>Xanthopulos, Z.; Melachrinoudis, E.; Solomon, M. M.
This paper proposes a new framework for the solution of interactive multiobjective group decision-making problems with interval parameters. Its novelty stems from a learning phase where decision makers (DMs) explore the structural characteristics of the specific Multiple Criteria Decision Making (MCDM) problem. This provides important and timely feedback to the DMs. Its core consists of four indices and their relationships. The solution framework consists of three stages. In the first, each DM provides the limits of variation for each problem parameter. These are subsequently combined into a unique interval of variation. Then, the stochastic multiobjective problem is transformed into a deterministic one. In the second stage, DMs use the four MCDM characteristics to familiarize themselves with the problem before expressing their preferences for nondominated solutions. The DMs are then guided through an interactive procedure to find their best nondominated solutions. In the last stage, all best nondominated solutions provided by the DMs are combined using a twofold approach to find the best-compromise nondominated solution. This final choice represents the opinion of the group of DMs. Our results show that the learning phase is beneficial to DMs in judging the quality of solutions, leading to better informed decisions.</description><author>Xanthopulos, Z.; Melachrinoudis, E.; Solomon, M. M.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordination of pricing and multiple-period production across multiple constant priced goods</title><link>http://www.example.com/articles/1</link><description>Gilbert, S. M.
This paper addresses the problem of jointly determining prices and production schedules for a set of items that are produced on the same production equipment. Under the assumptions that the production setup costs are negligible and that demand is seasonal but price dependent, we exploit the special structure of the problem to develop a solution procedure. Through a set of numerical examples, we demonstrate how a product's contribution to aggregate seasonality can increase its optimal price. Our examples also demonstrate that, among products that experience demand peaks during the firm's busy season, those that peak early in the busy season should be priced more aggressively than those that peak later.</description><author>Gilbert, S. M.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price-endings when prices signal quality</title><link>http://www.example.com/articles/1</link><description>Stiving, M.
This paper provides a theoretical explanation for why firms behave as though they use round prices to signal quality. By replacing the linear demand curve in Bagwell and Riordan's (1991) price as a signal of quality model with a kinked demand curve, and analyzing what price endings firms are most likely to use, the following observations can be made: (1) Firms that are using high prices to signal quality are more likely to set those prices at round numbers, and (2) price-endings themselves are not necessarily signals of quality. A simulation was conducted to demonstrate that these findings generally hold true even in the presence of demand spikes at 9-ending prices (e.g., Schindler and Kibarian 1996). Finally, empirical evidence is provided to demonstrate that firms tend to use more round prices for higher-quality products, and that this relationship is even stronger for product categories where consumers are less able to detect the true level of quality prior to purchase.</description><author>Stiving, M.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal models for meal-break and start-time flexibility in continuous tour scheduling</title><link>http://www.example.com/articles/1</link><description>Brusco, M. J.; Jacobs, L. W.
This paper presents a compact integer-programming model for large-scale continuous tour scheduling problems that incorporate meal-break window, start-time band, and start-time interval policies. For practical scheduling environments, generalized set-covering formulations (GSCFs) of such problems often contain hundreds of millions of integer decision variables, usually precluding identification of optimal solutions. As an alternative, we present an implicit integer-programming model that frequently has fewer than 1,500 variables and can be formulated and solved using PC-based hardware and software platforms. An empirical study using labor-requirement distributions for customer service representatives at a Motorola, Inc. call center was used to demonstrate the importance of having a model that can evaluate tradeoffs among the various scheduling policies.</description><author>Brusco, M. J.; Jacobs, L. W.</author><pubDate>Fri, 01 Dec 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cross-functional influence in new product development: An exploratory study of marketing and R&amp;D perspectives</title><link>http://www.example.com/articles/1</link><description>Atuahene-Gima, K.; Evangelista, F.
Previous research in new product development (NPD) has focused on the participation of marketing and R&amp;D personnel, but little attention has been paid to their influence. This study examined the effects of marketing's and R&amp;D's influence and participation on new product performance and the differential effects of personal, new product, and organizational factors on their influence in the NPD process as seen from each other's perspective. The results suggest that marketing's and R&amp;D's self-reported influence and their influence as reported by the other have a differential impact on new product performance. Unlike R&amp;D's participation as perceived by marketing, marketing's participation as perceived by R&amp;D affects new product performance only when its influence is high. The results also suggest that marketing's and R&amp;D's influence in the NPD process is differentially affected by personal, new product, and organizational factors.</description><author>Atuahene-Gima, K.; Evangelista, F.</author><pubDate>Sun, 01 Oct 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Centralization vs. decentralization in a multi-unit organization: A computational model or a retail chain as a multi-agent adaptive system</title><link>http://www.example.com/articles/1</link><description>Chang, M. H.; Harrington, J. E.
A computational model of a retail chain is developed in which store managers continually search for better practices. Search takes place over a rugged landscape defined over the space of store practices. The main objective of this research is to determine how the amount of discretion given to store managers as to how they run their stores influences the rate of innovation at the store level. We find that greater decentralization enhances firm performance when stores' markets are sufficiently different, the horizon is sufficiently long, and markets are sufficiently stable.</description><author>Chang, M. H.; Harrington, J. E.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Drive out fear (unless you can drive it in): The role of agency and job security in process improvement</title><link>http://www.example.com/articles/1</link><description>Repenning, N. P.
Understanding the wide range of outcomes achieved by firms trying to implement Total Quality Management (TQM) and similar process improvement initiatives presents a challenge to management theory: A few firms reap sustained benefits from their programs, but most efforts fail and are abandoned. In this paper I study one dimension of this phenomenon: the role of impending layoffs in determining employee commitment to process improvement. Currently, the literature provides two opposing theories concerning the effect of job security on the ability of firms to implement change initiatives. The "Drive Out Fear" school argues that firms must commit to job security, while the ''Drive In Fear'' school emphasizes the positive role that insecurity plays in motivating change. In this study a contract theoretic model is developed to analyze the role of agency in process improvement. The main insight of the study is that there are two types of job security, internal and external, that have opposing impacts on the firm's ability to implement improvement initiatives. The distinction is useful in explaining the results of different case studies and can reconcile the two change theories.</description><author>Repenning, N. P.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient supply contracts for fashion goods with forecast updating and two production modes</title><link>http://www.example.com/articles/1</link><description>Donohue, K. L.
We examine the problem of developing supply contracts that encourage proper coordination of forecast information and production decisions between a manufacturer and distributor of high fashion, seasonal products operating in a two-mode production environment. The first production mode is relatively cheap but requires a long lead time while the second is expensive but offers quick turnaround. We focus on contracts of the form (w(1), W-2, b) where w(i) is the wholesale price offered for production mode i and b is a return price offered for items left over at the end of the season. We find that such a contract can coordinate the manufacturer and distributor to act in the best interest of the channel. The pricing conditions needed to ensure an efficient solution vary depending on the degree of demand forecast improvement between periods and the manufacturer's access to forecast information. We also examine whether these conditions ensure a Pareto optimal solution with respect to two traditional production settings.</description><author>Donohue, K. L.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory management of remanufacturable products</title><link>http://www.example.com/articles/1</link><description>Toktay, L. B.; Wein, L. M.; Zenios, S. A.
We address the procurement of new components for recyclable products in the context of Kodak's single-use camera. The objective is to find an ordering policy that minimizes the total expected procurement, inventory holding, and lost sales cost. Distinguishing characteristics of the system are the uncertainty and unobservability associated with return flows of used cameras. We model the system as a closed queueing network, develop a heuristic procedure for adaptive estimation and control, and illustrate our methods with disguised data from Kodak. Using this framework, we investigate the effects of various system characteristics such as informational structure, procurement delay, demand rate, and length of the product's life cycle.</description><author>Toktay, L. B.; Wein, L. M.; Zenios, S. A.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A two-echelon repairable inventory system with stocking-center-dependent depot replenishment lead times</title><link>http://www.example.com/articles/1</link><description>Wang, Y. Z.; Cohen, M. A.; Zheng, Y. S.
Consider a two-echelon repairable inventory system consisting of a central depot and multiple stocking centers. The centers provide parts replacement service to customers and replenish their inventory from the depot, following a one-for-one policy. The depot fills center replenishment orders on a first-come-first-serve basis. Defectives received at the centers are passed to the depot for repair and depot inventory replenishment. For this system, existing models (e.g., the METRIC model) usually assume that the depot replenishment lead times (DRLTs) are i.i.d., which however, does not fit well into the service parts logistics system that motivated this research. Because the DRLTs consist of the sum of repair times, defective return times, and transportation times, they are different across stocking centers, which are located globally. We study the impact of such center-dependent DRLTs on system performance. We derive probability distributions of the random delays at the depot experienced by center replenishment orders. We prove that a center with shorter DRLTs experiences shorter delays, and therefore, delivers better customer service. We show that for such systems, using the i.i.d. DRLT assumption introduces errors in estimating system performance. These errors become significant when both the demand rates and the depot planned inventory level are low.</description><author>Wang, Y. Z.; Cohen, M. A.; Zheng, Y. S.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The gas transmission problem solved by an extension of the simplex algorithm</title><link>http://www.example.com/articles/1</link><description>De Wolf, D.; Smeers, Y.
The problem of distributing gas through a network of pipelines is formulated as a cost minimization subject to nonlinear flow-pressure relations, material balances, and pressure bounds. The solution method is based on piecewise linear approximations of the nonlinear flow-pressure relations. The approximated problem is solved by an extension of the Simplex method. The solution method is tested on real-world data and compared with alternative solution methods.</description><author>De Wolf, D.; Smeers, Y.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inducing performance in a queue via prices: The case of a riverine port</title><link>http://www.example.com/articles/1</link><description>Dasgupta, A.; Ghosh, M.
To optimize large-scale queuing systems configurations, OR professionals typically use discrete event simulation packages to examine in detail the movement of entities through such systems, assuming stochastic but fixed arrival patterns. Demand aspects are, however, routinely ignored as few attempts are made to capture the feedback effect of queue performance on the arrival process. Econometricians, on the other hand, use a simultaneous equations estimation approach relying on past data, but they typically disregard the technological insights provided by simulation. This paper combines both tools to study the ailing port system of Calcutta, India, and concludes that raising prices will improve both economic and engineering performances. Microeconomic models of shipowner behavior are constructed to explain the nature of the empirical findings. Finally, full-equilibrium demand elasticities are calculated using the dual prices from an appropriate nonlinear program, which are then compared to the benchmark value expected of profit-maximizing behavior.</description><author>Dasgupta, A.; Ghosh, M.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A parameter-free elicitation of the probability weighting function in medical decision analysis</title><link>http://www.example.com/articles/1</link><description>Bleichrodt, H.; Pinto, J. L.
An important reason why people violate expected utility theory is probability weighting. Previous studies on the probability weighting function typically assume a specific parametric form, exclude heterogeneity in individual preferences, and exclusively consider monetary decision making. This study presents a method to elicit the probability weighting function in rank-dependent expected utility theory that makes no prior assumptions about the functional form of the probability weighting function. We use both aggregate and individual subject data, thereby allowing for heterogeneity of individual preferences, and we examine probability weighting in a new domain, medical decision making. There is significant evidence of probability weighting both at the aggregate and at the individual subject level. The modal probability weighting function is inverse S-shaped, displaying both lower subadditivity and upper subadditivity. Probability weighting is in particular relevant at the boundaries of the unit interval. Compared to studies involving monetary outcomes, we generally find more elevation of the probability weighting function. The robustness of the empirical findings on probability weighting indicates its importance. Ignoring probability weighting in modeling decision under risk and in utility measurement is likely to lead to descriptively invalid theories and distorted elicitations.</description><author>Bleichrodt, H.; Pinto, J. L.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Parameter-free elicitation of utility and probability weighting functions</title><link>http://www.example.com/articles/1</link><description>Abdellaoui, M.
This paper proposes a two-step method to successively elicit utility functions and decision weights under rank-dependent expected utility theory and its "more descriptive" version: cumulative prospect theory. The novelty of the method is that it is parameter-free, and thus elicits the whole individual preference functional without imposing any prior restriction. This method is used in an experimental study to elicit individual utility and probability weighting functions for monetary outcomes in the gain and loss domains. Concave utility functions are obtained for gains and convex utility functions for losses. The elicited weighting functions satisfy upper and lower subadditivity and are consistent with previous parametric estimations. The data also show that the probability weighting function for losses is more "elevated" than for gains.</description><author>Abdellaoui, M.</author><pubDate>Wed, 01 Nov 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note. Decomposing technical efficiency with care</title><link>http://www.example.com/articles/1</link><description>Fare, R.; Grosskopf, S.
nan</description><author>Fare, R.; Grosskopf, S.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal control of drug epidemics: Prevent and treat - But not at the same time?</title><link>http://www.example.com/articles/1</link><description>Behrens, D. A.; Caulkins, J. P.; Tragler, G.; Feichtinger, G.
Drug use and related problems change substantially over time, so it seems plausible that drug interventions should vary too. To investigate this possibility, we set up a continuous time version of the first-order difference equation model of cocaine use introduced by Everingham and Rydell (1994), extended to make initiation an endogenous function of prevalence. We then formulate and solve drug treatment and prevention spending decisions in the framework of dynamic optimal control under different assumptions about how freely drug control budgets can be manipulated. Insights include: (1) The effectiveness of prevention and treatment depend critically on the stage in the epidemic in which they are employed. Prevention is most appropriate when there are relatively few heavy users, e.g. in the beginning of an epidemic. Treatment is more effective later. (2) Hence, the optimal mix of interventions varies over time. (3) The transition period when it is optimal to use extensively both prevention and treatment is brief. (4) Total social costs increase dramatically if control is delayed.</description><author>Behrens, D. A.; Caulkins, J. P.; Tragler, G.; Feichtinger, G.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The optimal choice of promotional vehicles: Front-loaded or rear-loaded incentives?</title><link>http://www.example.com/articles/1</link><description>Zhang, Z. J.; Krishna, A.; Dhar, S. K.
We examine the key factors that influence a firm's decision whether to use front-loaded or rear-loaded incentives. When using price packs, direct mail coupons, FSI coupons or peel-off coupons, consumers obtain an immediate benefit upon purchase or a front-loaded incentive. However, when buying products with in-pack coupons or products affiliated with loyalty programs, promotion incentives are obtained on the next purchase occasion or later, i.e., a rear-loaded incentive. Our analysis shows that the innate choice process of consumers in a market (variety-seeking or inertia) is an important determinant of the relative impact of front-loaded and rear-loaded promotions. While in both variety-seeking and inertial markets, the sales impact and the sales on discount are higher for front-loaded promotions than for rear-loaded promotions, from a profitability perspective, rear-loaded promotions may be better than front-loaded promotions. We show that in markets with high variety-seeking it is more profitable for a firm to rear-load, and in markets with high inertia it is more profitable to front-load. Model implications are verified using two empirical studies: (a) a longitudinal experiment (simulating markets with variety-seeking consumers and inertial consumers) and (b) market data on promotion usage. The data in both studies are consistent with the model predictions.</description><author>Zhang, Z. J.; Krishna, A.; Dhar, S. K.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Telecommunication node clustering with node compatibility and network survivability requirements</title><link>http://www.example.com/articles/1</link><description>Park, K.; Lee, K.; Park, S.; Lee, H.
We consider the node clustering problem that arises in designing a survivable two-level telecommunication network. The problem simultaneously determines an optimal partitioning of the whole network into clusters (local networks) and hub locations in each cluster. Intercluster traffic minimization is chosen as the clustering criterion to improve the service quality. Various constraints on the clustering are considered which reflect both the physical structures of local networks, such as the connectivity requirement, and the node compatibility relations such as community of interest or policy. Additional constraints may be imposed on the hub selection to ensure network survivability. We propose an integer programming formulation of the problem by decomposing the entire problem into a master problem and a number of column generation problems. The master problem is solved by column generation and the column generation problems by branch-and-cut. We develop and use strong cutting-planes for the cluster generation subproblems. Computational results using real data are reported.</description><author>Park, K.; Lee, K.; Park, S.; Lee, H.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal dynamic pricing for perishable assets with nonhomogeneous demand</title><link>http://www.example.com/articles/1</link><description>Zhao, W.; Zheng, Y. S.
We consider a dynamic pricing model for selling a given stock of a perishable product over a finite time horizon. Customers, whose reservation price distribution changes over time, arrive according to a nonhomogeneous Poisson process. We show that at any given time, the optimal price decreases with inventory. We also identify a sufficient condition under which the optimal price decreases over time for a given inventory level. This sufficient condition requires that the willingness of a customer to pay a premium for the product does not increase over time. In addition to shedding managerial insight, these structural properties enable efficient computation of the optimal policy. Numerical studies are conducted to show the revenue impact of dynamic price policies. Price changes are set to compensate for statistical fluctuations of demand and to respond to shifts of the reservation price. For the former, our examples show that using optimal dynamic optimal policies achieves 2.4-7.3% revenue improvement over the optimal single price policy. For the latter, the revenue increase can be as high as 100%. These results explain why yield management has become so essential to fashion retailing and travel service industries.</description><author>Zhao, W.; Zheng, Y. S.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Preference factoring for stochastic trees</title><link>http://www.example.com/articles/1</link><description>Hazen, G.
Stochastic trees are extensions of decision trees that facilitate the modeling of temporal uncertainties. Their primary application has been to medical treatment decisions. It is often convenient to present stochastic trees in factored form, allowing loosely coupled pieces of the model to be formulated and presented separately. Ln this paper, we show how the notion of factoring can be extended as well to preference components of the stochastic model. We examine updateable-state utility, a flexible class of expected utility models that permit stochastic trees to be rolled back much in the manner of decision trees. We show that preference summaries for updateable-state utility can be factored out of the stochastic tree. In addition, we examine utility decompositions which can arise when factors in a stochastic tree are treated as attributes in a multiattribute utility function.</description><author>Hazen, G.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>