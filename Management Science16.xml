<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Management Science16</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:07:42 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Decision bias in the newsvendor problem with a known demand distribution: Experimental evidence</title><link>http://www.example.com/articles/1</link><description>Schweitzer, M. E.; Cachon, G. P.
In the newsvendor problem a decision maker orders inventory before a one period selling season with stochastic demand. If too much is ordered, stock is left over at the end of the period, whereas if too little is ordered, sales are lost. The expected profit-maximizing order quantity is well known, but little is known about how managers actually make these decisions. We describe two experiments that investigate newsvendor decisions across different profit conditions. Results from these studies demonstrate that choices systematically deviate from these that maximize expected profit. Subjects order too few of high-profit products and too many of low-profit products. These results are not consistent with risk-aversion, risk-seeking preferences, Prospect Theory preferences, waste aversion, stockout aversion, or the consequences of underestimating opportunity costs. Two explanations are consistent with the data. One, subjects behave as if their utility function incorporates a preference to reduce ex-post inventory error, the absolute difference between the chosen quantity and realized demand. Two, subjects suffer from the anchoring and insufficient adjustment bias. Feedback and training did not mitigate inventory order errors. We suggest techniques to improve decision making.</description><author>Schweitzer, M. E.; Cachon, G. P.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Synchronous unpaced flow lines with worker differences and overtime cost</title><link>http://www.example.com/articles/1</link><description>Doerr, K. H.; Klastorin, T. D.; Magazine, M. J.
In this paper, we consider the design of a synchronous, unpaced how line where workers operate at different skill levels and overtime is used, if necessary, to meet a daily production quota. The line is unpaced in the sense that items only move to the next workstation when all workers on the line have completed their respective tasks. The design problem in this case is to assign both workers and tasks to workstations to minimize the expected sum of regular and overtime costs. To solve this problem, we develop an optimization algorithm for smaller problems and a heuristic algorithm for larger problems, which we use to investigate the sensitivity of total expected cost to changes in the price of overtime, hiring practices, worker differences, and the overall amount of work time variability. Based on an extensive computational analysis, we found that (1) planned overtime is frequently beneficial, (2) more workers should be hired as worker variability increases, and (3) increases in overtime costs frequently yield a relatively lower percentage increase in total expected cost. Other managerial implications are discussed in the paper.</description><author>Doerr, K. H.; Klastorin, T. D.; Magazine, M. J.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quantifying the bullwhip effect in a simple supply chain: The impact of forecasting, lead times, and information</title><link>http://www.example.com/articles/1</link><description>Chen, F.; Drezner, Z.; Ryan, J. K.; Simchi-Levi, D.
An important observation in supply chain management, known as the bullwhip effect, suggests that demand variability increases as one moves up a supply chain In this payer we quantify this effect for simple, two-stage supply chains consisting of a single retailer and a single manufacturer. Our model includes two of the factors commonly assumed to cause the bullwhip effect: demand forecasting and order lead times. We extend these results to multiple-stage supply chains with and without centralized customer demand information and demonstrate that the bullwhip effect can be reduced, but not completely eliminated, by centralizing demand information.</description><author>Chen, F.; Drezner, Z.; Ryan, J. K.; Simchi-Levi, D.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A supplier's optimal quantity discount policy under asymmetric information</title><link>http://www.example.com/articles/1</link><description>Corbett, C. J.; de Groote, X.
In the supply-chain literature, an increasing body of work studies how suppliers can use incentive schemes such as quantity discounts to influence buyers' ordering behaviour, thus reducing the supplier's (and the total supply chain's) costs. Various functional forms for such incentive schemes have been proposed, but a critical assumption always made is that the supplier has full information about the buyer's cost structure. We derive the optimal quantity discount policy under asymmetric information and compare it to the situation where the supplier has full information.</description><author>Corbett, C. J.; de Groote, X.</author><pubDate>Wed, 01 Mar 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling intercategory and generational dynamics for a growing information technology industry</title><link>http://www.example.com/articles/1</link><description>Kim, N.; Chang, D. R.; Shocker, A. D.
Previous studies dealing with product growth have dealt only with substitution effects among successive generations of one product category and not with complementarity and competition provided by related product categories. Based on a broadened concept of the competitive information technology (IT) market, we develop a dynamic market growth model that is able to incorporate both interproduct category and technological substitution effects simultaneously. The market potential for each category or generation is treated as a variable rather than a constant parameter, which is typical of recently growing IT sectors such as wireless telecommunications. The model is calibrated, its plausibility discussed, and its face and predictive validity assessed using data on wireless telecommunications services from two Asian markets. Results show that the market potential (and sales growth) of one category or generation is significantly affected by others and by the overall structure of a geographic market. The model is shown to make relatively good predictions even when the data from recently introduced categories/generations are limited.</description><author>Kim, N.; Chang, D. R.; Shocker, A. D.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational architecture and success in the information technology industry</title><link>http://www.example.com/articles/1</link><description>Mendelson, H.
This paper studies an organizational architecture that I call information-age architecture. I define a measure of organizational IQ and test whether it is related to financial and market success using data from the fast-moving information technology industry. Higher organizational IQ is associated with higher profitability and growth. This relationship is stronger in business environments that are characterized by faster clockspeeds.</description><author>Mendelson, H.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Executive compensation in the information technology industry</title><link>http://www.example.com/articles/1</link><description>Anderson, M. C.; Banker, R. D.; Ravindran, S.
An innovative business practice attributed to the information technology (IT) industry is the aggressive use of employee stock options to compensate executives and other employees. In this study, we investigate whether the greater use of stock options in the IT industry can be explained on the basis of general economic relationships that apply to firms in all industries. To examine differences in compensating top executives, we estimate a system of simultaneous equations that is designed to accommodate interconnections between performance, the level of compensation, and the mix of compensation components. We document that the shares of both bonus and option pay increase with performance and that the pay level and the extent of incentive pay positively affect firm performance. We identify economic factors that may influence the use of options and show that there are significant differences in these factors between IT and other industries. We find that, while much of the greater use of options by IT firms is explained by the economic factors, significant residual differences remain. We also find that, when performance and other factors are considered, the level of executive pay in the IT industry is not higher than in other industries.</description><author>Anderson, M. C.; Banker, R. D.; Ravindran, S.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information technology and productivity: Evidence from country-level data</title><link>http://www.example.com/articles/1</link><description>Dewan, S.; Kraemer, K. L.
This paper studies a key driver of the demand for the products and services of the global IT industry-returns from IT investments. We estimate an intercountry production function relating IT and non-IT inputs to GDP output, on panel data from 36 countries over the 1985-1993 period. We find significant differences between developed and developing countries with respect to their structure of returns from capital investments. For the developed countries in the sample, returns from IT capital investments are estimated to be positive and significant, while returns from non-IT capital investments are not commensurate with relative factor shares. The situation is reversed for the developing countries subsample, where returns from non-IT capital are quite substantial, but those from IT capital investments are not statistically significant. We estimate output growth contributions of IT and non-IT capital and discuss the contrasting policy implications for capital investment by developed and developing economies.</description><author>Dewan, S.; Kraemer, K. L.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Frictionless commerce? A comparison of Internet and conventional retailers</title><link>http://www.example.com/articles/1</link><description>Brynjolfsson, E.; Smith, M. D.
here have been many claims that the Internet represents a new nearly "frictionless market." Our research empirically analyzes the characteristics of the Internet as a channel for two categories of homogeneous products-books and CDs. Using a data set of over 8,500 price observations collected over a period of 15 months, we compare pricing behavior at 41 Internet and conventional retail outlets. We find that prices on the Internet are 9-16% lower than prices in conventional outlets, depending on whether taxes, shipping, and shopping costs are included in the price. Additionally, we find that Internet retailers' price adjustments over time are up to 100 times smaller than conventional retailers' price adjustments-presumably reflecting lower menu costs in Internet channels. We also find that levels of price dispersion depend importantly on the measures employed. When we compare the prices posted by different Internet retailers we find substantial dispersion. Internet retailer prices differ by an average of 33% for books and 25% for CDs. However, when we weight these prices by proxies for market share, we find dispersion is lower in Internet channels than in conventional channels, reflecting the dominance of certain heavily branded retailers. We conclude that while there is lower friction in many dimensions of Internet competition, branding, awareness, and trust remain important sources of heterogeneity among Internet retailers.</description><author>Brynjolfsson, E.; Smith, M. D.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A combinatorial auction with multiple winners for universal service</title><link>http://www.example.com/articles/1</link><description>Kelly, F.; Steinberg, R.
We describe a discrete-time auction procedure called PAUSE (Progressive Adaptive User Selection Environment) for use in assigning COLR (Carrier of Last Resort) responsibility for universal service. The auction incorporates synergies by permitting all combinatorial bids, is transparent to the bidders, allows for multiple winners, and minimizes the possibility of bidder collusion. The procedure is computationally tractable for the auctioneer and thus very efficient to run. The inherent computational complexity of combinatorial bidding cannot be eliminated. However, in this auction the computational burden of evaluating synergies rests with the bidders claiming those synergies, while the auctioneer simply checks that a bid is valid.</description><author>Kelly, F.; Steinberg, R.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An "alternating recognition" model of English auctions</title><link>http://www.example.com/articles/1</link><description>Harstad, R. M.; Rothkopf, M. H.
We present an alternative abstraction of an English (oral ascending) auction to the standard, in Milgrom and Weber (1982), that accords more closely with practices in some auction markets. In particular, the assumptions that exits are irrevocable and necessarily public are dropped, making endogenous the decision to compete silently and privately, or openly. In the model, the price rises in a stylization of an auctioneer alternately recognizing two bidders who affirm willingness to pay the current price. The auctioneer pays attention to other bidders only when a recognized bidder exits. Such exits may be temporary, although we construct an equilibrium in which there is no benefit to exit and reentry. The number of public exits is stochastic; frequently a losing "bidder" will remain silent, giving no indication of his willingness to pay, and hence yielding no useful inference about his private information. Thus, the source of the expected revenue advantage of English auctions over second-price auctions is only stochastically available. Moreover, when public exits are incomplete, the ordinal rank of the bidder whose private information can be inferred is unknown, making that information less valuable. Consequently, the Simpler formula for expected revenue in second-price auctions may be the preferred approximation for English auctions.</description><author>Harstad, R. M.; Rothkopf, M. H.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Predecisional distortion of information by auditors and salespersons</title><link>http://www.example.com/articles/1</link><description>Russo, J. E.; Meloy, M. G.; Wilks, T. J.
As people are deciding between two alternatives, they may distort new information to support whichever alternative is tentatively preferred. The presence of such predecisional distortion of information was tested in decisions made by two groups of professionals, auditors and salespersons. Both groups exhibited substantial distortion of information, with Little reduction for professional decisions compared to nonprofessional ones. However, auditors' distortion was significantly smaller than that of salespersons. In addition, holding professionals accountable for their decisions, akin to a supervisory review, lowered distortion somewhat for salespersons but not at all for auditors. The latter seemed to act as if they were always being held accountable. Because people seem unaware that they are distorting information, at least at the moment this bias is occurring, they are fully convinced of the soundness of their choices. This may make it difficult for distortion to be detected by decision makers themselves or even by supervisors who cannot completely duplicate their subordinate's knowledge.</description><author>Russo, J. E.; Meloy, M. G.; Wilks, T. J.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>What's experience got to do with it? Sources of cost reduction in a large specialty chemicals producer</title><link>http://www.example.com/articles/1</link><description>Sinclair, G.; Klepper, S.; Cohen, W.
Conventional learning curves relating unit cost to measures of production experience are estimated for 221 specialty chemicals produced by a Fortune 500 company. Detailed records on cost and R&amp;D coupled with insights from company personnel are used to explain the variation across products in the rate of cost reduction. Products that exhibited the strongest relationship between unit cost and measures of production experience were subject to specific initiatives, particularly process R&amp;D. The R&amp;D was not, however, generally motivated or informed, by production experience. However, cumulative past output, the most commonly used measure of production experience, was related to expected future output, which conditioned the expected future returns from R&amp;D and the choice of R&amp;D projects. Thus, cumulative output was connected to unit costs through its role in conditioning incentives to undertake process R&amp;D rather than as a proxy for production experience. This suggests that the strong relationship commonly found between unit cost and measures of production experience may reflect incentives to reduce cost as much as learning from production experience.</description><author>Sinclair, G.; Klepper, S.; Cohen, W.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A discrete-time approach to arbitrage-free pricing of credit derivatives</title><link>http://www.example.com/articles/1</link><description>Das, S. R.; Sundaram, R. K.
This paper develops a framework for modelling risky debt and valuing credit derivatives that is flexible and simple to implement, and that is, to the maximum extent possible, based on observables. Our approach is based on expanding the Heath-Jarrow-Morton term-structure model to allow for defaultable debt. Rather than follow the procedure of implying out the behavior of spreads from assumptions concerning the default process, we work directly with the evolution of spreads. The risk-neutral drifts in the resulting model possess a recursive representation that facilitates implementation and makes it possible to handle path-dependence and early exercise features without difficulty. The framework permits embedding a variety of specifications for default; we present an empirical example of a default structure which provides promising calibration results.</description><author>Das, S. R.; Sundaram, R. K.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Shifts of reference points for framing of strategic decisions and changing risk-return associations</title><link>http://www.example.com/articles/1</link><description>Lehner, J. M.
Previous results on nonlinear risk-return associations, predicted by prospect theory, are replicated with mean quadratic differences instead of variance as a measure of risk. In contrast to assumptions of these studies, results with a sample from the COMPUSTAT-database provide evidence that at least a minority of firms shift to individual reference levels, which are represented here through levels of minimal risk. Further, changes of environmental conditions as an alternative explanation for switching risk-return relationships are tested against prospect theory predictions. It is shown that risk-return relationships remain stable as long as the relative position to the individual reference level is stable. This explains switching risk-return relationships better than changing environmental conditions.</description><author>Lehner, J. M.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stock rationing in an M/E-k/1 make-to-stock queue</title><link>http://www.example.com/articles/1</link><description>Ha, A. Y.
This paper considers the stock rationing problem of a single-item, make-to-stock production system with several demand classes and lost sales. When demand is Poisson and processing time has an Erlang distribution, we show that a single-state variable called work storage level can be employed to completely capture the information regarding inventory level and the status of current production. The optimal rationing policy can be characterized by a sequence of monotone critical work storage levels. For each demand class, there exists a work storage level at or below which it is optimal to start rejecting the demand of this class in anticipation of future arrival of higher-priority demands. The optimal production policy can also be characterized by a critical work storage level. Our numerical examples indicate that a critical stock level policy, which ignores information on the status of current production, performs very well.</description><author>Ha, A. Y.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Attribute conflict and preference uncertainty: Effects on judgment time and error</title><link>http://www.example.com/articles/1</link><description>Fischer, G. W.; Luce, M. F.; Jia, J. M.
This research investigates preference uncertainty generated as a function of specific alternative characteristics during multiattribute evaluative judgments. We propose that preference uncertainty has at least two behavioral manifestations: longer judgment times and greater response error in expressed preferences. We investigate two hypotheses regarding stimulus-based causes of preference uncertainty. As predicted by our attribute conflict hypothesis, greater within-alternative conflict (discrepancy among the attributes of an evaluative alternative) led to longer judgment times and greater response error. As predicted by our attribute extremity hypothesis, greater attribute extremity (very high or low attribute values) resulted in shorter judgment times and less response error. We also found that judgment times and response errors were strongly positively correlated at the item level, consistent with our assumption that preference uncertainty generated by stimulus characteristics is manifested in judgment time and error. Finally, we found that the item-level preference uncertainty effects proposed here operate in parallel with strategy-level, effort-accuracy tradeoffs observable across participants. These findings are consistent with the RandMAU random multiattribute utility model developed in a companion article by Fischer et al. (2000).</description><author>Fischer, G. W.; Luce, M. F.; Jia, J. M.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Heuristic computation of periodic-review base stock inventory policies</title><link>http://www.example.com/articles/1</link><description>Roundy, R. O.; Muckstadt, J. A.
We study the problem of determining production quantities in each period of an infinite horizon for a single item produced in a capacity-limited facility. The demand for the product is random, and it is independent and identically distributed from period to period. The demand is observed at the beginning of a time period, but it need not be filled until the end of the period. Unfilled demand is backordered. A base stock or order-up-to policy is used. The shortfall is the order-up-to level minus the inventory position. The inventory system is easily understood and managed if we know the distribution of the shortfall. We develop a new approximation for this distribution, and perform extensive computational tests of existing approximations. Our new approximation works extremely well as long as the coefficient of variation of the demand is less than two. Far practical applications this is by far the most interesting case; No known approximations work well consistently when the coefficient of variation of the demand is greater than two.</description><author>Roundy, R. O.; Muckstadt, J. A.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Processes and their frameworks</title><link>http://www.example.com/articles/1</link><description>Mackenzie, K. D.
A process is a time-dependent sequence of events governed by a process framework. A group process has five components: the entities performing the process, the steps or elements of a process, the relationship between any pair of elements, the links to other processes, and the resources and their characteristics-in-use involved with the elements. A process framework is denoted by Y = F(C) where Y is the set of outcomes or consequences of a process, C is the set of considerations or elements in the process, and F is the network linking the considerations to each other and to the outcomes. The properties of the set of considerations, the linkages between pairs of consequences, the set of outcomes or consequences, the network, F, and the use of process frameworks are discussed in detail with examples. Process models are compared to variable models.</description><author>Mackenzie, K. D.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A survey of algorithms for convex multicommodity flow problems</title><link>http://www.example.com/articles/1</link><description>Ouorou, A.; Mahey, P.; Vial, J. P.
Routing problems appear frequently when dealing with the operation of communication or transportation networks. Among them, the message routing problem plays a determinant role in the optimization of network performance. Much of the motivation for this work comes from this problem which is shown to belong to the class of nonlinear convex multicommodity flow problems. This paper emphasizes the message routing problem in data networks, but it includes a broader literature overview of convex multicommodity flow problems. We present and discuss the main solution techniques proposed for solving this class of large-scale convex optimization problems. We conduct some numerical experiments on the message routing problem with four different techniques.</description><author>Ouorou, A.; Mahey, P.; Vial, J. P.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Three scheduling algorithms applied to the earth observing systems domain</title><link>http://www.example.com/articles/1</link><description>Wolfe, W. J.; Sorensen, S. E.
This paper describes three approaches to assigning tasks to earth observing satellites (EOS). A fast and simple priority dispatch method is described and shown to produce acceptable schedules most of the time. A look ahead algorithm is then introduced that outperforms the dispatcher by about 12% with only a small increase in run time. These algorithms set the stage for the introduction of a genetic algorithm that uses job permutations as the population. The genetic approach presented here is novel in that it uses two additional binary variables, one to allow the dispatcher to occasionally skip a job in the queue and another to allow the dispatcher to occasionally allocate the worst position to the job. These variables are included in the recombination step in a natural way. The resulting schedules improve on the look ahead by as much as 15% at times and 3% on average. We define and use the "window-constrained packing" problem to model the bare bones of the EOS scheduling problem.</description><author>Wolfe, W. J.; Sorensen, S. E.</author><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How the incumbent can win: Managing technological transitions in the semiconductor industry</title><link>http://www.example.com/articles/1</link><description>Iansiti, M.
The paper reports on an empirical study of the management of technological transitions. It focuses on project-level mechanisms for the generation of knowledge through experimentation and for its accumulation through individual experience. It proposes a model that links these mechanisms to effectiveness in the management of revolutionary and evolutionary development approaches. This argument is tested with data describing projects conducted by all major competitors in the semiconductor industry. Each project was aimed at a technological transition, defined as the introduction of a major new generation of process technology. The analysis shows substantial differences among competitors in the approach taken (i.e., evolutionary vs. revolutionary) and results achieved. Additionally, it shows that individual organizations can migrate, over time, from evolution to revolution and vice versa. The analysis further indicates that accumulating experience and generating knowledge through experimentation are significantly associated with project performance. While product performance improvement through revolution is associated with research experience and with parallel experimentation capacity, improvement through evolution is associated with project experience and minimum experimental iteration time.</description><author>Iansiti, M.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theoretical extension of the Technology Acceptance Model: Four longitudinal field studies</title><link>http://www.example.com/articles/1</link><description>Venkatesh, V.; Davis, F. D.
The present research develops and tests a theoretical extension of the Technology Acceptance Model (TAM) that explains perceived usefulness and usage intentions in terms of social influence and cognitive instrumental processes. The extended model, referred to as TAM2, was tested using longitudinal data collected regarding four different systems at four organizations (N = 156), two involving voluntary usage and two involving mandatory usage. Model constructs were measured at three points in time at each organization: preimplementation, one month postimplementation, and three months postimplementation. The extended model was strongly supported for all four organizations at all three points of measurement, accounting for 40%-60% of the variance in usefulness perceptions and 34%-52% of the variance in usage intentions. Both social influence processes (subjective norm, voluntariness, and image) and cognitive instrumental processes (job relevance, output quality, result demonstrability, and perceived ease of use) significantly influenced user acceptance. These findings advance theory and contribute to the foundation for future research aimed at improving our understanding of user adoption behavior.</description><author>Venkatesh, V.; Davis, F. D.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capital budgeting, the hold-up problem, and information system design</title><link>http://www.example.com/articles/1</link><description>Arya, A.; Fellingham, J.; Glover, J.; Sivaramakrishnan, K.
In this article, we explore the connection between information system design and incentives for project search. The choice of an information system affects the level of managerial slack that is generated during project implementation. Whether slack is beneficial or costly to an organization has been the subject of debate. Ln our model of the hold-up problem in capital budgeting, there are both costs and benefits to having managerial slack. The cost of slack is the consumption of perquisites by the manager. The benefit of slack is that it can serve as a motivational tool. The possibility of increasing his slack may encourage a self-interested manager to conduct a more diligent search for a profitable project. To trade off the costs and benefits of slack in our model, an optimal information system sometimes incorporates coarse information, late information, and a mix of monitored and self-reported information. These features are familiar to accountants. Accounting incorporates both verified (monitored) and unverified (self-reported) information and provides information that is aggregated (coarse) and historical (late).</description><author>Arya, A.; Fellingham, J.; Glover, J.; Sivaramakrishnan, K.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stock replenishment and shipment scheduling for vendor-managed inventory systems</title><link>http://www.example.com/articles/1</link><description>Cetinkaya, S.; Lee, C. Y.
Vendor-managed inventory (VMI) is a supply-chain initiative where the supplier is authorized to manage inventories of agreed-upon stock-keeping units at retail. locations. The benefits of VMI are well recognized by successful retail businesses such as Wal-Mart. In VMI, distortion of demand information (known as bullwhip effect) transferred from the downstream supply-chain member (e.g., retailer) to the upstream member (e.g., supplier) is minimized, stockout situations ape less frequent, and inventory-carrying costs are reduced. Furthermore, a VMI supplier has the liberty of controlling the downstream resupply decisions rather than filling orders as they are placed. Thus, the approach offers a framework for synchronizing inventory and transportation decisions. In this paper, we present an analytical model for coordinating inventory and transportation decisions in VMI systems. Although the coordination of inventory and transportation has been addressed in the literature, our particular problem has not been explored previously. Specifically, we consider a vendor realizing a sequence of random demands from a group of retailers located in a given geographical region. ideally, these demands should be shipped immediately. However, the vendor has the autonomy of holding small orders until an agreeable dispatch time with the expectation that an economical consolidated dispatch quantity accumulates. As a result, the actual inventory requirements at the vendor are partly dictated by the parameters of the shipment-release policy in use. We compute the optimum replenishment quantity and dispatch frequency simultaneously. We develop a renewal-theoretic model for the case of Poisson demands, and present analytical results.</description><author>Cetinkaya, S.; Lee, C. Y.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the determination of subjective probability by choices</title><link>http://www.example.com/articles/1</link><description>Karni, E.; Mongin, P.
The paper explores the uniqueness properties of the subjective probabilities in two axiomatizations of state-dependent preferences Karni, Schmeidler, and Vind's (KSV 1983) system depends on selecting an arbitrary auxiliary probability, and as such, does not guarantee the uniqueness of the derived subjective probability. However, an axiom system initially designed by Karni and Schmeidler (KS 1981) and further elaborated upon here does guarantee the desired uniqueness as well as a useful property of "stability" of the derived solution. When the preference relation displays state-independence, even the KS probabilities may not agree with those derived from the classic Anscombe-Aumann (AA 1963) theorem. However, we claim that, in this case, the KS rather than the AA probabilities are the appropriate representation of the agent's beliefs.</description><author>Karni, E.; Mongin, P.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Turning datamining into a management science tool: New algorithms and empirical results</title><link>http://www.example.com/articles/1</link><description>Cooper, L. G.; Giuffrida, G.
This article develops and illustrates a new knowledge discovery algorithm tailored to the action requirements of management science applications. The challenge is to develop tactical planning forecasts at the SKU level. We use a traditional market-response model to extract information from continuous variables and use datamining techniques on the residuals to extract information from the many-valued nominal variables, such as the manufacturer or merchandise category. This combination means that a more complete array of information can be used to develop tactical planning forecasts. The method is illustrated using records of the aggregate sales during promotion events conducted by a 95-store retail chain in a single trading area. In a longitudinal cross validation, the statistical forecast (PromoCast(TM)) predicted the exact number of cases of merchandise needed in 49% of the promotion events and was within +/- one case in 82% of the events. The dataminer developed rules from an independent sample of 1.6 million observations and applied these rules to almost 460,000 promotion events in the validation process. The dataminer had sufficient confidence to make recommendations on 46% of these forecasts. In 66% of those recommendations, the dataminer indicated that the forecast should not be changed. In 96% of those promotion events where "no change" was recommended, this was the correct "action" to take. Even including these "no change" recommendations, the dataminer decreased the case error by 9% across all promotion events in which rules applied.</description><author>Cooper, L. G.; Giuffrida, G.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Improving manufacturing performance through process change and knowledge creation</title><link>http://www.example.com/articles/1</link><description>Carrillo, J. E.; Gaimon, C.
A model is introduced to guide a profit maximizing firm in its quest to enhance performance through process change. The key benefit sought from process change is a long term increase in effective capacity. However, realizing success from process change is not trivial. First, while process change may increase effective capacity in the long run, the disruptions during implementation typically reduce short term capacity. Second, competitive forces such as decreasing revenue streams and shrinking product life cycles complicate the implementation of process change. Third, while knowledge may enhance the ultimate benefits derived from process change, the correct timing and means of knowledge creation are difficult to discern. Lastly, a variety of trade-offs must be evaluated when selecting the particular process change to pursue. For example, choices range from hardware and software replacements to modification of manufacturing procedures. The model introduced here explicitly considers both the short term loss due to disruption and the long term gain in effective capacity associated with the process change. In addition, investments in the accumulation of knowledge are investigated for their potential to enhance process change effectiveness. Knowledge is generated from investment in preparation and training (learning-before-doing) and as a by-product of process change (learning-by-doing). Analysis of the model provides managerial recommendations for several key decisions relating to process change implementation including: (i) the selection of an appropriate process change alternative, (ii) the rate and timing for investment in process change, and (iii) the rate and timing for investment in preparation and training. New results are reported reflecting the important relationship between process change and knowledge. For example, we show that under certain conditions, a firm should optimally delay investment in process change until sufficient accumulation of knowledge is achieved. More generally, we identify conditions whereby investment in process change occurs at an increasing rate over time. This result is particularly important since it demonstrates a limitation of the existing literature where process change always occurs at a decreasing rate.</description><author>Carrillo, J. E.; Gaimon, C.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Third degree stochastic dominance and mean-risk analysis</title><link>http://www.example.com/articles/1</link><description>Gotoh, J. Y.; Konno, H.
In their recent article, Ogryczak and Ruszczynski (1999) proved that those portfolios associated with the efficient frontiers generated by mean-lower semi-standard deviation model and mean- (lower semi-)absolute deviation model are efficient in the sense of second degree stochastic dominance. This rather surprising result reveals the importance of lower partial risk models in portfolio analysis. In this paper, we extend the results of Ogryczak and Ruszczynski for second degree stochastic dominance to third degree stochastic dominance. We show that portfolios on a significant portion of the efficient frontier generated by mean-lower semi-skewness model are efficient in the sense of third degree stochastic dominance. Also, we prove that the portfolios generated by mean-variance-skewness model are semi-efficient in the sense of third degree stochastic dominance.</description><author>Gotoh, J. Y.; Konno, H.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of coefficient correlation structure in two-dimensional knapsack problems on solution procedure performance</title><link>http://www.example.com/articles/1</link><description>Hill, R. R.; Reilly, C. H.
This paper presents the results of an empirical study of the effects of coefficient correlation structure and constraint slackness settings on the performance of solution procedures on synthetic two-dimensional knapsack problems (2KP). The population correlation structure among 2KP coefficients, the level of constraint slackness, and the type of correlation (product moment or rank) are varied in this study. Representative branch-and-bound and heuristic solution procedures are used to investigate the influence of these problem parameters on solution procedure performance. Population correlation structure, and in particular the interconstraint component of the correlation structure, is found to be a significant factor influencing the performance of both the algorithm and the heuristic. In addition, the interaction between constraint slackness and population correlation structure is found to influence solution procedure performance.</description><author>Hill, R. R.; Reilly, C. H.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Improved rolling schedules for the dynamic single-level lot-sizing problem</title><link>http://www.example.com/articles/1</link><description>Stadtler, H.
A major argument for favoring simple lot-sizing heuristics-like the Silver/Meal or Groff's heuristic-to solve instances of the dynamic single-level uncapacitated lot-sizing problem (SLLSP) instead of exact algorithms-like those of Wagner/Whitin or Federgruen/Tzur-is that exact algorithms applied in a rolling horizon environment are heuristics too and may be outperformed by simple heuristics. This article shows how to modify the model of the SLLSP by looking beyond the planning horizon. Extensive tests within a rolling horizon environment have demonstrated that the modified model solved by an exact algorithm now performs at least as well as well-known heuristics and is fairly insensitive to the length of the planning horizon. Furthermore, our principal idea of improving rolling schedules by considering only a portion of the fixed cost related to a decision with an impact on periods beyond the planning horizon is applicable to a wide range of decision models.</description><author>Stadtler, H.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technical note: Mathematical properties of the optimal product line selection problem using choice-based conjoint analysis</title><link>http://www.example.com/articles/1</link><description>Chen, K. D.; Hausman, W. H.
Selecting and pricing product lines is an essential activity in many businesses, in recent years, quantitative approaches for such tasks have been gaining in popularity. One often-employed method is to use data from traditional rankings/ratings-based conjoint analysis and attack the product line selection problem with enumeration or heuristics. In this note, we employ a relatively new methodology known as choice-based conjoint analysis (to model customer preferences) and investigate its mathematical properties when used to model the product line selection problem. Despite some inherent limitations resulting from its aggregated formulation, we show that this more parsimonious conjoint approach has some special mathematical properties that lead to an efficient optimal algorithm to tackle the product line/price selection problem. As a result, problems of realistic size can be solved efficiently using standard, commercially available mathematical programming codes.</description><author>Chen, K. D.; Hausman, W. H.</author><pubDate>Tue, 01 Feb 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizing distribution channels for information goods on the Internet</title><link>http://www.example.com/articles/1</link><description>Dewan, R.; Freimer, M.; Seidmann, A.
Rapid technological developments and deregulation of the telecommunications industry have changed the way in which content providers distribute and price their goods and services. Instead of selling a bundle of content and access through proprietary networks, these firms are shifting their distribution channels to the Internet. In this new setting, the content and Internet service providers find themselves in a relationship that is simultaneously cooperative and competitive. We find that proprietary content providers prefer the Internet channels to direct channels only if the access market is sufficiently competitive. Furthermore, maintaining a direct channel in addition to the Internet channels changes the equilibrium enough that the proprietary content providers prefer having the Internet channels, regardless of the level of competition in the access market. Telecommunications technology developments uniformly increase content providers' profit. On the other hand, the technology impact on Internet service provider profits is nonmonotonic: Their profits may increase or decrease as a result of lower telecommunication costs. While initially the ISP profit increases as more customers are drawn to the Internet, it eventually decreases as the spatial competition becomes more intense. We also show that proprietary content providers should benefit from having some free content available at the Internet service providers' sites to induce more customers to join the Internet.</description><author>Dewan, R.; Freimer, M.; Seidmann, A.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price protection in the personal computer industry</title><link>http://www.example.com/articles/1</link><description>Lee, H. L.; Padmanabhan, V.; Taylor, T. A.; Whang, S. J.
Price protection is a commonly used practice between manufacturers and retailers in the personal computer (PC) industry, motivated by drastic declines of product values during the product life cycle. It is a form of rebate given by the manufacturer to the retailer for units unsold at the retailer when the price drops during the product life cycle. It is a controversial policy in the PC industry because it is not clear how such a policy benefits the supply chain and its participants. We show that price protection is an instrument for channel coordination. For products with long manufacturing lead times, so the retailer has a single buying opportunity, a properly chosen price protection credit coordinates the channel. For products with shorter manufacturing lead times, so the retailer has two buying opportunities, price protection alone cannot guarantee channel coordination when wholesale prices are exogenous. However, when the price protection credit is set endogenously together with the wholesale prices, channel coordination is restored. In the two-buying-opportunity setting with fixed wholesale prices, we show that price protection has two primary impacts: (1) shifting sales forward in time and (2) increasing total sales. Finally, we present a simple numerical example that suggests, given the current economics of the PC industry, that price protection under fixed wholesale prices may benefit the total chain and the retailer but hurt the manufacturer.</description><author>Lee, H. L.; Padmanabhan, V.; Taylor, T. A.; Whang, S. J.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effects of process maturity on quality, cycle time, and effort in software product development</title><link>http://www.example.com/articles/1</link><description>Harter, D. E.; Krishnan, M. S.; Slaughter, S. A.
The information technology (IT) industry is characterized by rapid innovation and intense competition. To survive, IT firms must develop high quality software products on time and at low cost. A key issue is whether high levels of quality can be achieved without adversely impacting cycle time and effort. Conventional beliefs hold that processes to improve software quality can be implemented only at the expense of longer cycle times and greater development effort. However, an alternate view is that quality improvement, faster cycle time, and effort reduction can be simultaneously attained by reducing defects and rework. In this study, we empirically investigate the relationship between process maturity, quality, cycle time, and effort for the development of 30 software products by a major IT firm. We find that higher levels of process maturity as assessed by the Software Engineering Institute's Capability Maturity Model(TM) are associated with higher product quality, but also with increases in development effort. However, our findings indicate that the reductions in cycle time and effort due to improved quality outweigh the increases from achieving higher levels of process maturity. Thus, the net effect of process maturity is reduced cycle time and development effort.</description><author>Harter, D. E.; Krishnan, M. S.; Slaughter, S. A.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An empirical analysis of productivity and quality in software products</title><link>http://www.example.com/articles/1</link><description>Krishnan, M. S.; Kriebel, C. H.; Kekre, S.; Mukhopadhyay, T.
We examine the relationship between life-cycle productivity and conformance quality in software products. The effects of product size, personnel capability, software process, usage of tools, and higher front-end investments on productivity and conformance quality were analyzed to derive managerial implications based on primary data collected on commercial software projects from a leading vendor. Our key findings are as follows. First, our results provide evidence for significant increases in life-cycle productivity from improved conformance quality in software products shipped to the customers. Given that the expenditure on computer software has been growing over the last few decades, empirical evidence for cost savings through quality improvement is a significant contribution to the literature. Second, our study identifies several quality drivers in software products. Our findings indicate that higher personnel capability, deployment of resources in initial stages of product development (especially design) and improvements in software development process factors are associated with higher quality products.</description><author>Krishnan, M. S.; Kriebel, C. H.; Kekre, S.; Mukhopadhyay, T.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management without forecasting or optimization: An adaptive algorithm for determining airline seat protection levels</title><link>http://www.example.com/articles/1</link><description>van Ryzin, G.; McGill, J.
We investigate a simple adaptive approach to optimizing seat protection levels in airline revenue management systems. The approach uses only historical observations of the relative frequencies of certain seat-filling events to guide direct adjustments of the seat protection levels in accordance with the optimality conditions of Brumelle and McGill (1993). Stochastic approximation theory is used to prove the convergence of this adaptive algorithm to the optimal protection levels. in a simulation study, we compare the revenue performance of this adaptive approach to a more traditional method that combines a censored forecasting method with a common seat allocation heuristic (EMSR-b).</description><author>van Ryzin, G.; McGill, J.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information, contracting, and quality costs</title><link>http://www.example.com/articles/1</link><description>Baiman, S.; Fischer, P. E.; Rajan, M. V.
This article analyzes the relation between product quality, the cost of quality, and the information that can be contracted upon. We consider a setting where a risk neutral supplier sells an intermediate product to a risk neutral buyer. The supplier incurs prevention costs to reduce the probability of selling a defective product, and the buyer incurs appraisal costs to identify defects. Both decisions are subject to moral hazard. We show that the first-best outcome can be obtained if either: (i) the supplier's prevention decision is contractible; or (ii) the buyer's appraisal decision and either internal failure (i.e., the product's failing the buyer's appraisal test) or external failure (i.e., the product's failing after being sold by the buyer) are contractible events; or (iii) both internal and external failure are contractible events. We then focus on the second-best setting where actions and failures are not contractible and study the effect of making the buyer's appraisal result contractible. Relative to first-best, if a buyer's return decision is contractible (but not his appraisal result), the supplier incurs lower prevention costs, the buyer incurs higher appraisal costs, expected internal failure costs are higher, and the total cost of quality is higher. The expected costs of external failure, however, may actually be lower relative to first-best. We then show that installing an information system that makes the appraisal result contractible reduces the inefficiency associated with the seller's prevention activity, increases the inefficiency associated with the buyer's quality appraisal activity, and unambiguously improves product quality.</description><author>Baiman, S.; Fischer, P. E.; Rajan, M. V.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Accounting information, aggregation, and discriminant analysis</title><link>http://www.example.com/articles/1</link><description>Arya, A.; Fellingham, J.; Schroeder, D.
Aggregation is a pervasive theme in accounting. The preparation of financial statements involves extensive aggregation-information regarding several transactions is summarized using a few account balances. In this article, we study Linear, double-entry aggregation rules. The level of aggregation (transactions versus account balance information) affects a decision maker's ability to discriminate between two entities. We show Brat the orientation of the discriminant function relative to the row space and the nullspace (two fundamental subspaces) of the double-entry matrix determines the information loss due to aggregation. In addition, we observe that an interdependency in account balances is introduced by the double-entry process. The cause and effect property (debit and credit) translates into a negative covariance being introduced among account balances; this, in turn, affects the decision maker's optimal use of information. Finally, in discussing benefits to aggregation, we present an example in which adopting a double-entry aggregation rule serves as a commitment device for the owner.</description><author>Arya, A.; Fellingham, J.; Schroeder, D.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring the robustness of empirical efficiency valuations</title><link>http://www.example.com/articles/1</link><description>Kuntz, L.; Scholtes, S.
We study the robustness of empirical efficiency valuations of production processes in an extended Farrell model. Based on input and output data, an empirical efficiency status-efficient or inefficient-is assigned to each of the processes. This status may change if the data of the observed processes change. As illustrated by a capacity planning problem for hospitals in Germany, the need arises to gauge the robustness of empirical efficiency valuations. The example suggests to gauge the robustness of the efficiency valuation for a process with respect to perturbations of prespecified elements of the data. A natural measure of robustness is the minimal perturbation, in terms of a suitable distance function, of the chosen data elements that is necessary to change the efficiency status of the process under investigation. Farrell's (1957) efficiency score is an example of such a robustness measure. We give further examples of relevant data perturbations for which the robustness measure can be computed efficiently. We then focus on weighted maximum norm distance functions, such as the maximal absolute or percentage deviation, but allow for independent perturbations of the elements of an arbitrary a priori fixed subset of the data. In this setting, the robustness measure is naturally related to a certain threshold value for a Linear monotone one-parameter family of perturbations and can be calculated by means of a Linear programming-based bisection method. Closed form solutions in terms of Farrell's efficiency score are obtained for specific perturbations, Following the theoretical developments, we revisit the hospital capacity planning problem to illustrate the managerial relevance of our techniques.</description><author>Kuntz, L.; Scholtes, S.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Imitation of complex strategies</title><link>http://www.example.com/articles/1</link><description>Rivkin, J. W.
Researchers examining loosely coupled systems, knowledge management, and complementary practices in organizations have proposed, informally, that the complexity of a successful business strategy can deter imitation of the strategy. This paper explores this proposition rigorously. A simple model is developed that parametrizes the two aspects of strategic complexity: the number of elements in a strategy and the interactions among those elements. The model excludes conventional resource-based and game-theoretic barriers to imitation altogether. The model is used to show that complexity makes the search for an optimal strategy intractable in the technical sense of the word provided by the theory of NP-completeness. Consequently, would-be copycats must rely on search heuristics or on learning, not on algorithmic "solutions," to match the performance of superior firms. However, complexity also undermines heuristics and learning, in the face of complexity, firms that follow simple hill-climbing heuristics are quickly snared on low "local peaks," and firms that try to learn and mimic a high performer's entire strategy suffer large penalties from small errors. The model helps to explain why some winning strategies remain unmatched even though they are open to public scrutiny; why certain bundles of organizational practices diffuse slowly even though they lead to superior performance; and why some strategies yield superior returns even after many of their critical ingredients are adopted by competitors. The analysis also suggests roles for management science and managerial choice in a world of complex strategies.</description><author>Rivkin, J. W.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal parallel inspection for finding the first nonconforming unit in a batch - An information theoretic approach</title><link>http://www.example.com/articles/1</link><description>Herer, Y. T.; Raz, T.
We consider the case of a batch of discrete units produced by a process subject to failures under a known probability distribution function, and apply information theory to the problem of finding the first nonconforming unit in the batch at minimum cost. Two distinct but related aspects of this problem were treated: determining which units should be inspected, and determining how many units should be sent for inspection at the same time. The solution is based on the principles of inspecting the product units that maximize the reduction in the uncertainty regarding the location of the first nonconforming unit, and of minimizing the cost per unit of uncertainty reduced. These principles are formalized by means of a series of theorems leading to an easy-to-implement algorithm for managing parallel inspection. This approach is successfully compared with the optimal solution obtained with dynamic programming and with other heuristics.</description><author>Herer, Y. T.; Raz, T.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Convex input and output projections of nonconvex production possibility sets</title><link>http://www.example.com/articles/1</link><description>Bogetoft, P.; Tama, J. M.; Tind, J.
In this paper we characterize the smallest production possibility set that contains a specified set of (input, output) combinations. In accordance with neoclassical production economics, this possibility set has convex projections into the input and output spaces (convex isoquants), and it satisfies the assumption of free disposability. We obtain it by means of a possibly infinite recursion which builds the possibility set as an ever larger union of convex Sets. We remark on the nature of the approximations obtained by truncating the recursion, and we obtain a necessary and sufficient condition, checkable in one iteration for the recursion to stop in the next. For the case in which the recursion stops, we provide a succinct characterization of the dominance relations among the constituent sets produced by the procedure. Finally, we present examples of both finite and infinite cases. The example for the finite case illustrates the construction of the possibility set along with its associated production and consumption sets.</description><author>Bogetoft, P.; Tama, J. M.; Tind, J.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A remark on third degree stochastic dominance</title><link>http://www.example.com/articles/1</link><description>Ng, M. C.
This note presents two counterexamples to illustrate that neither implication of Theorem 4 in Levy (1992) is correct.</description><author>Ng, M. C.</author><pubDate>Thu, 01 Jun 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on the information technology industry</title><link>http://www.example.com/articles/1</link><description>Mendelson, H.; Whang, S. J.
nan</description><author>Mendelson, H.; Whang, S. J.</author><pubDate>Sat, 01 Apr 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Distribution of knowledge, group network structure, and group performance</title><link>http://www.example.com/articles/1</link><description>Rulke, D. L.; Galaskiewicz, J.
This study investigates the effect of knowledge distribution and group structure on performance in MBA game teams. We found that group performance was contingent on the distribution of knowledge within the group and networks of social relationships among group members. Studying 39 teams of MBA students in two management simulation games, we found that, in general, groups that had broadly distributed knowledge, i.e., groups made up of members who had general knowledge, outperformed groups that had knowledge concentrated in different members, i.e., groups made up of members who had specialized or both specialized and general knowledge. However, the advantage that the former enjoyed over the latter disappeared when groups of specialists or mixed groups had decentralized network structures.</description><author>Rulke, D. L.; Galaskiewicz, J.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Behind the learning curve: Linking learning activities to waste reduction</title><link>http://www.example.com/articles/1</link><description>Lapre, M. A.; Mukherjee, A. S.; Van Wassenhove, L. N.
This exploratory research on a decade of Total Quality Management in one factory opens up the black box of the learning curve. Based on the organizational learning literature, we derive a quality learning curve that links different types of learning in quality improvement projects to the evolution of the factory's waste rate. Only 25% of the quality improvement projects-which acquired both know-why and know-how-accelerated waste reduction. The other 75% of the projects either impeded or did not affect waste reduction. In complex and dynamic production environments, locally acquired knowledge is difficult to disseminate. The combination of know-why and know-how facilitates its dissemination.</description><author>Lapre, M. A.; Mukherjee, A. S.; Van Wassenhove, L. N.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The value of information sharing in a two-level supply chain</title><link>http://www.example.com/articles/1</link><description>Lee, H. L.; So, K. C.; Tang, C. S.
Many companies have embarked on initiatives that enable more demand information sharing between retailers and their upstream suppliers. While the literature on such initiatives in the business press is proliferating, it is not clear how one can quantify the benefits of these initiatives and how one can identify the drivers of the magnitudes of these benefits. Using analytical models, this paper aims at addressing these questions for a simple two-level supply chain with nonstationary end demands. Our analysis suggests that the value of demand information sharing can be quite high, especially when demands are significantly correlated over time.</description><author>Lee, H. L.; So, K. C.; Tang, C. S.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A continuous-time yield management model with multiple prices and reversible price changes</title><link>http://www.example.com/articles/1</link><description>Feng, Y. Y.; Xiao, B. C.
This article studies a continuous-time yield management model in which reversible price changes are allowed. We assume that perishable assets are offered at a set of discrete price levels. Demand at each level is a Poisson process. To maximize the expected revenue, management controls the price dynamically as sales evolve. We show that a subset of these prices that form a concave envelope is potentially optimal. We formulate the problem into an intensity control model and derive the optimal solution in closed form. Properties of the optimal solution and their policy implementations are discussed. Numerical examples are provided.</description><author>Feng, Y. Y.; Xiao, B. C.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The minimum variance hedge ratio under stochastic interest rates</title><link>http://www.example.com/articles/1</link><description>Lioui, A.; Poncet, P.
In an environment where interest rates are stochastic, we examine the case of a "pure" hedger endowed with a fixed position in a long term bond. In contrast to conventional wisdom according to which the difference between hedging through forward contracts and futures is immaterial, it turns out that the minimum variance hedge ratio using forwards comprises two terms instead of one only when using futures. The magnitude of the difference between the two hedge ratios may be important under some plausible assumptions. This result is due to the presence of additional interest rate risk that bears on the profit-and-loss statement associated with the forward position. This sheds some additional light on the respective features of forward and futures contracts written on interest rate-sensitive securities.</description><author>Lioui, A.; Poncet, P.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Attribute conflict and preference uncertainty: The RandMAU model</title><link>http://www.example.com/articles/1</link><description>Fischer, G. W.; Jia, J. M.; Luce, M. F.
This paper extends the behavioral results reported in Fischer et al. (2000) by developing a model addressing preference uncertainty in multiattribute evaluation. The model is motivated by two hypotheses regarding properties of multiattribute profiles that lead to greater preference uncertainty. Our attribute conflict hypothesis predicts that greater within-alternative conflict (discrepancy among the attributes of an alternative) leads to more preference uncertainty. Our attribute extremity hypothesis predicts that greater attribute extremity (very high or low attribute values) leads to less preference uncertainty. To provide a deeper explanation of attribute conflict and extremity effects, we develop RandMAU, a family of additive (RandAUF) and multiplicative (RandMUF) random weights multiattribute utility models. In RandMAU models, preference uncertainty is represented as random variation in both the weighting parameters governing trade-offs among attributes and the curvature parameters governing single-attribute evaluations. Simulation results show that RandMUF successfully predicts both the attribute conflict and attribute extremity effects exhibited by the experimental participants in Fischer et al. (2000). It also predicts an outcome value effect on error whose form depends on the shape of single-attribute functions and on the type of multiattribute combination rule.</description><author>Fischer, G. W.; Jia, J. M.; Luce, M. F.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technical note: Longitudinal performance stratification - An iterative Kolmogorov-Smirnov approach</title><link>http://www.example.com/articles/1</link><description>Ruefli, T. W.; Wiggins, R. R.
The stratification of entities into statistically distinct levels of performance over time is a problem encountered in a number of research and management settings. Traditional techniques to address this issue (e.g., cluster analysis) often require, either ex ante or ex post, the exogenous specification of the number of groups to be employed in further analysis-and are not especially suited to dealing with distributions over time. The methodology presented here iteratively applies the Kolmogorov-Smirnov two-sample test to identify the number and membership of statistically significantly different performance strata on a longitudinal basis. Monte Carlo simulations compare the new methodology with traditional clustering techniques. An application that stratifies mutual funds by returns illustrates the technique.</description><author>Ruefli, T. W.; Wiggins, R. R.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A decomposition-based pricing procedure for large-scale linear programs: An application to the linear multicommodity flow problem</title><link>http://www.example.com/articles/1</link><description>Mamer, J. W.; McBride, R. D.
We propose and test a new pricing procedure for solving large-scale structured linear programs. The procedure interactively solves a relaxed subproblem to identify potential entering basic columns. The subproblem is chosen to exploit special structure, rendering it easy to solve. The effect of the procedure is the reduction of the number of pivots needed to solve the problem. Our approach is motivated by the column-generation approach of Dantzig-Wolfe decomposition. We test our procedure on two sets of multicommodity flow problems. One group of test problems arises in routing telecommunications traffic and the second group is a set of logistics problem which have been widely used to test multicommodity flow algorithms.</description><author>Mamer, J. W.; McBride, R. D.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Scheduling resource-constrained projects competitively at modest memory requirements</title><link>http://www.example.com/articles/1</link><description>Sprecher, A.
We consider the resource-constrained project scheduling problem. The purpose of this paper is to direct and focus to a branch-and-bound concept that can, by simple adaptations, operate on a wide range of problem settings. The general approach can, e.g., deal with multimode problems, resource availability varying with time, and a wide range of objectives. Even the simple assembly Lint balancing problem of type-1 can be competitively approached with some modifications. Although the algorithm is the most general and simple one currently available for resource-constrained project scheduling, the computational performance can compete with the best approaches available for the single-mode problem. The algorithm uses far less memory than the state-of-the-art procedure, i.e., 256 KB versus 24 MB, for solving the standard benchmark set with projects consisting of 32 activities within comparable time. if both approaches are allowed to make limited use of memory, i.e., 256 KB, then more than 97% of the benchmark instances can be solved within fractions of the time required by the current state-of-the-art procedure. The truncated version of our algorithm achieves at 256 KB approximately the results of the truncated version of the state-of-the-art approach at 24 MB. Since in general the memory requirements exponentially grow with the number of activities the project consists of, memory will become a critical resource, and the strategy to access previously stored information will gain fundamental importance when solving larger projects.</description><author>Sprecher, A.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>bc-prod: A specialized branch-and-cut system for lot-sizing problems</title><link>http://www.example.com/articles/1</link><description>Belvaux, G.; Wolsey, L. A.
bc - prod is a prototype modelling and optimization system designed and able to tackle a wide variety of the discrete-time lot-sizing problems arising both in practice and in the literature. To use be - prod, the user needs to formulate his/her problem as a mixed integer program using XPRESS-MP's mp - model, a standard mathematical programming modelling language, taking into account a reserved set of key words for specific lot-sizing objects, such as production variables, storage, and demand data, etc. The problem is then solved by the XPRESS-MP branch-and-bound system including lot-sizing specific preprocessing, cutting planes for different aspects of lot-sizing problems, plus general cutting planes, and a lot-sizing-specific primal heuristic. Results are presented for a wide variety of big bucket and small bucket models with set-up and start-up costs and times.</description><author>Belvaux, G.; Wolsey, L. A.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A note on assemble-to-order systems with batch ordering</title><link>http://www.example.com/articles/1</link><description>Song, J. S.
We study an assemble-to-order inventory system. The stocks are held for components, with final products assembled only when customer orders are realized. Customer orders form a multivariate compound Poisson process, component replenishment leadtimes are constant, and demands in excess of inventory on hand are backlogged. The component inventories are controlled by (R, nQ) policies. We show that under certain general conditions the inventory position vector has a uniform equilibrium distribution. This result generalizes the corresponding single-item theory considerably. It allows us to express the key performance measures of the system, such as order fill rates and average order-based backorders, as the averages of their counterparts in the base-stock systems.</description><author>Song, J. S.</author><pubDate>Mon, 01 May 2000 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>