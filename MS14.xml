<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>MS14</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:27:11 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Application of the fast Gauss transform to option pricing</title><link>http://www.example.com/articles/1</link><description>Broadie, M.; Yamamoto, Y.
In many of the numerical methods for pricing American options based on the dynamic programming approach, the most computationally intensive part can be formulated as the summation of Gaussians. Though this operation usually requires O(NN') work when there are N' summations to compute and the number of terms appearing m each summation is N, we can reduce the amount of work to O(N + N') by using a technique called the fast Gauss transform. In this paper, we apply this technique to the multinomial method and the stochastic mesh method, and show by numerical experiments how it can speed up these methods dramatically, both for the Black-Scholes model and Merton's lognormal jump-diffusion model. We also propose extensions of the fast Gauss transform method to models with non-Gaussian densities.</description><author>Broadie, M.; Yamamoto, Y.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A generalization of Pratt-Arrow measure to nonexpected-utility preferences and inseparable probability and utility</title><link>http://www.example.com/articles/1</link><description>Nau, R. F.
The Pratt-Arrow measure of local risk aversion is generalized for the n-dimensional state-preference model of choice under uncertainty in which the decision maker may have inseparable subjective probabilities and utilities, unobservable stochastic prior wealth, and/or smooth nonexpected-utility preferences. Local risk aversion is measured by the matrix of derivatives of the decision maker's risk-neutral probabilities, without reference to true subjective probabilities or riskless wealth positions, and comparative risk aversion is measured without requiring agreement on true probabilities. Risk-neutral probabilities and their derivatives are shown to be sufficient statistics for approximately optimal investment and financing decisions in complete markets for contingent claims.</description><author>Nau, R. F.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Computing power indices for large voting games</title><link>http://www.example.com/articles/1</link><description>Leech, D.
Voting power indices enable the analysis of the distribution of power in a legislature or voting body in which different members have different numbers of votes. Although this approach to the measurement of power has been known for a long time, its application to large games has been limited by the difficulty of computing these indices. This paper presents a new method for computing power indices that combines exact methods with an approximate method due to Owen. This method is of most utility in situations where the number of players is large and the voting weights are concentrated in the hands of a small number of members.</description><author>Leech, D.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Mode locking and chaos in a deterministic queueing model with feedback</title><link>http://www.example.com/articles/1</link><description>Haxholdt, C.; Larsen, E. R.; van Ackere, A.
We consider a simple, deterministic queueing system with feedback, which exhibits the phenomena of sustained oscillation, mode locking, quasi-periodic behaviour, and chaos. This implies that a fully deterministic queueing system can exhibit seemingly unpredictable behaviour. We ignore variability, and focus on two forms of feedback: (i) the service rate increases as queue length increases, and (ii) the arrival rate depends on customers' perception of past waiting times. We model a customer's decision to seek service as a two-stage process: (i) deciding whether or not to use a facility, and (ii) deciding the frequency of visit (daily, weekly, monthly, etc.). This frequency is initially constant, and later on replaced by a deterministic, time-dependent pattern. Although highly stylised, this model captures the essential features of many real-life systems whose average arrival rate varies over time. Reducing the amplitude of cycles in demand makes the system more predictable and thus easier to manage. Although we represent this model as a queue of customers waiting for service, the model can be interpreted more generally as any situation where an increase in demand lowers the quality of service.</description><author>Haxholdt, C.; Larsen, E. R.; van Ackere, A.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>When private beliefs shape collective reality: The effects of beliefs about coworkers on group discussion and performance</title><link>http://www.example.com/articles/1</link><description>Kim, P. H.
The study presented in this paper examines how beliefs about coworkers affect group discussion and performance. Two beliefs are considered: (1) Perceptions of coworker task competence, and (2) achievement motivation. This study investigates whether these perceptions can actually hinder group discussion and performance, and considers the contexts in which these detrimental effects are more or less likely to arise. Results indicate that although perceptions of higher achievement motivation in coworkers lowered performance when task information was partially shared, they raised performance when task information was fully shared. A content analysis of group discussions reveals that the discussion behaviors examined by this study, rather than the more frequently examined bias toward discussing common information, mediated these results.</description><author>Kim, P. H.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quality improvement and infrastructure activity costs in software development: A longitudinal analysis</title><link>http://www.example.com/articles/1</link><description>Harter, D. E.; Slaughter, S. A.
This study draws upon theories of task interdependence and organizational inertia to analyze the effect of quality improvement on infrastructure activity costs in software development. Although increasing evidence indicates that quality improvement reduces software development costs, the impact on infrastructure activities is not known. Infrastructure activities include services like computer operations, data integration, and configuration management that support software development. Because infrastructure costs represent a substantial portion of firms' information technology budgets, it is important to identify innovations that yield significant cost savings in infrastructure activities. We evaluate quality and cost data collected in nine infrastructure activity centers over 10 years of product development in a major software firm undergoing a quality transformation. Findings indicate that infrastructure activities do benefit from quality improvement. The greatest marginal cost savings are realized in infrastructure activities that are highly interdependent with development and that occur later in the software development life cycle. Organizational inertia influences the rapidity with which the infrastructure activities benefit from higher product quality, especially for the more specialized activities. Finally, our findings suggest that although the savings in infrastructure from quality improvement are substantial, there are diminishing returns to quality improvement in infrastructure activities.</description><author>Harter, D. E.; Slaughter, S. A.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investment implications of information acquisition and leakage</title><link>http://www.example.com/articles/1</link><description>Dye, R. A.; Sridhar, S. S.
This paper studies when a firm will acquire additional information about a potential new project by consulting outsiders, when doing so runs the risk of reducing the value of implementing the project as a consequence of information leakage. The analysis evaluates the firm's information acquisition activities in both the presence and absence of moral hazard in project production.</description><author>Dye, R. A.; Sridhar, S. S.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Overcoming local search through alliances and mobility</title><link>http://www.example.com/articles/1</link><description>Rosenkopf, L.; Almeida, P.
Recent research suggests that, due to organizational and relational constraints, firms are limited contextually-both geographically and technologically-in their search for new knowledge. But distant contexts may offer ideas and insights that can be extremely useful to innovation through knowledge recombination. So how can firms reach beyond their existing contexts in their search for new knowledge? In this paper, we suggest that two mechanisms-alliances and the mobility of inventors-can serve as bridges to distant contexts and, thus, enable firms to overcome the constraints of contextually localized search. Through the analysis of patent citation patterns in the semiconductor industry, we first demonstrate both the geographic and technological localization of knowledge. We then explore if the formation of alliances and mobility of active inventors facilitate interfirm knowledge flows across contexts. We find that mobility is associated with interfirm knowledge flows regardless of geographic proximity and, in fact, the usefulness of alliances and mobility increases with technological distance. These findings suggest that firms can employ knowledge acquisition mechanisms to fill in the holes of their existing technological and geographic context.</description><author>Rosenkopf, L.; Almeida, P.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Problem-solving oscillations in complex engineering projects</title><link>http://www.example.com/articles/1</link><description>Mihm, J.; Loch, C.; Huchzermeier, A.
Coordination among many interdependent actors in complex product development projects is recognized as a key activity in organizational theory. It is well known that this coordination becomes progressively more difficult with project size, but we do not yet sufficiently understand whether this effect can be controlled with frequent and rich communication among project members, or whether it is inevitable. Recent work in complexity theory suggests that a project might form a "rugged landscape," for which performance deterioration with system size is inevitable. This paper builds a mathematical model of a complex design project that is divided into components (subproblems) and integrated back to the system. The model explicitly represents local component decisions, as well as component interactions in determining system performance. The model shows, first, how a rugged performance landscape arises even from simple components with simple performance functions, if the components are interdependent. Second, we characterize the dynamic behavior of the system analytically and with simulations. We show under which circumstances it exhibits performance oscillations or divergence to design solutions with low performance. Third, we derive classes of managerial actions available to improve performance dynamics, such as modularization, immediate communication, and exchanging preliminary information. Some of these have not yet received adequate attention in literature and practice.</description><author>Mihm, J.; Loch, C.; Huchzermeier, A.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The value of resource flexibility in the resource-constrained job assignment problem</title><link>http://www.example.com/articles/1</link><description>Vairaktarakis, G. L.
We consider the problem of minimizing project duration in an environment where each project activity can be executed by a number of different flexible resources. The capabilities of the flexible resources are modeled using a binary activity-resource matrix A called the availability matrix. Activity durations are known deterministically. We develop tight lower bounds and a variety of heuristics accompanied with extensive computational tests regarding their performance. It is shown that our algorithms consistently perform near optimally. Using these heuristics, we perform experiments on the effect of operating flexibility on project duration, where resource flexibility is measured by the number of resources available per activity, and the form of the availability matrix A. Our experiments lead to important managerial guidelines regarding the role of resource flexibility on project duration. For instance, it is found that when the flexible capabilities of the resources are evenly distributed across the activities, small improvements in resource flexibilities provide nearly the same benefits in project duration as a system of fully flexible resources.</description><author>Vairaktarakis, G. L.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capturing the risk-pooling effect through demand reshape</title><link>http://www.example.com/articles/1</link><description>Eynan, A.; Fouque, T.
The risk-pooling effect has been documented to benefit inventory systems by reducing the need for safety stock and consequently lowering costs such as inventory holding and shortage penalty. In this paper, we propose a new approach, called "demand reshape," to take advantage of the risk-pooling effect. It is demonstrated that a company can improve its profit by encouraging some of its customers, who intended to purchase one product to switch to another. The effectiveness of this approach is evaluated in various scenarios and found to be very promising.</description><author>Eynan, A.; Fouque, T.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A threshold inventory rationing policy for service-differentiated demand classes</title><link>http://www.example.com/articles/1</link><description>Deshpande, V.; Cohen, M. A.; Donohue, K.
Motivated by a study of the logistics systems used to manage consumable service parts for the U.S. military, we consider a static threshold-based rationing policy that is useful when pooling inventory across two demand classes characterized by different arrival rates and shortage (stockout and delay) costs. The scheme operates as a (Q, r) policy with the following feature. Demands from both classes are filled on a first-come-first-serve basis as long as on-hand inventory lies above a threshold level K. Once on-hand inventory falls below this level, low-priority (i.e., low shortage cost) demand is backordered while high-priority demand continues to be filled. We analyze this static policy first under the assumption that backorders are filled according to a special threshold clearing mechanism. Structural results for the key performance measures are established to enable an efficient solution algorithm for computing stock control and rationing parameters (i.e., Q, r, and K). Numerical results confirm that the solution under this special threshold clearing mechanism closely approximates that of the priority clearing policy. We next highlight conditions where our policy offers significant savings over traditional "round-up" and "separate stock" policies encountered in the military and elsewhere. Finally, we develop a lower bound on the cost of the optimal rationing policy. Numerical results show that the performance gap between our static threshold policy and the optimal policy is small in environments typical of the military and high-technology industries.</description><author>Deshpande, V.; Cohen, M. A.; Donohue, K.</author><pubDate>Sun, 01 Jun 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The importance of ordering in sequential auctions</title><link>http://www.example.com/articles/1</link><description>Elmaghraby, W.
To date, the largest part of literature on multi-unit auctions has assumed that there are k homogeneous objects being auctioned, where each bidder wishes to win exactly one or all of k units. These modeling assumptions have made the examination of ordering in sequential auctions inconsequential. The aim of this paper is to introduce and highlight the critical influence that ordering can have on the efficiency of an auction. We study a buyer who outsources via sequential 2nd-price auctions two heterogeneous jobs, and faces a diverse set of suppliers with capacity constraints.</description><author>Elmaghraby, W.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Salesforce compensation scheme and consumer inferences</title><link>http://www.example.com/articles/1</link><description>Kalra, A.; Shi, M.; Srinivasan, K.
We investigate the salesforce compensation strategy of a firm selling products in a category that several consumers find technically sophisticated, such as electronics or financial products with legal fine print. Consumers are unable to judge the value difference between a baseline product and a product upgrade with add-on features. While the firm and the salespeople are informed of the value of these features, consumers are uncertain. Thus consumers have to rely on sales assistance to evaluate alternatives. The salesperson decision variables include selling effort and whether to "oversell" the consumer by overclaiming the value of added features. Because sales revenue depends on both the salesperson's selling effort and consumers' valuation of the added features, the salesforce incentive scheme (which can consist of salary, sales commission, or consumer satisfaction-based commission) may induce the short-term oriented salesperson to misrepresent the value of the upgrade. Exaggeration of the value of the added features, however, results in reduced satisfaction levels leading to lower profits for the firm. We show that a salesperson selling products where the value of the upgrade is low prefers to make higher claims when the sales commission rate is sufficiently high. We conjecture that consumers aware of the incentive structure facing the salesperson expect the true value of the add-on feature to be lower than the claimed value. We study the optimal compensation scheme of a firm, which has to communicate her true type and retain its salesforce credibility. We identify the conditions under which a high-upgrade-type firm indicates its true value by altering sales commission rate and satisfaction-based commission rate.</description><author>Kalra, A.; Shi, M.; Srinivasan, K.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Predicting the equity premium with dividend ratios</title><link>http://www.example.com/articles/1</link><description>Goyal, A.; Welch, I.
Our paper suggests a simple, recursive residuals (out-of-sample) graphical approach to evaluating the predictive power of popular equity premium and stock market time-series forecasting regressions.. When applied, we find that dividend ratios should have been known to have no predictive ability even prior to the 1990s, and that any seeming ability even then was driven by only two years, 1973 and 1974. Our paper also documents changes in the time-series processes of the dividends themselves and shows that an increasing persistence of dividend-price ratio is largely responsible for the inability of dividend ratios to predict equity premia. Cochrane's (1997) accounting identity-that dividend ratios have to predict long-run dividend growth or stock returns-empirically holds only over horizons longer than 5-10 years. Over shorter horizons, dividend yields primarily forecast themselves.</description><author>Goyal, A.; Welch, I.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Newsvendor bounds and heuristic for optimal policies in serial supply chains</title><link>http://www.example.com/articles/1</link><description>Shang, K. H.; Song, J. S.
We consider the classic N-stage serial supply systems with linear costs and stationary random demands. There are deterministic transportation leadtimes between stages, and unsatisfied demands are backlogged. The optimal inventory policy for this system is known to be an echelon base-stock policy, which can be computed through minimizing, N nested convex functions recursively. To identify the key determinants, of the optimal policy, we, develop a simple and surprisingly good heuristic. This method minimizes 2N separate newsvendor-type cost functions, each of which uses the original problem data only. These functions are lower and upper bounds for the echelon cost functions; their minimizers form bounds for the optimal echelon base-stock levels. The heuristic is the simple average of the solution bounds. In extensive numerical experiments, the average relative error of the heuristic is 0.24%, with the maximum error less than 1.5%. The bounds and the, heuristic, which can be easily obtained by simple spreadsheet calculations, enhance the accessibility and implementability of the multiechelon inventory theory. More importantly, the closed form expressions provide an analytical tool for us to gain insights into issues such as system bottlenecks, effects of system parameters, and coordination mechanisms in decentralized systems.</description><author>Shang, K. H.; Song, J. S.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The utilization of competing technologies within the firm: Evidence from cardiac procedures</title><link>http://www.example.com/articles/1</link><description>Huckman, R. S.
This paper examines the role of technological status in determining the rates at which competing techniques are used within a firm. Consistent with prior studies, technological status is measured on the basis of an actor's prior contributions to the body of knowledge concerning a given technique. The empirical analysis considers two treatments for coronary artery disease (CAD), each of which is associated with a distinct professional group within a hospital. These two groups are often characterized as engaging in a "turf war" for patients. After controlling for several factors that might explain technological choice-the clinical severity of patients, the relative quality of the two procedures at a given facility, firm-level financial performance, and other firm-level characteristics-I find that the technological status of the group associated with each technique affects the relative rate at which it is used within a given hospital. Moreover, this effect is strongest for patients at the margin between the two techniques. These results suggest that viewing the choice between competing innovations as a single, firm-level decision may not always capture the true dynamics underlying such a situation.</description><author>Huckman, R. S.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>How communication links influence coalition bargaining: A laboratory investigation</title><link>http://www.example.com/articles/1</link><description>Bolton, G. E.; Chatterjee, K.; McGinn, K. L.
Complexity of communication is one of the important factors that distinguishes multilateral negotiation from its bilateral cousin. We investigate how the communication configuration affects a three-person coalition negotiation. Restricting who can communicate with whom strongly influences outcomes, and not always in ways that current theory anticipates. Competitive frictions, including a tendency to communicate offers privately, appear to shape much of what we observe. Our results suggest that parties with weaker alternatives would benefit from a more constrained structure, especially if they can be the conduit of communication, while those endowed with stronger alternatives would do well to work within a more public communication structure that promotes competitive bidding.</description><author>Bolton, G. E.; Chatterjee, K.; McGinn, K. L.</author><pubDate>Thu, 01 May 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The data of Levy and Levy (2002) "Prospect theory: Much ado, about nothing?" - Actually support prospect theory</title><link>http://www.example.com/articles/1</link><description>Wakker, P. P.
Levy and-Levy (Management Science 2002) p resent data that, according to their claims, violate prospect theory. They suggest that prospect theory's hypothesis of an S-shaped value function, concave for gains and convex for losses, is incorrect. However, all the data of Levy and Levy are perfectly consistent with the predictions of prospect theory, as can be verified by simply applying prospect theory formulas. The mistake of Levy and Levy is that they, incorrectly, thought that probability weighting could be ignored.</description><author>Wakker, P. P.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Term structure of interest rates and implied market frictions: The min-max approach</title><link>http://www.example.com/articles/1</link><description>Ioffe, I. D.; Prisman, E. Z.
It is often assumed that financial markets are frictionless. Bond markets are illiquid and bond prices are observed with errors. The magnitude of these errors leads to violation of no-arbitrage conditions and, consequently, prevents researchers from obtaining an estimate of the term structure (TS) of interest rates. Researchers have had to settle for a second-best estimate of the TS (e.g., obtained via regression) at a cost of an economically unrealistic assumption of symmetric market frictions. The true shape of market frictions, however, is not known and generally is a highly complex issue. A no-arbitrage-based methodology that avoids making detrimental assumptions is developed here. It facilitates empirical investigation of the shape of the market frictions and of the TS that are simultaneously imputed from market data assuming "efficient" market frictions that minimize the maximum net arbitrage. The empirical investigation performed in the Canadian and U.S. markets shows that in both markets the frictions are asymmetric and the estimates of the TS produced via regression and our methodology significantly differ.</description><author>Ioffe, I. D.; Prisman, E. Z.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Solving the convex cost integer dual network flow problem</title><link>http://www.example.com/articles/1</link><description>Ahuja, R. K.; Hochbaum, D. S.; Orlin, J. B.
In this paper,(1) we consider an integer convex optimization problem where the objective function is the sum of separable convex functions (that is, of the form Sigma((i,j)is an element ofQ) (F) over bar (ij)(omega(ij)) + Sigma(iis an element ofP) (B) over bar (i)(mu(i))), the constraints. are similar to those arising in the dual of a minimum cost flow problem (that is, of the form mu(i) - mu(j) less than or equal to omega(ij), (i, j) is an element of Q), with lower and upper bounds on variables. Let n = \P\, m = \Q\, and U be the largest magnitude in the lower and upper bounds of variables. We call this problem the convex cost integer dual network flow problem. In this paper, we describe several applications of the convex cost integer dual network flow problem arising in a dial-a-ride transit problem, inverse spanning tree problem, project management and regression analysis. We develop network flow-based algorithms to solve the convex cost integer dual network flow problem. We show that using the Lagrangian relaxation technique, the convex cost integer dual network flow problem can be transformed into a convex cost primal network flow problem where each cost function is a piecewise linear convex function with integer slopes. Its special structure allows the convex cost primal network flow problem to be solved in O(nm log(n(2)/m)log(nU)) time. This time bound is the same time needed to solve the minimum cost flow problem using the cost-scaling algorithm, and is also is best available time bound to solve the convex cost integer dual network flow problem.</description><author>Ahuja, R. K.; Hochbaum, D. S.; Orlin, J. B.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Subjective rationality, self-confirming equilibrium, and corporate strategy</title><link>http://www.example.com/articles/1</link><description>Ryall, M. D.
This paper presents a formal theory of subjective rationality and demonstrates its application to corporate strategy. An agent is said to be subjectively rational when decisions are consistent with the available facts and, where these are lacking, with the agent's own subjective assessments. A self-confirming equilibrium arises when agents' subjectively rational actions generate events that are consistent with their own expectations. Equilibrium strategies may be suboptimal because certain counterfactual beliefs may be erroneous and yet fail to be contradicted by events observed in equilibrium. This weakening of the stronger rationality assumptions inherent in many of the more familiar equilibrium ideas appears well suited to applications in strategy. In particular, performance advantage may be sustained by a firm when its subjectively rational competitors persistently employ suboptimal self-confirming strategies.</description><author>Ryall, M. D.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inferring infection transmission parameters that influence water treatment decisions</title><link>http://www.example.com/articles/1</link><description>Chick, S. E.; Soorapanth, S.; Koopman, J. S.
One charge of the United States Environmental Protection Agency is to study the risk of infection for microbial agents that can be disseminated through drinking water systems and to recommend water treatment policy to counter that risk. Recently proposed dynamical system models quantify indirect risks due to secondary transmission, in addition to primary infection risk from the water supply considered by standard assessments. Unfortunately, key parameters that influence water treatment policy are unknown, in part because of lack of data and effective inference methods. This paper develops inference methods for those parameters by using stochastic process models to better incorporate infection dynamics into the inference process. Our use of endemic data provides an alternative to waiting for, identifying; and measuring an outbreak. Data both from: simulations and from New York City illustrate the approach.</description><author>Chick, S. E.; Soorapanth, S.; Koopman, J. S.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Process flexibility in supply chains</title><link>http://www.example.com/articles/1</link><description>Graves, S. C.; Tomlin, B. T.
Process flexibility whereby a production facility can produce multiple products, is a critical design consideration in multiproduct supply chains facing uncertain demand. The challenge is to determine a cost-effective flexibility configuration that is able to meet the demand with high likelihood. In this paper, we present a framework for analyzing the benefits from flexibility in multistage supply chains. We find two phenomena, stage-spanning bottlenecks and floating bottlenecks, neither of which are present in single-stage supply chains, which reduce the effectiveness of a flexibility configuration. We. develop a flexibility measure g,and show that increasing this measure results in,greater protection from these supply-chain inefficiencies. We also identify flexibility guidelines that perform very well for multistage supply chains. These guidelines employ and adapt the single-stage chaining strategy of Jordan and Graves (1995) to multistage supply chains.</description><author>Graves, S. C.; Tomlin, B. T.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The logistics impact of a mixture of order-streams in a manufacturer-retailer system</title><link>http://www.example.com/articles/1</link><description>Iyer, A. V.; Jain, A.
We model a supply chain with two retail warehouses that place replenishment orders with a common manufacturing capacity. The two,retailers differ in the variability of their order-streams. The order-stream from one retail warehouse is modeled as a Poisson process and from the other as a hyperexponential renewal process. Each retail warehouse uses a base-stock policy to place replenishment orders with, the manufacturer. The manufacturer is modeled as a first-come-first-serve, single exponential server queue. We analyze the supply-side impact of this mixture of order-streams received by the manufacturer on both retailers. An exact analysis of this base-model generates closed-form expressions for distributions of the. lead-time, outstanding orders, and expected inventory. costs for each retailer, and leads to comparative results about the two retailers' performance measures. The base-model is extended to accommodate finished goods at the manufacturer, more than two,, retailers, and bulk-arrivals. We use the model to suggest managerial insights about the impact of the presence of a high-variability retailer on other retailers who share capacity, the distorting impact of manufacturer finished goods inventory on retailer incentives, and the incentives for retailers to participate in variability-reduction programs in the grocery industry.</description><author>Iyer, A. V.; Jain, A.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customer satisfaction in virtual environments: A study of online investing</title><link>http://www.example.com/articles/1</link><description>Balasubramanian, S.; Konana, P.; Menon, N. M.
Many firms are moving to make virtual interfaces their primary, or even sole, points of customer contact In this environment, some traditional service quality dimensions that. determine customer satisfaction, such as the physical appearance of facilities, employees, and equipment, and employees' responsiveness and empathy, are unobservable. In contrast, trust may play a central role here, in enhancing customer satisfaction. We model trust as an endogenously, formed, entity that ultimately impacts customer satisfaction, and we elucidate the linkages between trust and other factors related to the performance of the online service provider, and to the service environment. The model is validated using two samples-one comprising 225 online investors of a large online broker,and the other comprising;03 members of the American Association of Individual Investors,(AAII). The findings suggest that perceived trustworthiness of an online broker is a significant antecedent to investors' satis faction, and. that perceived. environmental security and perceived operational competence impact the formation of trust. The results have important managerial implications.</description><author>Balasubramanian, S.; Konana, P.; Menon, N. M.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conceptualizing and measuring variety in the execution of organizational work processes</title><link>http://www.example.com/articles/1</link><description>Pentland, B. T.
Variability in organizational work processes is believed to influence productivity quality, flexibility, and a host of other aspects of organizational design and management, but. this construct has never been Clearly conceptualized and measured. This paper introduces the concept of sequential variety which accounts for variability in the sequence of events or actions that make up a process. This paper also proposes and compares three measures of sequential variety in organizational processes: Average distance (based on optimal string matching), algorithmic complexity, and deviation from uniform, random Markov. These measures are compared and validated using a simulated data set that embodies the range of variation likely to be encountered in empirical studies. All. three measures correlate well and provide useful indicators of sequential variety, but. the measures based on optimal string matching, and deviation from the uniform, random Markov seem likely to be more useful in various potential applications.</description><author>Pentland, B. T.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Gatekeepers and referrals in services</title><link>http://www.example.com/articles/1</link><description>Shumsky, R. A.; Pinker, E. J.
This paper examines services in which customers encounter a gatekeeper who makes, an initial diagnosis of the customer's problem and then may refer the customer to a specialist, The gatekeeper may also attempt to solve the-problem, but the probability of treatment success decreases as the problem's complexity increases. Given the costs of treatment by the gatekeeper and. the specialist, we find. the firm.'s. optimal referral rate from a particular gatekeeper to the specialists. We then consider the principal-agent problem that arises when the gatekeeper, but not the firm, observes the gatekeeper's treatment ability as well as the complexity of each customer's problem. We examine the relative benefits of compensation systems designed to overcome the effects of this information asymmetry and show that, bonuses based solely on referral rates do not always ensure first-best system performance and that an appropriate bonus based on, customer volume may be necessary as well. We also consider the value of such output-based contracts when gatekeepers are, heterogeneous in ability so, that. two gatekeeper types face different probabilities of treatment success when given the same problem. We,show that the firm may achieve first-best performance by either offering. two contracts that separate the gatekeeper types or by offering a single contract that coordinates the treatment decisions of both gatekeepers.</description><author>Shumsky, R. A.; Pinker, E. J.</author><pubDate>Tue, 01 Jul 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Group decisions with multiple criteria</title><link>http://www.example.com/articles/1</link><description>Baucells, M.; Sarin, R. K.
We consider a decision problem where a group of individuals evaluates multiattribute alternatives. We explore the minimal required agreements that are sufficient to specify the group utility function. A surprising result is that, under some conditions, a bilateral agreement among pairs of individuals on a single attribute is sufficient to derive the multiattribute group utility. The bilateral agreement between a pair of individuals could be on the weight of an attribute, on an attribute evaluation function, or on willingness to pay. We focus on the case in which each individual's utility function is additive. We show that the group utility can be represented as the weighted sum of group attribute weights and, more remarkably, of attribute evaluation functions. These group attribute evaluation functions are in turn weighted sums of individual attribute evaluation functions.</description><author>Baucells, M.; Sarin, R. K.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Solving project scheduling problems by minimum cut computations</title><link>http://www.example.com/articles/1</link><description>Mohring, R. H.; Schulz, A. S.; Stork, F.; Uetz, M.
In project scheduling, a set of precedence-constrained jobs has to be scheduled so as to minimize a given objective. In resource-constrained project scheduling, the jobs additionally compete for scarce resources. Due to its universality, the latter problem has a variety of applications in manufacturing, production planning, project management, and elsewhere. It is one of the most intractable problems in operations research, and has therefore become a popular playground for the latest optimization techniques, including virtually all local search paradigms. We show that a somewhat more classical mathematical programming approach leads to both competitive feasible solutions and strong lower bounds, within reasonable computation times. The basic ingredients of our approach are the Lagrangian relaxation of a time-indexed integer programming formulation and relaxation-based list scheduling, enriched with a useful idea from recent approximation algorithms for machine scheduling problems. The efficiency of the algorithm results from the insight that the relaxed problem can be solved by computing a minimum cut in an appropriately defined directed graph. Our computational study covers different types of resource-constrained project scheduling problems, based on several notoriously hard test sets, including practical problem instances from chemical production planning.</description><author>Mohring, R. H.; Schulz, A. S.; Stork, F.; Uetz, M.</author><pubDate>Sat, 01 Mar 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Using neural network rule extraction and decision tables for credit-risk evaluation</title><link>http://www.example.com/articles/1</link><description>Baesens, B.; Setiono, R.; Mues, C.; Vanthienen, J.
Credit-risk evaluation is a very challenging and important management science problem in the domain of financial analysis. Many classification methods have been suggested in the literature to tackle this problem. Neural networks, especially, have received a lot of attention because of their universal approximation property. However, a major drawback associated with the use of neural networks for decision making is their lack of explanation capability. While they can achieve a high predictive accuracy rate, the reasoning behind how they reach their decisions is not readily available. In this paper, we present the results from analysing three real-life credit-risk data sets using neural network rule extraction techniques. Clarifying the neural network decisions by explanatory rules that capture the learned knowledge embedded in the networks can help the credit-risk manager in explaining why a particular applicant is classified. as either bad or good. Furthermore, we also discuss how these rules can be visualized as a decision table in a compact and intuitive graphical format that facilitates easy consultation. It is concluded that neural network rule extraction and decision tables are powerful management tools that allow us to build advanced and user-friendly decision-support systems for credit-risk evaluation.</description><author>Baesens, B.; Setiono, R.; Mues, C.; Vanthienen, J.</author><pubDate>Sat, 01 Mar 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Innovation and strategic divergence: An empirical study of the US pharmaceutical industry from 1920 to 1960</title><link>http://www.example.com/articles/1</link><description>Lee, J.
Today, firms employing two distinct survival strategies-(l) innovation and (2) imitation-coexist in the U.S. pharmaceutical industry. History indicates that this intraindustry heterogeneity did not exist prior to 1940. This study empirically investigates the origin of this strategic divergence by focusing on changes in firms' R&amp;D inputs and outputs. It finds that some U.S. pharmaceutical firms responded to the opportunity presented by the discovery of antibiotics in the 1940s by investing more in R&amp;D, while many others did not. Over time, the innovators dominated in developing new drugs, and the gap between innovators and imitators steadily increased. These findings also shed light on "the genesis of strategic groups," a phenomenon that is not yet well understood.</description><author>Lee, J.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance impacts of information technology: Is actual usage the missing link?</title><link>http://www.example.com/articles/1</link><description>Devaraj, S.; Kohli, R.
The relationship between investment in information technology (IT) and its effect on organizational performance continues to interest academics and practitioners. In many cases, due to the nature of the research design employed, this stream of research has been unable to identify the impact of individual technologies on organizational performance. This study posits that the driver of IT impact is not the investment in the technology, but the actual usage of the technology. This proposition is tested in a longitudinal setting of a healthcare system comprising eight hospitals. Monthly data for a three-year period on various financial and nonfinancial measures of hospital performance and technology usage were analyzed. The data analysis provides evidence for the technology usage-performance link after controlling for various external factors. Technology usage was positively and significantly associated with measures of hospital revenue and quality, and this effect occurred after time lags. The analysis was triangulated using three measures of technology usage. The general support for the principal proposition of this paper that "actual usage" may be a key variable in explaining the impact of technology on performance suggests that omission of this variable may be a missing link in IT payoff analyses.</description><author>Devaraj, S.; Kohli, R.</author><pubDate>Sat, 01 Mar 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Relative performance of incentive mechanisms: Computational modeling and simulation of delegated investment decisions</title><link>http://www.example.com/articles/1</link><description>Raghu, T. S.; Sen, P. K.; Rao, H. R.
This paper evaluates the relative performances of several well-known and widely-used incentive mechanisms under controlled experimental conditions. The scenario utilized is a delegated investment setting where effort and risk aversions contribute to moral hazard among fund managers. Analytical intractability of the problem requires a computational modeling approach to simulate comparative solutions for specific contracts under different parametric settings. Through a simulation exercise, we consider multiple agents who decide their investment strategy over several consecutive periods. Agents learn about estimation and market uncertainty through repeated realizations of investment returns. In each sequence of periods, a number of different incentive mechanisms based on the agent's communication and/or outcome are considered. Results of the computational experiments are presented. Our results overwhelmingly show the efficacy of the incentive contracts in improving the welfare of the investors. In the presence of an estimation risk, when agents learn from their past performances, the market volatility interacts with the estimation risk that makes risk-sharing arrangements such as limited liability overly important. Paying the agent to assume the risk may no longer lead to the best performance incentives.</description><author>Raghu, T. S.; Sen, P. K.; Rao, H. R.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Timber harvesting as a part of the portfolio management: A multiperiod stochastic optimisation approach</title><link>http://www.example.com/articles/1</link><description>Heikkinen, V. P.
A multiperiod stochastic optimization model is formulated for a land owner who can speculate between investing harvesting income in financial assets and postponing harvesting. This paper demonstrates the benefits from using a multiperiod model, the effects of cointegration on optimal portfolio, and the differences between the timber harvesting model and the standard financial portfolio optimisation model. The demonstrations are made partly by using a real Finnish forest and price data, and partly by using artificial data. In the real data example, the system is demonstrated using a case where it is assumed that the land owner has several mature forest stands, which can be harvested at any time during the next 3 years. Investment alternatives are stocks, government bonds, and bank deposits. The forestry returns were defined as a sum of exponential physical growth and stumpage price return. The chosen definition of forestry returns makes the model very useful, for example, when speculating on what speed of physical growth is needed to make forestry a competitive investment alternative when both returns and risks are considered.</description><author>Heikkinen, V. P.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Truth or consequences: An experiment</title><link>http://www.example.com/articles/1</link><description>Brandts, J.; Charness, G.
This paper presents evidence that the willingness to punish an unfair action is sensitive to whether this action was preceded by a deceptive message. One player first senda a message indicating an intended play, which is either favorable or unfavorable to the other player in the game. After the message, the sender, and the receiver play a simultaneous 2 x 2 game, in which the sender may or may not play according to his message. Outcome cells may, hence, be reached following true or false messages. In the third stage, the receiver may (at a cost) punish or reward, depending on which cell of the simultaneous game has been reached. We test whether receivers' rates of monetary sacrifice depend on the process by which an outcome is reached. We study two decision-elicitation methods: the strategy and the direct response methods. For each method, deception more than doubles the punishment rate as a response to an action that is unfavorable to the receiver. We also find evidence that 17-25% of all participants choose to reward a favorable action choice made by the sender, even though doing so leaves one at a payoff disadvantage. Our results reflect on current economic models of utility and have implications for organizational decision-making behavior.</description><author>Brandts, J.; Charness, G.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal conflict in preference assessment</title><link>http://www.example.com/articles/1</link><description>Delquie, P.
Conflict arises in decision making when the choice alternatives present strong advantages and disadvantages over one another, that is, when the trade-offs involved are large. Conflict affects human response to choice, in particular, it increases decision difficulty and response unreliability. On the other hand, larger trade-offs, i.e., higher conflict, reveal more information about an individual's preferences and mitigate the influence of measurement unreliability on preference model estimation. This suggests, somewhat counterintuitively, that there may exist some optimal level of conflict for efficient measurement of preferences. How to determine this level? This issue is examined from behavioral and analytical angles. We outline a general analysis of the interaction between trade-off size and modeling accuracy, and demonstrate its application on a simple example. The kind of analysis developed here can be conveniently implemented in a computer spreadsheet, and would be especially valuable when large amounts of preference data are to be collected, as in consumer preference studies, experimental research, and contingent valuation surveys.</description><author>Delquie, P.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Analysis and design of business-to-consumer online auctions</title><link>http://www.example.com/articles/1</link><description>Bapna, R.; Goes, P.; Gupta, A.
Business-to-consumer online auctions form an important element in the portfolio of mercantile processes that facilitate electronic commerce activity. Much of traditional auction.. theory has focused on analyzing single-item auctions in isolation from the market context in which they take place. We demonstrate the weakness of such approaches in online settings where a majority of auctions are multiunit in nature. Rather than pursuing a classical approach and assuming knowledge of the distribution of consumers' valuations, we emphasize the largely ignored discrete and sequential nature of such auctions. We derive a general expression that characterizes the multiple equilibria that can arise in such auctions and segregate these into desirable and undesirable categories. Our analytical and empirical results,, obtained by tracking real-world online auctions, indicate that bid increment is an important factor amongst the control factors that online auctioneers can manipulate and control. We show that consumer bidding strategies in such auctions-are not-uniform and that the level of bide increment chosen influences them. With a motive of providing concrete strategic directions to online auctioneers, we derive an absolute upper bound for the bid increment. Based on the theoretical upper bound we propose a heuristic decision rule for setting the bid increment. Empirical evidence lends support to the hypothesis that setting a bid increment higher than that suggested by the heuristic decision rule has a negative impact on the auctioneer's revenue.</description><author>Bapna, R.; Goes, P.; Gupta, A.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Is subsidizing inefficient bidders actually costly?</title><link>http://www.example.com/articles/1</link><description>Rothkopf, M. H.; Harstad, R. M.; Fu, Y. H.
A widespread practice, particularly in public-sector procurement and dispersal, is to subsidize a class of competitors believed to be at an economic disadvantage. Arguments for such policies vary, but they typically assume that benefits of subsidization must be large enough to outweigh a presumed economic cost of the subsidy. When disadvantaged competitors compete in auctions, the subsidy serves to. make them more competitive rivals. Other bidders rationally respond by bidding more aggressively. We consider a model of procurement auctions and show that a policy of subsidizing inefficient competitors can lower expected project cost and also enhance economic efficiency. Some subsidy is generally better than no subsidy for-a wide range of parameters.</description><author>Rothkopf, M. H.; Harstad, R. M.; Fu, Y. H.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investment under uncertainty in information technology: Acquisition and development projects</title><link>http://www.example.com/articles/1</link><description>Schwartz, E. S.; Zozaya-Gorostiza, C.
In this paper, we develop two models for the valuation-of information technology (IT) investment projects using the real options approach. The IT investment projects discussed in this paper are categorized into development and acquisition projects, depending upon the time it takes to start benefiting from the IT asset once the decision to invest has been taken. The models account for uncertainty both in the costs and benefits associated with the investment opportunity. Our stochastic cost function for IT development projects incorporates the technical and input cost uncertainties of Pindyck's model (1993), but also considers the fact that the investment costs of some IT projects might change even if no investment takes place. In contrast to other models in the real options literature in which benefits are summarized in the underlying asset value, our model for IT acquisition projects represents these benefits as a stream of stochastic cash flows.</description><author>Schwartz, E. S.; Zozaya-Gorostiza, C.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning by doing something else: Variation, relatedness, and the learning curve</title><link>http://www.example.com/articles/1</link><description>Schilling, M. A.; Vidal, P.; Ployhart, R. E.; Marangoni, A.
Many organizational learning studies have an implicit assumption that the learning rate is maximized through specialization: the more an individual or organization focuses on a particular task, the faster it will improve. However, through contrasting the various learning process theories described in the research on organizational, group, and individual learning, we develop a set of competing hypotheses that suggest some degree of variation might improve the learning rate. Furthermore, such comparison yields competing arguments about how related or unrelated such task variation should be to improve the learning rate. This research uses an experimental study to answer the following research questions: Is the learning rate maximized through specialization? Or does variation, related or unrelated, enhance the learning process? We find that the learning rate under conditions of related variation is significantly greater than under conditions of specialization or unrelated variation, indicating the possibility of synergy between related learning efforts consistent with an implicit learning or insight effect. We find no significant differences in the rates of learning under the conditions of specialization and unrelated variation. These results yield important implications for how work should be organized, and for future research into the learning process.</description><author>Schilling, M. A.; Vidal, P.; Ployhart, R. E.; Marangoni, A.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Individual centrality and performance in virtual R&amp;D groups: An empirical study</title><link>http://www.example.com/articles/1</link><description>Ahuja, M. K.; Galletta, D. F.; Carley, K. M.
Communication technologies support virtual R&amp;D groups by enabling immediate and frequent interaction of their geographically-distributed members. Performance of members in such groups has yet to be studied longitudinally. A model proposes not only direct effects of. functional role, status, and communication role on individual performance, but also indirect effects through individual centrality. Social network analysis was performed on e-mail samples from two time periods separated by four years. Analysis revealed both direct and indirect effects as hypothesized; however, the indirect effects were more consistent in both time periods, The clearest findings were that centrality mediates the effects of functional role, status., and communication role on individual performance. Interestingly, centrality was a stronger direct predictor of performance than the individual characteristics considered in this study. The study illustrates the usefulness of accounting for network effects for better understanding individual performance in virtual groups.</description><author>Ahuja, M. K.; Galletta, D. F.; Carley, K. M.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>From the editor</title><link>http://www.example.com/articles/1</link><description>Hopp, W. J.
nan</description><author>Hopp, W. J.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Direct-marketing, indirect profits: A strategic analysis of dual-channel supply-chain design</title><link>http://www.example.com/articles/1</link><description>Chiang, W. Y. K.; Chhajed, D.; Hess, J. D.
The advent of e-commerce has prompted many-manufacturers to redesign their traditional channel structures by engaging in direct sales. The model conceptualizes the impact-of customer acceptance of a direct channel, the degree to which customers accept a direct channel as a substitute for. shopping at a traditional store, on supply-chain design. The customer acceptance of a direct channel can be strong enough that an indepent manufacturer Would open a direct channel to, compete with its own retailers. Here, direct marketing is used for strategic channel control purposes even though it is inefficient on its own and, surprisingly, it can profit the manufacturer even when so direct sales occur. Specifically, we construct a price-setting game between a manufacturer and its independent retailer. Direct marketing, which indirectly increases the flow of profits through the retail channel, helps the manufacturer improve overall Profitability by reducing the degree of inefficient price double marginalization. While operated by the manufacturer to constrain the retailer's pricing behavior, the direct channel may-not always be detrimental to the retailer because it will be accompanied by a wholesale price reduction. This combination of manufacturer pull and push can benefit the retailer in equilibrium. Finally, we show that the mere threat of introducing the direct channel can increase the manufacturer's negotiated share of cooperative profits even if price efficiency is obtained by using other business practices.</description><author>Chiang, W. Y. K.; Chhajed, D.; Hess, J. D.</author><pubDate>Wed, 01 Jan 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note: Optimal policies for serial inventory systems under fill rate constraints</title><link>http://www.example.com/articles/1</link><description>Axsater, S.
A continuous review serial production/distribution system with discrete compound Poisson demand for the end product is considered. Unmet demand is back-ordered. Production/transportation times are constant. All deliveries from one stage to the next must be multiples of given batch sizes. We consider the problem of minimizing the holding costs under a fill rate constraint. Using recent results by Chen (2000), we show that under a set of restricted but plausible assumptions, the optimal policy is an echelon stock multistage (R, nQ) policy with one of the reorder points varying over time. We provide a simple procedure for the determination of the optimal policy.</description><author>Axsater, S.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Probabilistic error bounds for simulation quantile estimators</title><link>http://www.example.com/articles/1</link><description>Jin, X.; Fu, M. C.; Xiong, X. P.
Quantile estimation has become increasingly important, particularly in the financial industry, where value at risk (VaR) has emerged as a standard measurement tool for controlling portfolio risk. In this paper, we analyze the probability that a simulation-based quantile estimator fails to lie in a prespecified neighborhood of the true quantile. First, we show that this error probability converges to zero exponentially fast with sample size for negatively dependent sampling. Then we consider stratified quantile estimators and show that the error probability for these estimators can be guaranteed to be 0 with sufficiently large, but finite, sample size. These estimators, however, require sample sizes that grow exponentially in the problem dimension. Numerical experiments on a simple VaR example illustrate the potential for variance reduction.</description><author>Jin, X.; Fu, M. C.; Xiong, X. P.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Old is gold? The value of temporal exploration in the creation of new knowledge</title><link>http://www.example.com/articles/1</link><description>Nerkar, A.
In this paper, knowledge creation is considered as a path-dependent evolutionary process that involves recombining knowledge spread over time. The findings of the paper suggest that a balance in combining current knowledge with the knowledge available across large time spans is an important factor that explains the impact of new knowledge. These ideas are empirically tested using patent data from the pharmaceutical industry. Results from the analysis offer support for the hypotheses developed in the paper</description><author>Nerkar, A.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The financial rewards of new product introductions in the personal computer industry</title><link>http://www.example.com/articles/1</link><description>Bayus, B. L.; Erickson, G.; Jacobson, R.
Based on data from firms in the personal computer industry, we study the effect of new product introductions on three key drivers of firm value: profit rate, profit-rate persistence, and firm size as reflected in asset growth. Consistent with our theoretical development, we find that new product introductions influence profit rate and size; however, we find no effect on profit-rate persistence. Interestingly, we also find that the effect of new product introductions on profit rate stems from a reduction in selling and general administrative expenditure intensity rather than through an increase in gross operating return. Notably, firms decrease their advertising intensity in the wake of a new product introduction. Firm profitability in this industry apparently benefits from new product introductions because new products need less marketing support than older products.</description><author>Bayus, B. L.; Erickson, G.; Jacobson, R.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A Bayesian model for prelaunch sales forecasting of recorded music</title><link>http://www.example.com/articles/1</link><description>Lee, J.; Boatwright, P.; Kamakura, W. A.
In a situation where several hundred new music albums are released each month, producing sales forecasts in a reliable and consistent manner is a rather difficult and cumbersome task. The purpose of this study is to obtain sales forecasts for a new album before it is introduced. We develop a hierarchical Bayesian model based on a logistic diffusion process. It allows for the generalization of various adoption patterns out of discrete data and can be applied in a situation where the eventual number of adopters is unknown. Using sales of previous albums along with information known prior to the launch of a new album, the model constructs informed priors, yielding prelaunch sales forecasts, which are out-of-sample predictions. In the context of new product forecasting before introduction, the information we have is limited to the relevant background characteristics of a new album. Knowing only the general attributes of a new album, the meta-analytic approach proposed here provides an informed prior on the dynamics of duration, the effects of marketing variables, and the unknown market potential. As new data become available, weekly sales forecasts and market size (number of eventual adopters) are revised and updated. We illustrate our approach using weekly sales data of albums that appeared in Billboard's Top 200 albums chart from January 1994 to December 1995.</description><author>Lee, J.; Boatwright, P.; Kamakura, W. A.</author><pubDate>Sat, 01 Feb 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Balancing search and stability: Interdependencies among elements of organizational design</title><link>http://www.example.com/articles/1</link><description>Rivkin, J. W.; Siggelkow, N.
We examine how and why elements of organizational design depend on one another. An agent-based simulation allows us to model three design elements and two contextual variables that have rarely been analyzed jointly: a vertical hierarchy that reviews proposals from subordinates, an incentive system that rewards subordinates for departmental or firm-wide performance, the decomposition of an organization's many decisions into departments, the underlying pattern of interactions among decisions, and limits on the ability of managers to process information. Interdependencies arise among these features because of a basic, general tension. To be successful, an organization must broadly search for good sets of decisions, but it must also stabilize around good decisions once discovered. An effective organization balances search and stability. We identify sets of design elements that encourage broad search and others that promote stability. The adoption of elements that encourage broad search typically raises the marginal benefit of other elements that provide offsetting stability. Hence, the need to balance search and stability generates interdependencies among the design elements. We pay special attention to interdependencies that involve the vertical hierarchy. Our findings confirm :many aspects of conventional wisdom about vertical hierarchies, but challenge or put boundary conditions on others. We place limits, for instance, on the received wisdom that firm-wide incentives and capable subordinates make top-level oversight less valuable. We also identify circumstances in which vertical hierarchies can lead to inferior long-term performance.</description><author>Rivkin, J. W.; Siggelkow, N.</author><pubDate>Sat, 01 Mar 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Replenishment strategies for distribution systems under advance demand information</title><link>http://www.example.com/articles/1</link><description>Ozer, O.
Customers with positive demand lead times place orders in advance of their needs. A portfolio of customers with different demand lead times gives rise to what we call advance demand information. We develop effective inventory policies for a distribution system to account for this information. In particular, we study a centralized system with one warehouse serving multiple retailers under advance demand information. The inventory manager replenishes the warehouse from an outside supplier: Units arriving to the warehouse are allocated to the retailers. To control this system, we develop a lower bound and proposed a close-to-optimal heuristic for which the optimality gap is on average 1.92%. We also provide a closed-form solution to approximate the system-wide inventory level. Using this explicit solution, the model and the heuristic, we investigate (1) the benefit of advance demand information, and its impact on allocation decisions, (2) the joint role of risk pooling and advance demand information, and (3) the system performance with respect to supplier and retailer lead times. We illustrate how advance demand information can be a substitute for lead times and inventory, and how it enhances the outcome of delayed differentiation.</description><author>Ozer, O.</author><pubDate>Sat, 01 Mar 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing knowledge in organizations: An integrative framework and review of emerging themes</title><link>http://www.example.com/articles/1</link><description>Argote, L.; McEvily, B.; Reagans, R.
In this concluding article to the Management Science special issue on "Managing Knowledge in Organizations: Creating, Retaining, and Transferring Knowledge," we provide an integrative framework for organizing the literature on knowledge management. The framework has two dimensions. The knowledge management outcomes of knowledge creation, retention, and transfer are represented along one dimension. Properties of the context within which knowledge management occurs are represented on the other dimension. These properties, which affect knowledge management outcomes, can be organized according to whether they are properties of a unit (e.g., individual, group, organization) involved in knowledge management, properties of relationships between units or properties of the knowledge itself. The framework is used to identify where research findings about knowledge management converge and where gaps in our understanding exist. The article discusses mechanisms of knowledge management and how those mechanisms affect a unit's ability to create, retain and transfer knowledge. Emerging themes in the literature on knowledge management are identified. Directions for future research are suggested.</description><author>Argote, L.; McEvily, B.; Reagans, R.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploration and exploitation in the presence of network externalities</title><link>http://www.example.com/articles/1</link><description>Lee, J.; Lee, J.; Lee, H.
This paper examines the conditions under which exploration of a new, incompatible technology is conducive to firm growth in the presence of network externalities. In particular, this study is motivated by the divergent evolutions of the PC and the workstation markets in response to a new technology: reduced instruction set computing (RISC). In the PC market, Intel has developed new microprocessors by maintaining compatibility with the established architecture, whereas it was radically replaced by RISC in the workstation market. History indicates that unlike the PC market, the workstation market consisted of a large number of power users, who are less sensitive to compatibility than ordinary users. Our numerical analysis indicates that the exploration of a new, incompatible technology is more likely to increase the chance of firm growth when there are a substantial number of power users or when a new technology is introduced before an established technology takes off.</description><author>Lee, J.; Lee, J.; Lee, H.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multimarket competition, consumer search, and the organizational structure of multiunit firms</title><link>http://www.example.com/articles/1</link><description>Chang, M. H.; Harrington, J. E.
This research explores how market competition influences a firm's optimal organizational structure. For this purpose, a computational model of competing multiunit firms is developed in which unit managers and corporate staff continually search for better practices, while consumers search among units to find a better match. Organizational structure impacts both the practices at the unit level and the extent of knowledge transfer. An increasing returns mechanism is identified, which results in the relative performance of the centralized form being greater when competition is more intense.</description><author>Chang, M. H.; Harrington, J. E.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning negotiation skills: Four models of knowledge creation and transfer</title><link>http://www.example.com/articles/1</link><description>Nadler, J.; Thompson, L.; Van Boven, L.
Our review of the learning and training literature revealed four common methods for training people to be more effective negotiators: didactic learning, learning via information revelation, analogical learning, and observational learning. We tested each of these methods experimentally in an experiential context and found that observational learning and analogical learning led to negotiated outcomes that were more favorable for both parties, compared to a baseline condition of learning through experience alone. Information revelation and didactic learning were not significantly different from any other condition. Process measures revealed that negotiators' schemas about the task (reflected in open-ended essays) were strong predictors of performance in the analogical learning condition, but were poor predictors of performance in the remaining conditions. Interestingly, negotiators in the observation group showed the largest increase in performance, but the least ability to articulate the learning principles that helped them improve, suggesting that they had acquired tacit knowledge that they were unable to articulate.</description><author>Nadler, J.; Thompson, L.; Van Boven, L.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interruptive events and team knowledge acquisition</title><link>http://www.example.com/articles/1</link><description>Zellmer-Bruhn, M. E.
Interruptions have commonly been viewed as negative and as something for managers to control or limit. In this paper, I explore the relationship between interruptions and acquisition of routines-a form of knowledge-by teams. Recent research suggests that interruptions may play an important role in changing organizational routines, and as such may influence knowledge transfer activities. Results suggest that interruptions influence knowledge transfer effort, and both knowledge transfer effort and interruptions are positively related to the acquisition of new work routines. I conclude with implications for research and practice.</description><author>Zellmer-Bruhn, M. E.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Valuing internal vs. external knowledge: Explaining the preference for outsiders</title><link>http://www.example.com/articles/1</link><description>Menon, T.; Pfeffer, J.
This paper compares how managers value knowledge from internal and external sources. Although many theories account for favoritism toward insiders, we find that preferences for knowledge obtained from outsiders are also prevalent. Two complementary case studies and survey data from managers demonstrate the phenomenon of valuing external knowledge more highly than internal knowledge and reveal some mechanisms through which this process occurs. We found evidence that the preference for outsider knowledge is the result of managerial responses to (1) the contrasting status implications of learning from internal versus external competitors, and (2) the availability or scarcity of knowledge-internal knowledge is more readily available and hence subject to greater scrutiny, while external knowledge is more scarce, which makes it appear more special and unique. We conclude by considering some consequences of the external knowledge preference for organizational functioning.</description><author>Menon, T.; Pfeffer, J.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The halo effect and technology licensing: The influence of institutional prestige on the licensing of university inventions</title><link>http://www.example.com/articles/1</link><description>Sine, W. D.; Scott, S.; Di Gregorio, D.
Sociologists and organizational theorists have long claimed that the processes of knowledge creation and distribution are fundamentally social. Following in this tradition, we explore the effect of institutional prestige on university technology licensing. Empirically, we examine the influence of university prestige on the annual rate of technology licensing by 102 universities from 1991-1998. We show that institutional prestige increases a university's licensing rate over and above the rate that is explained by the university's past licensing:. performance. Because licensing success positively impacts future invention production, we argue that institutional prestige leads to stratification in the creation and distribution of university-generated knowledge.</description><author>Sine, W. D.; Scott, S.; Di Gregorio, D.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Who's really sharing? Effects of social and expert status on knowledge exchange within groups</title><link>http://www.example.com/articles/1</link><description>Thomas-Hunt, M. C.; Ogden, T. Y.; Neale, M. A.
This study investigated the effects of social status and perceived expertise on the emphasis of unique and shared knowledge within functionally heterogeneous groups. While perceived expertise did not increase the individual's emphasis of their own unique knowledge, perceived experts were more likely than nonexperts to emphasize shared knowledge and other member's unique knowledge contributions. Additionally, socially isolated members participated more in discussions and emphasized more of their unique knowledge than did socially connected members. While unique knowledge contributions increased the positive perception of social isolates, similar unique knowledge contributions decreased the positive perception of socially connected members. Finally, socially connected group members gave greater attention to the unique knowledge contributions of the socially isolated member than to the contributions of their socially connected other, but more favorably evaluated members to whom they were more favorably connected than those to whom they were not. We discuss the implications of our findings for managing knowledge exchange within diverse groups.</description><author>Thomas-Hunt, M. C.; Ogden, T. Y.; Neale, M. A.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interdependence and adaptability: Organizational learning and the long-term effect of integration</title><link>http://www.example.com/articles/1</link><description>Sorenson, O.
A growing body of research documents the role that organizational learning plays in improving firm performance over time. To date, however, this literature has given limited attention to the effect that the internal structure of the firm c an have on generating differences in these learning rates. This paper focuses on the degree to which interdependence-and in particular one structural characteristic that generates interdependence, vertical integration-affects organizational learning. Firms face a trade-off. In stable environments, vertically integrating severely limits the organization's ability to learn by doing because boundedly rational managers find the optimization of operations difficult when making highly interdependent choices. As the volatility of the environment increases though, integration can facilitate learning-by-doing by buffering activities within the firm from instability in the external environment. Thus, firms with a high degree of interdependence suffer less in these environments. Tests of these hypotheses on the growth and exit rates of computer workstation. manufacturers support this thesis.</description><author>Sorenson, O.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A relational view of information seeking and learning in social networks</title><link>http://www.example.com/articles/1</link><description>Borgatti, S. P.; Cross, R.
Research in organizational learning has demonstrated processes and occasionally performance implications of acquisition of declarative (know-what) and procedural (know-how) knowledge. However, considerably less attention has been paid to learned characteristics of relationships that affect the decision to seek information from other people. Based on a review of the social network, information processing, and organizational learning literatures, along with the results of a previous qualitative study, we propose a formal model of information seeking in which the probability of seeking information from another person is a function of (1) knowing what that person knows; (2) valuing what that person knows; (3) being able to gain timely access to that person's thinking; and (4) perceiving that seeking information from that person would not be too costly. We also hypothesize that the knowing, access, and cost variables mediate the relationship between physical proximity and information seeking. The model is tested using two separate research sites to provide replication. The results indicate strong support for the model and the mediation hypothesis (with the exception of the cost variable). Implications are drawn for the study of both transactive memory and organizational learning, as well as for management practice.</description><author>Borgatti, S. P.; Cross, R.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge and productivity in technical support work</title><link>http://www.example.com/articles/1</link><description>Das, A.
In this paper, we examine the process of technical support work and the role of knowledge in enhancing the productivity of such work. We develop the concepts of problem-solving tasks and moves to describe technical support work, while using call resolution time and problem escalation as measures of productivity. Using hierarchical log-linear modeling, we establish the link between problem-solving moves and productivity. We find that the mix of moves exercised in technical support strongly depends on the formulation of tasks by those requesting support. Because the formulation of tasks is performed by users, knowledge management initiatives must target users as well as support providers to have the desired impact on productivity.</description><author>Das, A.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Cultural conflict and merger failure: An experimental approach</title><link>http://www.example.com/articles/1</link><description>Weber, R. A.; Camerer, C. F.
We use laboratory experiments to explore merger failure due to conflicting organizational cultures. We introduce a laboratory paradigm for studying organizational culture that captures several key elements of the phenomenon. In our experiments, we allow subjects in "firms" to develop a culture, and then merge two firms. As, expected, performance decreases following the merging of two laboratory firms. In addition, subjects overestimate the performance of the merged firm and attribute the decrease in performance to members of the other firm rather than to situational difficulties created by conflicting culture.</description><author>Weber, R. A.; Camerer, C. F.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Relational embeddedness and learning: The case of bank loan managers and their clients</title><link>http://www.example.com/articles/1</link><description>Uzzi, B.; Lancaster, R.
As a complement to the literature on learning in firms, we investigate learning in markets, a nascent area of study that focuses on how learning occurs between, rather than within, firms. The core idea behind our framework is that networks shape knowledge transfer and learning processes by creating channels for knowledge trade and reducing the risk of learning. In developing our framework, we elaborate on the knowledge transfer capabilities of different types of social ties, the informational properties of public and private knowledge, and how types of knowledge transfer and forms of learning follow from the networks within which firms embed their exchanges. We conducted fieldwork at Chicago-area banks to examine our framework's plausibility and application to learning in financial lending markets, a setting relevant to most firms. Findings indicate that learning is located not only in actors' cognitions or past experiences, but also in relations among actors, and that viewing learning as a social process helps solve problems regarding knowledge transfer and learning in markets.</description><author>Uzzi, B.; Lancaster, R.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does good science lead to valuable knowledge? Biotechnology firms and the evolutionary logic of citation patterns</title><link>http://www.example.com/articles/1</link><description>Gittelman, M.; Kogut, B.
This study looks at the United States biotechnology industry as a community of practice caught between two evolutionary logics by which valuable scientific knowledge and valuable innovations are selected. We analyze the publications and patents of 116 biotechnology firms during the period 1988-1995. In models that link scientific capabilities to patent citations, we show that scientific ideas are not simply inputs into inventions; important scientific ideas and influential patents follow different and conflicting selection logics. Publication, collaboration, and science intensity are associated with patented innovations; however, important scientific papers are negatively associated with high-impact innovations. These results point to conflicting logics between science and innovation, and scientists must contribute to both while inhabiting a single epistemic community. We identify individuals listed on patents and scientific papers and find they effectively integrate science with innovation, leading to more successful innovations. Our findings suggest that the role of the small, research-intensive firm is to create a repository of knowledge; to act as an organizational mechanism to combine the capabilities of versatile scientists within and outside the boundaries of the firm; and to manage the selection of scientific ideas to produce valuable technical innovations.</description><author>Gittelman, M.; Kogut, B.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Learning-by-hiring: When is mobility more likely to facilitate interfirm knowledge transfer?</title><link>http://www.example.com/articles/1</link><description>Song, J.; Almeida, P.; Wu, G.
To investigate the conditions under which learning-by-hiring (or the acquisition of knowledge through the hiring of experts from other firms) is more likely, we study the patenting activities of engineers who moved from United States (U.S.) firms to non-U.S. firms. Statistical findings from negative binomial regressions show that mobility is more likely to result in interfirm knowledge transfer when (1) the hiring firm is less path dependent, (2) the hired engineers possess technological expertise distant from that-of the hiring firm, and (3) the hired engineers work in noncore technological areas in their new firm. In addition, the results support the idea that domestic mobility and international mobility are similarly conducive to learning-by-hiring. Thus, our paper suggests that learning-by-hiring can be useful when hired engineers are used for exploring technologically distant knowledge (rather than for reinforcing existing firm expertise) and also for extending the hiring firm's geographic reach.</description><author>Song, J.; Almeida, P.; Wu, G.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue on managing knowledge in organizations: Creating, retaining, and transferring knowledge</title><link>http://www.example.com/articles/1</link><description>Argote, L.; McEvily, B.; Reagans, R.
nan</description><author>Argote, L.; McEvily, B.; Reagans, R.</author><pubDate>Tue, 01 Apr 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Return on assets loss from situational and contingency misfits (vol 48, pg 1461, 2002)</title><link>http://www.example.com/articles/1</link><description>Burton, R. M.; Lauridsen, J.; Obel, B.
nan</description><author>Burton, R. M.; Lauridsen, J.; Obel, B.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>E-business and management science: Mutual impacts (part 1 of 2)</title><link>http://www.example.com/articles/1</link><description>Geoffrion, A. M.; Krishnan, R.
This begins a two-part commentary on management science and e-business, the theme of this two-part special issue. After explaining the topical clusters that give organization to both parts, we pose two key questions concerning the impact of the emerging digital economy on management science research: What fundamentally new research questions arise, and what kind of research enables progress on them. We sketch the papers appearing in this part from the perspective of both these questions, and offer summary comments on the first question based on the papers in both parts. The principal conclusion is that the digital economy is giving birth to new research questions in three main ways (not all independent): by enabling and popularizing several types of technology-mediated interactions, by spawning large-scale digital data sources,and by creating recurring operational decisions that need to be automated.</description><author>Geoffrion, A. M.; Krishnan, R.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic pricing in the presence of inventory considerations: Research overview, current practices, and future directions</title><link>http://www.example.com/articles/1</link><description>Elmaghraby, W.; Keskinocak, P.
The benefits of dynamic pricing methods have long been known in industries, such as airlines, hotels, and electric utilities, where the capacity is fixed in the short-term and perishable. In recent years, there has been an increasing adoption of dynamic pricing policies in retail and other industries, where the sellers have the ability to store inventory. Three factors contributed to this phenomenon: (1) the increased availability of demand data, (2) the ease of changing prices due to new technologies, and (3) the availability of decision-support tools for analyzing demand data and for dynamic pricing. This paper constitutes a review of the literature and current practices in dynamic pricing. Given its applicability in most markets and its increasing adoption in practice, our focus is on dynamic (intertemporal) pricing in the presence of inventory considerations.</description><author>Elmaghraby, W.; Keskinocak, P.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Internet-based virtual stock markets for business forecasting</title><link>http://www.example.com/articles/1</link><description>Spann, M.; Skiera, B.
The application of Internet-based virtual stock markets (VSMs) is an additional approach that can be used to predict short- and medium-term market developments. The basic concept involves bringing a group of participants together via the Internet and allowing them to trade shares of virtual stocks. These stocks represent a bet on the outcome of future market situations, and their value depends on the realization of these market situations. In this process, a VSM elicits and aggregates the assessments of its participants concerning future market developments. The aim of this article is to evaluate the potential use and the different design possibilities as well as the forecast accuracy and performance of VSMs compared to expert predictions for their application to business forecasting. After introducing the basic idea of using VSMs for business forecasting, we discuss the different design possibilities for such VSMs. In three real-world applications, we analyze the feasibility and forecast accuracy of VSMs, compare the performance of VSMs to expert predictions, and propose a new validity test for VSM forecasts. Finally, we draw conclusions and provide suggestions for future research.</description><author>Spann, M.; Skiera, B.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the use of optimization for data mining: Theoretical interactions and eCRM opportunities</title><link>http://www.example.com/articles/1</link><description>Padmanabhan, B.; Tuzhilin, A.
Previous work on the solution to analytical electronic customer relationship management (eCRM) problems has used either data-mining (DM) or optimization methods, but has not combined the two approaches. By leveraging the strengths of both approaches, the eCRM problems of customer analysis, customer interactions, and the optimization of performance metrics (such as the lifetime value of a customer on the Web) can be better analyzed. In particular, many eCRM problems have been traditionally addressed using DM methods. There are opportunities for optimization to improve these methods, and this paper describes these opportunities. Further, an online appendix (mansci.pubs.informs.org/ecompanion.html) describes how DM methods can help optimization-based approaches. More generally, this paper argues that the reformulation of eCRM problems within this new framework of analysis can result in more powerful analytical approaches.</description><author>Padmanabhan, B.; Tuzhilin, A.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The role of the management sciences in research on personalization</title><link>http://www.example.com/articles/1</link><description>Murthi, B. P. S.; Sarkar, S.
We present a review of research studies that deal with personalization and synthesize current knowledge about these areas. We identify issues that we envision will be of interest to researchers working in the management sciences, taking an interdisciplinary approach that spans the areas of economics, marketing, information technology (IT), and operations research. We present a framework for personalization that allows us to identify key players in the personalization process as well as key stages of personalization. The framework enables us to examine the strategic role of personalization in the interactions between a firm and other key players in the firm's value system. We conceptualize the personalization process as consisting of three stages: (1) learning about consumer preferences, (2) matching offerings to customers, and (3) evaluation of the learning and matching processes. This review focuses on the learning stage, with an emphasis on utility-based approaches to estimate preference functions using data on customer interactions with a firm.</description><author>Murthi, B. P. S.; Sarkar, S.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Revenue management and e-commerce</title><link>http://www.example.com/articles/1</link><description>Boyd, E. A.; Bilegan, I. C.
We trace the history of revenue management in an effort to illustrate a successful e-commerce model of dynamic, automated sales. Our discourse begins with a brief overview of electronic distribution as practiced in the airline industry, emphasizing the fundamental role of central reservation and revenue management systems. Methods for controlling the sale of inventory are then introduced along with related techniques for optimization and forecasting. Research contributions and areas of significant research potential are given special attention. We conclude by looking at how revenue management is practiced outside of the airline industry, its relationship to dynamic pricing, and future directions for the discipline.</description><author>Boyd, E. A.; Bilegan, I. C.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Models for supply chains in e-business</title><link>http://www.example.com/articles/1</link><description>Swaminathan, J. M.; Tayur, S. R.
Supply chain management is likely to play an important role in the digital economy. In this paper, we first describe major issues in traditional supply chain management. Next, we focus our attention on the supply chain issues of visibility supplier relationships, distribution and pricing, customization, and real-time decision technologies that have risen to importance with the prevalence of e-business. We present an overview of relevant analytical research models that have been developed in these areas, discuss their contributions, and conclude with a discussion on future modeling opportunities in this area.</description><author>Swaminathan, J. M.; Tayur, S. R.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The digitization of word of mouth: Promise and challenges of online feedback mechanisms</title><link>http://www.example.com/articles/1</link><description>Dellarocas, C.
Online feedback mechanisms harness the bidirectional communication capabilities of the Internet to engineer large-scale, word-of-mouth networks. Best known so far as a technology for building trust and fostering cooperation in online marketplaces, such as eBay, these mechanisms are poised to have a much wider impact on organizations. Their growing popularity has potentially important implications for a wide range of management activities such as brand building, customer acquisition and retention, product development, and quality assurance. This paper surveys our progress in understanding the new possibilities and challenges that these mechanisms represent. It discusses some important dimensions in which Internet-based feedback mechanisms differ from traditional word-of-mouth networks and surveys the most important issues related to their design, evaluation, and use. It provides an overview of relevant work in game theory and economics on the topic of reputation. It discusses how this body of work is being extended and combined with insights from computer science, management science, sociology, and psychology to take into consideration the special properties of online environments. Finally, it identifies opportunities that this new area presents for operations research/management science (OR/MS) research.</description><author>Dellarocas, C.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>World wide wait: A study of Internet scalability and cache-based approaches to alleviate it</title><link>http://www.example.com/articles/1</link><description>Datta, A.; Dutta, K.; Thomas, H.; VanderMeer, D.
The Internet is growing rapidly in terms of both use and infrastructure. Unfortunately, demand is outpacing the capacity of the infrastructure, as evidenced by unacceptably long response times. To support current load and further growth, we must address this problem. Several caching strategies have been proposed in the literature; many have been implemented to improve the quality of service on the Web. In this paper, we identify the main causes of delay on the Web, and provide a review of the various caching strategies employed to mitigate these delays. We also survey the application of Operations Research/Management Science (OR/MS) techniques to caching on the Web. Finally, we identify several open OR/MS research problems related to Web caching.</description><author>Datta, A.; Dutta, K.; Thomas, H.; VanderMeer, D.</author><pubDate>Wed, 01 Oct 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A postponement model for demand management</title><link>http://www.example.com/articles/1</link><description>Iyer, A. V.; Deshpande, V.; Wu, Z. P.
In this paper, we analyze demand postponement as a strategy to handle potential demand surges. Under demand postponement, a fraction of the demands from the "regular" period are postponed and satisfied during a "postponement" period. This permits capacity to be procured to satisfy the postponed demands. A reimbursement per unit is paid to customers whose demands are postponed. The basic idea is that by preempting stockouts through demand postponement, we can reduce overall stockout costs. We formulate and solve a two-stage capacity planning problem under demand postponement. We propose a power range class of distributions to capture the nature of demand surges. We establish the scalability and conjugate properties of the power range distributions under demand postponement, which leads to a tractable analysis of the problem. We analytically solve the problem of determining the optimal regular and postponement period capacities, and the demand splitting rule to minimize the supplier's expected cost. We show that (a) the value of postponement may be significant depending on cost and demand parameters, (b) a postponement strategy may lead to reduced investment in initial capacity, and (c) it may be optimal to do no demand postponement over a range of demands even after observing a higher demand signal. We then relax several model assumptions and provide results for these extensions. We conclude with managerial insights.</description><author>Iyer, A. V.; Deshpande, V.; Wu, Z. P.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>You are known by the directors you keep: Reputable directors as a signaling mechanism for young firms</title><link>http://www.example.com/articles/1</link><description>Deutsch, Y.; Ross, T. W.
In this paper, we develop an analytical model of outside. directors' signaling role-a role that is especially important for entrepreneurial firms. We formally demonstrate that in the face of a market failure in which stakeholders refuse to align themselves with new firms, high-quality new ventures may be able to credibly signal their type by appointing reputable directors to their boards. However, this option is not universally feasible. Both directors' reputations and the quality of their information determine the effectiveness of this strategy. In contrast to earlier adverse selection models, we demonstrate that when the middlemen (directors) have incomplete information on firm quality, bad and good firms can coexist in equilibrium. In this equilibrium, the quality of the directors' information determines the mix of good and bad firms in the population of surviving firms. Avenues for future research and normative implications for practitioners are discussed.</description><author>Deutsch, Y.; Ross, T. W.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and capacity sizing for systems with shared resources: Approximate solutions and scaling relations</title><link>http://www.example.com/articles/1</link><description>Maglaras, C.; Zeevi, A.
This paper considers pricing and capacity sizing decisions, in a single-class Markovian model motivated by communication and information services. The service provider is assumed to operate a finite set of processing resources that can be shared among users; however, this shared mode of operation results in a service-rate degradation. Users, in turn, are sensitive to the delay implied by the potential degradation in service rate, and to the usage fee charged for accessing the system. We study the equilibrium behavior of such systems in the specific context of pricing and capacity sizing under revenue and social optimization objectives. Exact solutions to these problems can only be obtained via exhaustive simulations. In contrast, we pursue approximate solutions that exploit large-capacity asymptotics. Economic considerations and natural scaling relations demonstrate that the optimal operational mode for the system is close to "heavy traffic." This, in turn, supports the derivation of simple approximate solutions to economic optimization problems, via asymptotic methods that completely alleviate the need for simulation. These approximations seem to be extremely accurate. The main insights that are gleaned in the analysis follow: congestion costs are "small," the optimal price admits a two-part decomposition, and the joint capacity sizing and pricing problem decouples and admits simple analytical solutions that are asymptotically optimal. All of the above phenomena are intimately related to statistical economies of scale that are an intrinsic part of these systems.</description><author>Maglaras, C.; Zeevi, A.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The capacitated lot-sizing problem with linked lot sizes</title><link>http://www.example.com/articles/1</link><description>Suerie, C.; Stadtler, H.
In this paper a new mixed integer programming (MIP) model formulation and its incorporation into a time-oriented decomposition heuristic for the capacitated lot-sizing problem with linked lot sizes (CLSPL) is proposed. The solution approach is based on an extended model formulation and valid inequalities to yield a tight formulation. Extensive computational tests prove the capability of this approach and show a superior solution quality with respect to other solution algorithms published so far.</description><author>Suerie, C.; Stadtler, H.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product customization and price competition on the Internet</title><link>http://www.example.com/articles/1</link><description>Dewan, R.; Jing, B.; Seidmann, A.
The Internet provides an unprecedented capability for sellers to learn about their customers and offer custom products at special prices. In addition, customization is more feasible today because of advances in manufacturing technologies that have improved sellers' manufacturing flexibility. We first develop a model of product customization and flexible pricing to incorporate the salient roles of the Internet and flexible manufacturing technologies in reducing the costs of designing and producing tailored consumer goods. We show how a monopoly seller may earn the highest profits by producing both standard and custom products and can raise prices for both types of products as customization and information collection technologies improve. Simultaneous adoption of customization in a duopoly reduces the differentiation between their standard products but does not intensify price competition. Compared with a. two-facility monopolist, the duopoly may underinvest in customization. Consumer surplus improves after sellers adopt customization. but does not monotonically increase as customization technologies advance. When firms face a fixed entry cost and adopt customization sequentially, the first entrant always achieves an advantage and may be able to deter subsequent entry by choosing its customization scope strategically.</description><author>Dewan, R.; Jing, B.; Seidmann, A.</author><pubDate>Fri, 01 Aug 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comparison of mixed-integer programming models for nonconvex piecewise linear cost minimization problems</title><link>http://www.example.com/articles/1</link><description>Croxton, K. L.; Gendron, B.; Magnanti, T. L.
We study a generic, minimization problem with separable nonconvex piecewise linear costs, showing that the linear programming (LP) relaxation of three textbook mixed-integer programming formulations each approximates the cost function by its lower convex envelope. We also show a relationship between this result and classical Lagrangian duality theory.</description><author>Croxton, K. L.; Gendron, B.; Magnanti, T. L.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competitive price discrimination strategies in a vertical channel using aggregate retail data</title><link>http://www.example.com/articles/1</link><description>Besanko, D.; Dube, J. P.; Gupta, S.
We explore opportunities for targeted pricing for a retailer that only tracks weekly store-level aggregate sales and marketing-mix information. We show that it is possible, using these data, to recover essential features of the underlying distribution of consumer willingness to pay. Knowledge of this distribution may enable the retailer to generate additional profits from targeting by using choice information at the checkout counter. In estimating demand we incorporate a supply-side model of the distribution channel that captures important features of competitive price-setting behavior of firms. This latter aspect helps us control for the potential endogeneity generated by unmeasured product characteristics in aggregate data. The channel controls for competitive aspects both between manufacturers and between manufacturers and a retailer. Despite this competition, we find that targeted pricing need not generate the prisoner's dilemma in our data. This contrasts with the findings of theoretical models due to the flexibility of the empirical model of demand. The demand system we estimate captures richer forms of product differentiation, both vertical and horizontal, as well as a more flexible distribution of consumer heterogeneity.</description><author>Besanko, D.; Dube, J. P.; Gupta, S.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price and delivery logistics competition in a supply chain</title><link>http://www.example.com/articles/1</link><description>Ha, A. Y.; Li, L. D.; Ng, S. M.
We consider a supply chain in which two suppliers compete for supply to a customer. Pricing and delivery-frequency decisions in the system are analyzed by two three-stage noncooperative games with different decision rights designated to the parties involved. The customer first sets the price (or delivery frequency) for each supplier. Then, the suppliers offer the delivery frequencies (or prices) simultaneously and independently. Finally, the customer determines how much demand to allocate to each of the suppliers. We show that delivery frequency, similar to delivery speed in time-based competition, can be a source of competitive advantage. It also allows firms that sell identical products to offer complementary services to the customer because she can lower her inventory with deliveries from more suppliers. In general, higher delivery frequencies lower the value of getting deliveries from the second supplier and therefore intensify price competition. Assuming the cost structures do not change and the suppliers are identical, we show that when the customer controls deliveries, she would strategically increase delivery frequencies to lower prices. The distortion in delivery frequencies is larger and the overall performance of the supply chain is lower when the customer, not the suppliers, controls deliveries. Moreover, the customer is better off under delivery competition, while the suppliers are better off under price competition.</description><author>Ha, A. Y.; Li, L. D.; Ng, S. M.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain management with guaranteed delivery</title><link>http://www.example.com/articles/1</link><description>Huggins, E. L.; Olsen, T. L.
We consider a two-stage supply chain under centralized control. The downstream facility faces discrete stochastic demand and passes supply requests to the upstream facility. The upstream facility always meets the supply requests from downstream. If the upstream facility cannot meet the supply requests from inventory on hand, the shortage must be filled by expediting, which will incur per unit and setup costs. Such expediting may take the form of overtime production, which occurs at the end of the period and incurs relatively high production costs, or premium freight shipments, which involves building products at the beginning of the period they are needed and shipping them very quickly with relatively high shipping costs. We consider the case where one method of filling shortages is available and determine novel optimal inventory policies under centralized control. At both stages, threshold policies that depend only on the current inventory in the system are optimal; for the total inventory in the system, a base-stock policy is optimal. Numerical analysis provides insight into the optimal policies and allows us to compare the supply chains under centralized and decentralized control.</description><author>Huggins, E. L.; Olsen, T. L.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A new decision rule for lateral transshipments in inventory systems</title><link>http://www.example.com/articles/1</link><description>Axsater, S.
This paper deals with a single-echelon inventory system consisting of a number of parallel local warehouses facing compound Poisson demand. There are standard holding and backorder costs as well as ordering costs at all warehouses. Normally, the warehouses replenish from an outside supplier. However, lateral transshipments between the warehouses are also possible. Such transshipments take no time but incur additional costs. When a demand,occurs at a warehouse, the question is whether the whole demand or part of it should be covered by a lateral transshipment from another warehouse. Given a set of alternative decisions, our decision rule minimizes the expected costs under the assumption that no further transshipments will take place. This rule is then used repeatedly as a heuristic. A simulation study illustrates how the suggested technique performs under different conditions.</description><author>Axsater, S.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Into the black box: The knowledge transformation cycle</title><link>http://www.example.com/articles/1</link><description>Carlile, P. R.; Rebentisch, E. S.
This paper examines how knowledge is integrated in complex technology and product development settings. By framing the task of knowledge integration as a cycle, we highlight the inability of current knowledge transfer theories to explain the consequences that arise from the path-dependent nature of knowledge. We compare the complexity of this knowledge integration task to previous efforts in terms of its novelty and the organizational properties of specialization and dependence that are required. Drawing on evidence from two empirical studies, we outline three stages of the "knowledge transformation cycle," which addresses the complexity of this integration task. We conclude with the implications of this knowledge transformation cycle on our understanding of knowledge management and organizational learning.</description><author>Carlile, P. R.; Rebentisch, E. S.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The boycott puzzle: Consumer motivations for purchase sacrifice</title><link>http://www.example.com/articles/1</link><description>John, A.; Klein, J.
A boycott is never far from a firm's exchanges with its customers. Researchers in marketing need to understand consumer protest behavior, both to aid nongovernmental organizations (NGOs) who wish to organize boycotts, and to assist managers who wish to develop appropriate strategic responses. Boycotts, like many other instances of collective action, are subject to free-rider and small-agent problems: there appears to be little or no motivation for an individual to participate. Yet they assuredly occur. We take an economic and psychological approach to the study of boycotts. Our approach is to develop a typology of motivations for consumer boycotts, to embed these motivations explicitly in a dynamic economic model, and thus to offer explanations for the extent of boycott participation.</description><author>John, A.; Klein, J.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Robust pricing of the American put option: A note on Richardson extrapolation and the early exercise premium</title><link>http://www.example.com/articles/1</link><description>Ibanez, A.
This paper presents a detailed analysis of the numerical implementation of the American put option decomposition into an equivalent European option plus an early exercise premium (Kim 1990, Jacka 1991, Carr et al. 1992). It subsequently introduces a. new algorithm based upon this decomposition and Richardson extrapolation. This new algorithm is based upon (a) the derivation of the correct order for the error term when applying Richardson extrapolation, which is used to control the error of the extrapolated prices, (b) an innovative adjustment of Kim's (1990) discrete-time early exercise premium, so that these premiums monotonically converge and, therefore, it is appropriate to use them in extrapolation, and (c) the optimal exercise frontier can be quickly computed through Newton's method, permitting the efficient implementation of the decomposition formula in practice. Numerical experiments show that this new algorithm is accurate, efficient, easy to implement, and competitive in comparison with other methods. Finally, it can also be applied to other American exotic securities.</description><author>Ibanez, A.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Virtual progress: The effect of path characteristics on perceptions of progress and choice</title><link>http://www.example.com/articles/1</link><description>Soman, D.; Shi, M. Z.
In goal-oriented services, consumers want to get transported from one well-defined state (start) to another (destination) state without much concern for intermediate states. A cost-based evaluation of such services should depend on the total cost associated with the service-i.e., the price and the amount of time taken for completion. In this paper, we demonstrate that the characteristics of the path to the final destination also influence evaluation and choice. Specifically, we show that segments of idle time and travel away from the final destination are seen as obstacles in the progress towards the destination, and hence lower the choice likelihood of the path. Further, we show that the earlier such obstacles occur during the service, the lower is the choice likelihood. We present an analytical model of consumer choice and test its predictions in a series of experiments. Our results show that in choosing between two services that cover the same displacement in the same time (i.e., identical average progress), consumer choice is driven by the perception of progress towards the goal (i.e., by virtual progress). In a final experiment, we show that the effects of virtual progress may outweigh the effects of actual average progress.</description><author>Soman, D.; Shi, M. Z.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The shape of utility functions and organizational behavior</title><link>http://www.example.com/articles/1</link><description>Pennings, J. M. E.; Smidts, A.
Based on measurements among 332 owner-managers, we investigate how the global shape of the utility function (i.e., S-shaped versus concave or convex over the total range of outcomes) relates to choice behavior. We find that the global shape of the utility function differs across decision makers (about one-third of the owner-managers exhibit an S-shaped utility function) and that the global shape is linked to organizational behavior (i.e., the production system employed), a result that does not change when using different methods to identify the decision maker's global shape of the utility function. The decision maker's risk attitude (risk averse or risk seeking) does not affect the choice of the production system. Whereas the degree of risk aversion, based on the local shape of the utility function, may be important in explaining owner-managers' trading behavior (Pennings and Smidts 2000), more structural organizational behavior appears to be linked to the global shape of the utility function.</description><author>Pennings, J. M. E.; Smidts, A.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Remark on "Appropriateness and impact of platform-based product development"</title><link>http://www.example.com/articles/1</link><description>Dana, J. D.
When platform-based manufacturing exhibits overdesign costs then platform adoption will lead to a decrease in product lined differentiation. In contrast, Krishnan and Gupta (2001) argued that in the presence of overdesign costs and platform economies, platform production always leads to a more differentiated product family. While Krishnan and Gupta analyzed one particular parameterized cost function and define overdesign costs and platform economies narrowly in terms of their parameters, I propose a general definition of overdesign costs and show that platform adoption. reduces product differentiation for all cost functions satisfying this definition, regardless of whether or not they exhibit platform economies.</description><author>Dana, J. D.</author><pubDate>Mon, 01 Sep 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrating long- and short-term contracting via business-to-business exchanges for capital-intensive industries</title><link>http://www.example.com/articles/1</link><description>Kleindorfer, P. R.; Wu, D. J.
his paper surveys the underlying theory and practice in the use of options in support of emerging business-to-business (B2B) markets. Such options, on both capacity and output, play an important role in integrating long- and short-term contracting between multiple buyers and sellers in such markets. This trend is especially important in capital-intensive industries, where improvements in fine tuning the coordination of supply and demand carry large economic benefits. Typically, such options are benchmarked (or defined) on the basis of spot market information conveyed through near real-time B2B transactions. This paper notes a broad set of goods and services currently being traded in both B2B short-run markets and long-term contract markets, and reviews economic and managerial frameworks that have been proposed to explain the structure of contracting in these markets. We provide a general framework based on transactions cost economics, and we use this framework to provide a review and synthesis of existing literature to explain various types of contracting linked to B2B exchanges in capital-intensive industries. The paper concludes with a discussion of implementation challenges and open research questions.</description><author>Kleindorfer, P. R.; Wu, D. J.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing Online auctions: Current business and research issues</title><link>http://www.example.com/articles/1</link><description>Pinker, E. J.; Seidmann, A.; Vakrat, Y.
The Internet's computational power and flexibility have made auctions a widespread and integral part of both consumer and business markets. Though online auctions are a multibillion dollar annual activity, with a growing variety of sophisticated trading mechanisms, scientific research on them is at an early stage. This paper analyzes the current state of management science research on online auctions. It develops a broad research agenda for issues such as the behavior of online auction participants, the optimal design of online auctions, the integration of auctions into the ongoing operation of firms, and the use of the data generated by online auctions to inform future trading mechanisms. These research areas will draw from applied and theoretical work spanning management science, economics, and information systems.</description><author>Pinker, E. J.; Seidmann, A.; Vakrat, Y.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Combinatorial auction design</title><link>http://www.example.com/articles/1</link><description>Pekec, A.; Rothkopf, M. H.
Combinatorial auctions have two features that greatly affect their design: computational complexity of winner determination and opportunities for cooperation among competitors. Dealing with these forces trade-offs between desirable auction properties such as allocative efficiency, revenue maximization, low transaction costs, fairness, failure freeness, and scalability. Computational complexity can be dealt with algorithmically by relegating the computational burden to bidders, by maintaining fairness in the face of computational limitations, by limiting biddable combinations, and by limiting the use of combinatorial bids. Combinatorial auction designs include single-round, first-price sealed bidding, Vickrey-Clarke-Groves (VCG) mechanisms, uniform and market-clearing price auctions, and iterative combinatorial. auctions. Combinatorial auction designs must deal with exposure problems, threshold problems, ways to keep the bidding moving at a reasonable pace, avoiding and resolving ties, and controlling complexity.</description><author>Pekec, A.; Rothkopf, M. H.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Costly bidding in Online markets for IT services</title><link>http://www.example.com/articles/1</link><description>Snir, E. M.; Hitt, L. M.
Internet-enabled markets are becoming viable venues for procurement of professional services. We investigate bidding behavior within the most active area of these early knowledge markets-the market for software development. These markets are important both because they provide an early view of the effectiveness of online service markets and because they have a potentially large impact on how software development services are procured and provided. Using auction theory, we develop a theoretical model that relates market characteristics to bidding and transaction behavior, taking into account costly bidding. We then test our model using data from an active online market for software development services, which yields contracts for 30%-40% of posted projects. In its current format, however, the studied market may induce excessive bidding by vendors. Consistent with our theoretical predictions and those of Carr (2003), higher-value projects attract significantly more bids, with lower average quality. Greater numbers of bids raise the cost to all participants, due to costly bidding and bid evaluation. Perhaps as a consequence, higher-value projects are also much less likely to be awarded.</description><author>Snir, E. M.; Hitt, L. M.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note on Online auctions with costly bid evaluation</title><link>http://www.example.com/articles/1</link><description>Carr, S. M.
As seen in the data of Snir and Hitt (2003), online service procurement auctions often end without achieving a contract despite active bidding, yet the existing auction literature is silent on why this occurs. One explanation for these unconsummated auctions may be that perfectly acceptable bids are submitted but disregarded because of the cost of assessing the bids' viability and the bidders' capabilities. This note examines this possibility by characterizing optimal and equilibrium bidding and bid-evaluation decisions in the presence of these costs.</description><author>Carr, S. M.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An inverse-optimization-based auction mechanism to support a multiattribute RFQ process</title><link>http://www.example.com/articles/1</link><description>Beil, D. R.; Wein, L. M.
We consider a manufacturer who uses a reverse, or procurement, auction to determine which supplier will be awarded a contract. Each bid consists of a price and a set of nonprice attributes (e.g., quality, lead time). The manufacturer is assumed to know the parametric form of the suppliers' cost functions (in terms of the nonprice attributes), but has no prior information on the parameter values. We construct a multiround open-ascending auction mechanism, where the manufacturer announces a slightly different scoring rule (i.e., a function that ranks the bids in terms of the price and nonprice attributes) in each round. Via inverse optimization, the manufacturer uses the bids from the first several rounds to learn the suppliers' cost functions, and then in the final round chooses a scoring rule that attempts to maximize his own utility. Under the assumption that suppliers submit their myopic best-response bids in the last round, and do not distort their bids in the earlier rounds (i.e., they choose their minimum-cost bid to achieve any given score), our mechanism, indeed, maximizes the manufacturer's utility within the open-ascending format. We also discuss several enhancements that improve the robustness of our mechanism with respect to the model's informational and behavioral assumptions.</description><author>Beil, D. R.; Wein, L. M.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Group buying on the Web: A comparison of price-discovery mechanisms</title><link>http://www.example.com/articles/1</link><description>Anand, K. S.; Aron, R.
Web-based group-buying mechanisms are being widely used for both business-to-business (B2B) and business-to-consumer (B2C) transactions. We survey currently operational online group-buying markets, and then study this phenomenon using analytical models. We build on the literatures in information economics and operations management in our analytical model of a monopolist offering Web-based group-buying under different kinds of demand uncertainty. We derive the monopolist's optimal group-buying schedule under varying conditions of heterogeneity in the demand regimes, and compare its profits with those that obtain under the more conventional posted-price mechanism. We further study the impact of production postponement by endogenizing the timing of the pricing and production decisions in a two-stage game between the monopolist and buyers. Our results have implications for firms' choice of price-discovery mechanisms in e-markets, and for the scheduling of production and pricing. decisions in the presence (and absence) of scale economies of production.</description><author>Anand, K. S.; Aron, R.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring the frictional costs of Online transactions: The case of a name-your-own-price channel</title><link>http://www.example.com/articles/1</link><description>Hann, I. H.; Terwiesch, C.
We study the offers submitted by consumers to a large Name-Your-Own-Price (NYOP) online retailer. A distinctive feature of this retailer is that it allows consumers to repeatedly submit offers on one and the same product. While consumers could identify the threshold price (the minimum price for which the retailer is willing to sell) by incrementing their offer in small steps in each consecutive round, such a strategy would require them to go through many additional online transactions. We define frictional cost as the disutility that the consumer experiences when conducting an online transaction, such as submitting an offer. Thus, in our setting, consumers trade off a direct financial value (lower price) for frictional costs. Based on a consumer choice model capturing this trade-off, we use the observed consumer behavior to reconstruct the frictional cost parameters for the consumers in our sample. We show that, perhaps contrary to the general wisdom, frictional costs in electronic markets are substantial, with median values ranging from EUR 3.54 for a portable digital music player (MP3) to EUR 6.08 for a personal digital assistant (PDA). We find that consumers who have gathered experience with the NYOP channel in previous transactions exhibit lower frictional costs than consumers who use the channel for the first time. Surprisingly, sociodemographic variables do not help to explain the variation in frictional costs.</description><author>Hann, I. H.; Terwiesch, C.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Consumer surplus in the digital economy: Estimating the value of increased product variety at Online booksellers</title><link>http://www.example.com/articles/1</link><description>Brynjolfsson, E.; Hu, Y.; Smith, M. D.
W e present a framework and empirical estimates that quantify the economic impact of increased product variety made available through electronic markets. While efficiency gains from increased competition significantly enhance consumer surplus, for instance, by leading to lower average selling prices, our present research shows that increased product variety made available through electronic markets can be a significantly larger source of consumer surplus gains.. One reason for increased product variety on the Internet is the ability of online retailers to catalog, recommend, and provide a large number of products for sale. For example, the number of book titles available at Amazon.com is more than 23 times larger than the number of books on the shelves of a typical Barnes &amp; Noble superstore, and 57 times greater than the number of books stocked in a typical large independent bookstore. Our analysis indicates that the increased product variety of online bookstores enhanced consumer welfare by $731 million to $1.03 billion in the year 2000, which is between 7 and 10 times as large as the consumer welfare gain from increased competition and lower prices in this market. There may also be large welfare gains in other SKU-intensive consumer goods such as music, movies, consumer electronics, and computer software and hardware.</description><author>Brynjolfsson, E.; Hu, Y.; Smith, M. D.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>E-business and management science: Mutual impacts (Part 2 of 2)</title><link>http://www.example.com/articles/1</link><description>Geoffrion, A. M.; Krishnan, R.
This concludes a two-part commentary on management science and e-business, the theme of this two-part special issue. After reviewing the topical clusters that give organization to both parts, we sketch the papers appearing in this second part from the perspective of two key questions concerning the impact of the emerging digital economy on management science research: What fundamentally new research questions arise, and what kind of research enables progress on them. We then offer summary comments on the second question based on the papers in both parts. The principal conclusions are that, in meeting the challenges posed by the digital economy, management science researchers are (a) making greater use of parts of economics and computer science/information technology, and (b) exploiting the improving productivity advantages of empirical and methodological work in comparison with theoretical work.</description><author>Geoffrion, A. M.; Krishnan, R.</author><pubDate>Sat, 01 Nov 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A comment on "price-endings when prices signal quality"</title><link>http://www.example.com/articles/1</link><description>Shoemaker, R.; Mitra, D.; Chen, Y. X.; Essegaier, S.
Stiving (2000) proposes an interesting model to explain price-endings. His analysis shows that even when customer demand increases at 9-ending price points, certain firms that use high prices to signal quality are more likely to set those prices at round numbers. This comment raises two issues about the model. First, it appears that the original paper imposes a condition that has the effect of eliminating a broad range of legitimate separating equilibria from the analyses. Second, it appears that the original model does not include constraints to ensure that the demand for each market segment will be nonnegative. When these constraints on demand are included, one obtains different aggregate demand curves, which leads to different equilibrium prices. Using the revised model and analysis, we find that 71% of the prices end in 9 and only 12% in 0. This contrasts with only 3% ending in 9 and 58% ending in 0 for the original study. Therefore, 9-endings still prevail even though high prices can be used by firms to signal high quality.</description><author>Shoemaker, R.; Mitra, D.; Chen, Y. X.; Essegaier, S.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A live baby or your money back: The marketing of in vitro fertilization procedures</title><link>http://www.example.com/articles/1</link><description>Schmittlein, D. C.; Morrison, D. G.
Many clinics that offer in vitro fertilization (IVF) have begun to market the following options to couples: (1) an a la carte program where the couple pays $7,500 per attempt regardless of the outcome; or (2) a money-back-guarantee program where the couple pays a $15,000 fee that covers up to three attempts, however, if after three cycles there is no live-birth delivery, then the full $15,000 is refunded. We assess the a la carte versus the money-back-guarantee programs, and find the surprising result that the money-back-guarantee program appears (for the patients) to be "too good to be true." That is, the money-back guarantee yields a substantial negative expected profit per couple for the clinics. More importantly from the patients' perspective, the money-back guarantee is the better option for all couples with less than 0.5 success probability per cycle. Virtually all traditional IVF patients have had per-cycle success probabilities below 0.5. A detailed analysis of the key variables - i.e., success rate per attempt, heterogeneity of couples' rates of success, individual couples' "learning" on successive attempts, and cost to the clinic per attempt-shows that these money-back guarantees are unprofitable for the clinics. Since presumably clinics are not in business to lose money, the standard analysis must be missing something major. We suggest that the marketing of money-back guarantees is inducing couples who would previously have used-successfully-other less invasive procedures with fewer side effects and less risk of multiple births to decide to proceed directly to IVF, and that this scenario makes the money-back guarantees profitable for the clinics. The implications of earlier use of IVF are then considered from an overall public policy point of view. just as mothers everywhere tell their children, "When something looks too good to be true, then it is too good to be true!"</description><author>Schmittlein, D. C.; Morrison, D. G.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Good reasons sell: Reason-based choice among group and individual investors in the stock market</title><link>http://www.example.com/articles/1</link><description>Barber, B. M.; Heath, C.; Odean, T.
In this paper, we compare the investment decisions of groups (stock clubs) and individuals. Both individuals and clubs are more likely to purchase stocks that are associated with good reasons (e.g., a company that is featured on a list of most-admired companies). However, stock clubs favor such stocks more than individuals, despite the fact that such reasons do not improve performance. We describe why social dynamics may make good reasons more important for groups than individuals.</description><author>Barber, B. M.; Heath, C.; Odean, T.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring imputed cost in the semiconductor equipment supply chain</title><link>http://www.example.com/articles/1</link><description>Cohen, M. A.; Ho, T. H.; Ren, Z. J.; Terwiesch, C.
We consider the order-fulfillment process of a supplier producing a customized capital good, such as production equipment, commercial aircraft, medical devices, or defense systems. As is common in these industries, prior to receiving a firm purchase order from the customer, the supplier receives a series of shared forecasts, which are called "soft orders." Facing a stochastic internal manufacturing lead time, the supplier must decide at what time to begin the fulfillment of the order. This decision requires a trade-off between starting too early, leading to potential holding or cancellation costs, and starting too late, leading to potential delay costs. We collect detailed data of shared forecasts, actual purchase orders, production lead times, and delivery dates for a supplier-buyer dyad in the semiconductor equipment supply chain. Under the assumption that the supplier acts rationally, optimally balancing the cancellation, holding, and delay costs, we are able to estimate the corresponding imputed cost parameters based on the observed data. Our estimation results reveal that the supplier perceives the cost of cancellation to be about two times higher and the holding costs to be about three times higher than the delay cost. In other words, the supplier is very conservative when commencing the order fulfillment, which undermines the effectiveness of the overall forecast-sharing mechanism.</description><author>Cohen, M. A.; Ho, T. H.; Ren, Z. J.; Terwiesch, C.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Contracts in offshore software development: An empirical analysis</title><link>http://www.example.com/articles/1</link><description>Gopal, A.; Sivaramakrishnan, K.; Krishnan, M. S.; Mukhopadhyay, T.
We study the determinants of contract choice in offshore software development projects and examine how the choice of contract and other factors in the project affect project profits accruing to the software vendor. Using data collected on 93 offshore projects from a leading Indian software vendor, we provide evidence that specific vendor-, client-, and project-related characteristics such as requirement uncertainty, project team size, and resource shortage significantly explain contract choice in these projects. Our analysis suggests that contract choice significantly determines project profit. Additionally, some ex ante vendor-, client-, and project-related characteristics known at the time of choosing the contract continue to significantly influence project profits after controlling for contract choice. We also provide evidence to show that project duration and team size affect project profits.</description><author>Gopal, A.; Sivaramakrishnan, K.; Krishnan, M. S.; Mukhopadhyay, T.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Social networks in organizational emergence: The university spinout phenomenon</title><link>http://www.example.com/articles/1</link><description>Nicolaou, N.; Birley, S.
This paper aims to ascertain the influence of social networks in the university spinout phenomenon. With respect to the instrumental role of social networks, it adopts a content contingency perspective pertaining to the role of closure and structural holes, and examines the interaction between relational and structural embeddedness in the academics' network structure. With respect to spinout outcomes, this paper distinguishes between academic exodus and stasis, and differentiates between types of spinouts based on the degree of involvement of the key academics. It also examines networks at the team level of analysis and submits that team-level structural differences exist between the different spinout structures.</description><author>Nicolaou, N.; Birley, S.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An interactive evolutionary metaheuristic for multiobjective combinatorial optimization</title><link>http://www.example.com/articles/1</link><description>Phelps, S.; Koksalan, M.
We propose an evolutionary metaheuristic for multiobjective combinatorial optimization problems that interacts with the decision maker (DM) to guide the search effort toward his or her preferred solutions. Solutions are presented to the DM, whose pairwise comparisons are then used to estimate the desirability or fitness of newly generated solutions. The evolutionary algorithm comprising the skeleton of the metaheuristic makes use of selection strategies specifically designed to address the multiobjective nature of the problem. Interactions With the DM are triggered by a probabilistic evaluation of estimated fitnesses, while memory structures with indifference thresholds restrict the presentation of solutions resembling those that have already been rejected. The algorithm has been tested on a number of random instances of the Multiobjective Knapsack Problem (MOKP) and the Multiobjective Spanning Tree Problem (MOST). Simulation results indicate that the algorithm requires only a small number of comparisons to be made for satisfactory solutions to be found.</description><author>Phelps, S.; Koksalan, M.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Resource allocation in multisite service systems with intersite customer flows</title><link>http://www.example.com/articles/1</link><description>Chao, X. L.; Liu, L. M.; Zheng, S. H.
Motivated by a project in health-care management, we study the resource allocation problem in multisite service systems with intersite customer flows. We aim at providing insights to and guidelines for resource allocation in these service systems when some service criterion, such as average waiting time, loss rate, or blocking probability, is a major concern. We develop analytical optimization models and from them we obtain the explicit optimal allocation policy. Our results demonstrate that the optimal resource allocation solution exhibits a structure of "one large and many small."</description><author>Chao, X. L.; Liu, L. M.; Zheng, S. H.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Safeguarding interorganizational performance and continuity under ex post opportunism</title><link>http://www.example.com/articles/1</link><description>Jap, S. D.; Anderson, E.
Opportunism is a central construct in exchange theory. Economists contend that despite the firm's best efforts to erect governance structures that reduce opportunism and preserve outcomes, there is always some opportunism that remains once the transaction is in,place. Despite this, there are few studies that systematically investigate the safeguarding efficacy of relationship attributes in the presence of such ex post opportunism. In this research, we develop a theoretical framework and provide a longitudinal test of the ability of various relationship safeguards to preserve performance outcomes and future expectations given varying levels of ex post opportunism in the relationship. Our survey results from over 300 buyers and suppliers indicates that given lower levels of opportunism, bilateral idiosyncratic investments and interpersonal trust enhance performance outcomes and future expectations, while goal congruence has no discernable effect. However, at higher levels of opportunism, goal congruence becomes a more powerful safeguard, while interpersonal trust becomes less effective. Bilateral idiosyncratic investments continue to preserve performance outcomes and future expectations even at higher levels of opportunism. Implications for the long-term management of interorganizational alliances are discussed.</description><author>Jap, S. D.; Anderson, E.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reply to "A comment on 'price-endings when prices signal quality'"</title><link>http://www.example.com/articles/1</link><description>Stiving, M.
Although the comment provides some additional insight, it in no way diminishes the contribution of the original paper. That paper is the first and only theoretical explanation for why firms behave as though they use round prices to signal quality. Further, the paper provides empirical evidence in support of this explanation.</description><author>Stiving, M.</author><pubDate>Mon, 01 Dec 2003 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Approximating multiobjective knapsack problems</title><link>http://www.example.com/articles/1</link><description>Erlebach, T.; Kellerer, H.; Pferschy, U.
For multiobjective optimization problems, it is meaningful to compute a set of solutions covering all possible trade-offs between the different objectives. The multiobjective knapsack problem is a generalization of the classical knapsack problem in which each item has several profit values. For this problem, efficient algorithms for computing a provably good approximation to the set of all nondominated feasible solutions, the Pareto frontier, are studied. For the multiobjective one-dimensional knapsack problem, a practical fully polynomial time approximation scheme (FPTAS) is derived. It is based on a new approach to the single objective knapsack problem using a partition of the profit space into intervals of exponentially increasing length. For the multiobjective m-dimensional knapsack problem, the first known polynomial-time approximation scheme (PTAS), based on linear programming, is presented.</description><author>Erlebach, T.; Kellerer, H.; Pferschy, U.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Perturbing nonnormal confidential attributes: The copula approach</title><link>http://www.example.com/articles/1</link><description>Sarathy, R.; Muralidhar, K.; Parsa, R.
Protecting confidential, numerical data in databases from. disclosure is an important issue both for commercial organizations as well as data-gathering and disseminating organizations (such as the Census Bureau). Prior studies have shown that perturbation methods are effective in protecting such confidential data from snoopers. Perturbation methods have to provide legitimate users. with accurate (unbiased) information, and also provide adequate security against disclosure of confidential information to snoopers. For databases described by nonnormal multivariate distributions, existing perturbation methods do not provide unbiased characteristics. In this study, we develop a copula-based perturbation method capable of maintaining the marginal distribution of perturbed attributes to be the same before and after perturbation. In addition, this method also preserves the rank order correlation between the confidential and nonconfidential attributes, thereby maintaining monotonic relationships between attributes. The method proposed in this study provides a high level of protection, against inferential disclosure. An investigation of the new perturbation method for simulated databases shows that the method performs effectively The methodology presented in this study represents a significant step toward improving the practical. applicability of data perturbation methods.</description><author>Sarathy, R.; Muralidhar, K.; Parsa, R.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Capacity management in decentralized networks</title><link>http://www.example.com/articles/1</link><description>Masuda, Y.; Whang, S.
Bottleneck analysis is a useful tool in capacity planning for centrally controlled network systems. However, under a decentralized network where individual users are allowed to select their own routes, straightforward application of bottleneck analysis does not necessarily yield an optimal performance. It may even hurt the system performance-an aspect of Braess's paradox. We investigate the capacity expansion problem for a decentralized system with general network topology. To this end, we first discuss the short-run problem and show that the, externality pricing solves the joint problem of demand and routing control. We then study the capacity expansion/reduction problem for decentralized systems that may or may not be optimally controlled in the short run.</description><author>Masuda, Y.; Whang, S.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note: Optimality conditions for an (s,S) policy with proportional and lump-sum penalty costs</title><link>http://www.example.com/articles/1</link><description>Cetinkaya, S.; Parlar, M.
We consider the optimality of the (s, S) policy for a periodic-review stochastic inventory problem with two types of shortage costs. The 'problem may arise in a rush-order application at a bank branch where the emergency provision costs during a foreign currency stockout are represented by proportional and lump-sum penalties. Aneja and Noori (1987) analyzed this problem and presented a set of conditions for the convexity of a particular function and made a claim about the K-convexity of another function to prove the optimality of the (s, S) policy. We show that because the function that is claimed to be K-convex is actually concave over a subset of its domain, Aneja and Noori' arguments cannot be used to prove the optimality of the (s, S) policy. However, we argue that Aneja and Noori's problem is equivalent to the typical lost-sales problem, and using this equivalence, we find a simple convexity. condition that assures the optimality of the (s, S) policy.</description><author>Cetinkaya, S.; Parlar, M.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note on "guarantees in auctions: The auction house as negotiator and managerial decision maker"</title><link>http://www.example.com/articles/1</link><description>Greenleaf, E. A.; Ma, J.; Qiu, W. H.; Rao, A. G.; Sinha, A. R.
In this note,(1) we identify two errors in Greenleaf, Rao, and Sinha's (1993) analysis of negotiation of guarantees in auctions. This note provides a high-level but self-contained summary of the revised results. We find that, in contrast with the earlier claim, guaranteed auctions lead to-greater total expected revenue than conventional auctions. The ability to bargain over guarantee values and commissions certainly benefits sellers but may hurt the profits of auction houses. We relate these results to recent events in auction markets.</description><author>Greenleaf, E. A.; Ma, J.; Qiu, W. H.; Rao, A. G.; Sinha, A. R.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Solving multi-item lot-sizing problems with an MIP solver using classification and reformulation</title><link>http://www.example.com/articles/1</link><description>Wolsey, L. A.
Based on research on the polyhedral structure of lot-sizing models over the last 20 years, we claim that there is a nontrivial fraction of practical lot-sizing problems that can now be solved by nonspecialists just by taking an appropriate a priori reformulation of the problem, and then feeding the resulting formulation into a commercial mixed-integer programming solver. This claim uses the fact that many multi-item problems decompose naturally into a set of single-item problems with linking constraints, and that there is now a large body of knowledge about single-item problems. To put this knowledge to use, we propose a classification of lot-sizing problems (in large part single-item) and then indicate in a set of tables, what is known about a particular problem class and how useful it might be. Specifically, we indicate for each class (i) whether a tight extended. formulation is known, and its size; (ii) whether one or more families of valid inequalities are known defining the convex hull of solutions, and the complexity of the corresponding separation algorithms; and (iii) the complexity of the corresponding optimization algorithms (which would be useful if a column generation or Lagrangian relaxation approach was envisaged). Three distinct multi-item lot-sizing instances are then presented to demonstrate the approach, and comparative computational results are presented. Finally, we also use the classification to point out what appear to be some of the important open questions and challenges.</description><author>Wolsey, L. A.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Comment on "generating scenario trees for multistage decision problems"</title><link>http://www.example.com/articles/1</link><description>Klaassen, P.
In models of decision making under uncertainty; one typically has to approximate the uncertainties by a limited number of discrete outcomes. Hoyland and Wallace (2001) formulate a nonlinear programming problem to generate such a limited number of discrete outcomes while satisfying specified statistical properties. They have developed and employed.. this method for a stochastic multistage asset-allocation problem. When the method is applied to such financial optimization problems under uncertainty, we argue here that it does not suffice to match statistical properties. To obtain realistic outcomes, the.(limited) description : of the uncertainty in such models should also exclude arbitrage opportunities, and thereby, be consistent with financial asset pricing theory. We illustrate that the method proposed by Hoyland and Wallace can result in arbitrage opportunities in the scenario tree if only statistical properties are imposed. We show how one can check ex post for the presence of arbitrage opportunities in a scenario tree by checking, for the existence of solutions to sets of linear equations. Arbitrage opportunities can also be precluded ex ante. in the scenario tree. by adding constraints to the nonlinear programming problem of Hoyland and Wallace.</description><author>Klaassen, P.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Product design with multiple quality-type attributes</title><link>http://www.example.com/articles/1</link><description>Kim, K.; Chhajed, D.
We consider a product line design problem with multiple attributes for a monopolist serving a market with two customer segments. Products are designed with quality-type attributes for which more is always better than less. By considering multiple attributes, we derive a measure of multidimensional customer preference and offer insights into the optimal product design. When customers' preferences exhibit different orders in different attributes, our results show that products are differentiated horizontally where no one product is better than the other with respect to all attributes, and that there exists a region where the first-best solution for the monopolist is feasible despite the problem of cannibalization. Furthermore, single-product offering strategies are never optimal, so pooling of customer segments or reduction of the number of segments served will not occur.</description><author>Kim, K.; Chhajed, D.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal stock allocation for a capacitated supply system</title><link>http://www.example.com/articles/1</link><description>de Vericourt, F.; Karaesmen, F.; Dallery, Y.
We consider a capacitated supply system that produces a single item that is demanded by several classes of customers. Each customer class may have a different backorder cost, so stock allocation arises as a key decision problem. We model the supply system as a multicustomer make-to-stock queue. Using dynamic programming, we show that the optimal allocation policy has a simple and intuitive structure. In addition, we present an efficient algorithm to compute the parameters of this optimal allocation policy. Finally, for. a typical supply chain design problem, we illustrate that ignoring the stock allocation dimension-a frequently encountered simplifying assumption-can lead to incorrect managerial decisions.</description><author>de Vericourt, F.; Karaesmen, F.; Dallery, Y.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Return on assets loss from situational and contingency misfits</title><link>http://www.example.com/articles/1</link><description>Burton, R. M.; Lauridsen, J.; Obel, B.
We develop a rule-based contingency misfit model and related hypotheses to test empirically the Burton and Obel (1998) multicontingency model forstrategic organizational design: The model is a set of "if-then" misfit rules, in which misfits lead to a loss in performance; they are complements to the strategy and organizational contingency theory fit rules. Using data from 224 small- and medium-sized Danish firms, misfits are categorized, and; identified: Then, performance hypotheses, are developed. and tested using. regression models. We confirm the hypotheses that firms with situational misfits or contingency misfits, or both,, incur performance losses in return on assets compared with firms with no misfits. Contrary to our hypotheses;; we did not find that additional misfits lead to increased performance loss. Our results suggest that just one misfit of any kind may significantly compromise performance. These results yield a deeper understanding of organizational contingency theory, as well as implications for the rule-based fit-misfit organizational design model.</description><author>Burton, R. M.; Lauridsen, J.; Obel, B.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Effective zero-inventory-ordering policies for the single-warehouse multiretailer problem with piecewise linear cost structures</title><link>http://www.example.com/articles/1</link><description>Chan, L. M. A.; Muriel, A.; Shen, Z. J. M.; Simchi-Levi, D.; Teo, C. P.
We analyze the problem faced by companies that rely on TL (Truckload) and LTL (Less than Truckload) carriers for the distribution of products across their supply chain. Our goal is to design simple inventory policies and transportation strategies to satisfy time-varying demands over a finite horizon, while minimizing systemwide cost by taking advantage of quantity discounts in the transportation cost structures. For this purpose, we study the cost effectiveness of restricting the inventory policies to the class of zero-inventory-ordering (ZIO) policies in a single-warehouse multiretailer scenario in which the warehouse serves as a cross-dock facility. In particular, we demonstrate that there exists a ZIO inventory policy whose total inventory and transportation cost is no more than 4/3 (5.6/4.6 if transportation costs are stationary) times the optimal cost. However, finding the best ZIO policy is an NP-hard problem as well. Thus, we propose two algorithms to find an effective ZIO policy: An exact algorithm whose running time is polynomial for any fixed number of retailers, and a linear-programming-based heuristic whose effectiveness is demonstrated in a series of computational experiments. Finally, we extend the worst-case results developed in this paper to systems in which the warehouse does hold inventory.</description><author>Chan, L. M. A.; Muriel, A.; Shen, Z. J. M.; Simchi-Levi, D.; Teo, C. P.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Human capital and institutional determinants of information technology compensation: Modeling multilevel and cross-level interactions</title><link>http://www.example.com/articles/1</link><description>Ang, S.; Slaughter, S.; Ng, K. Y.
Compensation is critical in attracting and retaining information technology (IT) professionals. However, there has been very little research on IT compensation. Juxtaposing theories of compensation that focus on human capital endowments and labor market segmentation, we hypothesize multilevel and cross-level determinants of compensation. We use hierarchical linear modeling to analyze archival salary data for 1,576 IT professionals across 39 institutions. Results indicate that compensation is directly determined by human capital endowments of education and experience. Institutional differentials do not directly drive compensation, but instead moderate the relationship of human capital endowments to compensation. Large institutions pay more than small institutions to IT professionals with more education, while small institutions pay more than large institutions to IT professionals with less education. Not-for-profit institutions pay more than for-profits to IT professionals with more or IT-specific education. Further, information-intensive institutions pay more than noninformation-intensive institutions to IT professionals with more or IT-specific education. We interpret these results in the context of institutional rigidity, core competencies, and labor shortages in the IT labor market.</description><author>Ang, S.; Slaughter, S.; Ng, K. Y.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordinating mechanisms in care provider groups: Relational coordination as a mediator and input uncertainty as a moderator of performance effects</title><link>http://www.example.com/articles/1</link><description>Gittell, J. H.
This paper proposes a model of how coordinating mechanisms work, and tests it in the context of patient care. Consistent with organization design theory, the performance effects of boundary spanners and team meetings were mediated by relational coordination, a communication- and relationship-intensive form of coordination. Contrary to organization. design theory, however, the performance effects of routines were also mediated :by relational coordination. Rather than serving as a replacement for interactions, as anticipated by organization design theory, routines work by enhancing interactions among participants. Likewise, all three coordinating mechanisms, including routines, were found to be increasingly effective under conditions of uncertainty.</description><author>Gittell, J. H.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal dynamic auctions for revenue management</title><link>http://www.example.com/articles/1</link><description>Vulcano, G.; van Ryzin, G.; Maglaras, C.
We analyze a dynamic auction, in which a seller, with C units to sell faces a sequence of buyers separated into T time periods. Each group of buyers has independent, private values for a single unit. Buyers compete directly against each other within a period, as in a traditional auction, and indirectly with buyers in other periods through the opportunity cost of capacity assessed by the seller. The number of buyers in each period, as well as the individual buyers' valuations, are random. The model is a variation of the traditional singleleg, multiperiod revenue management problem, in which consumers act strategically and bid for units of a fixed capacity over time. For this setting, we prove that dynamic variants of the first-price and second-price auction mechanisms maximize the seller's expected revenue. We also show explicitly how to compute and implement these optimal auctions. The optimal auctions are then compared to a traditional revenue management mechanism-in which list prices are used in each period together with capacity controls-and, to a simple auction heuristic that consists of allocating units to each period and running a sequence of standard, multiunit auctions with fixed reserve prices in each period. The traditional revenue management mechanism is proven to be optimal in the limiting cases when there is at most one buyer per period, when capacity is not constraining, and asymptotically when the number of buyers and the capacity increases. The optimal auction significantly outperforms both suboptimal mechanisms when there are a moderate number of periods, capacity is constrained, and. the total volume of sales is not too large. The benefit also increases when variability in the dispersion in buyers' valuations or in the number of buyers per period increases.</description><author>Vulcano, G.; van Ryzin, G.; Maglaras, C.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Hospital operating room capacity expansion</title><link>http://www.example.com/articles/1</link><description>Lovejoy, W. S.; Li, Y.
A large midwestern hospital is expecting an increase in, surgical caseload: New operating, room (OR) capacity can be had by building new ORs or extending the working hours in the current ORs. The choice among these options is complicated by the fact that patients; surgeons and, surgical staff, and hospital administrators are all important stakeholders in: the health service operation, and each has different priorities. This. paper investigates the trade-offs among three performance criteria (wait to get on schedule, scheduled procedure start-tune reliability; and hospital profits), which are of particular importance to the different constituencies. The objective is to determine how the hospital can best expand its capacity, acknowledging the key role that each constituency plays in that objective. En route, the paper presents supporting analysis for process improvements and suggestions for. optimal participation-inducing staff contracts for extending OR hours-of operation.</description><author>Lovejoy, W. S.; Li, Y.</author><pubDate>Fri, 01 Nov 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic portfolio selection of NPD programs using marginal returns</title><link>http://www.example.com/articles/1</link><description>Loch, C. H.; Kavadias, S.
Selecting. program portfolios within a budget constraint is, an important challenge in the management of, new product development (NPD). Optimal portfolios are difficult to define because of the combinatorial. complexity of project combinations. However, at the aggregate level of the strategic allocation of resources across product lines, investment in, a program is not an all-or-nothing decision, but can be adjusted,. resulting in a higher or lower program benefit (e.g., higher or lower quality). In some cases, resources can be adjusted even for individual projects. With this insight, one can use marginal analysis to optimally allocate the scarce budget. This article develops a dynamic model of resource allocation, taking into account multiple interacting factors, such as independent or correlated, uncertain market payoffs that change over, time, increasing or decreasing returns from the NPD investment, carry-over of the investment benefit over multiple periods, and interactions across market segments.. We characterize optimal policies in closed form and derive qualitative decision rules for managers.</description><author>Loch, C. H.; Kavadias, S.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Using a Bayesian approach to quantify scale compatibility bias</title><link>http://www.example.com/articles/1</link><description>Anderson, R. M.; Hobbs, B. F.
his paper proposes a new analytical framework to quantify and correct for scale compatibility bias in the assessment of trade-off weights in multiattribute value analysis. The procedure is demonstrated with an application to a fisheries management problem. Tradeoff judgments are elicited from a group of fisheries experts with management responsibility in the Lake Erie basin. Then we use a Bayesian method to compute posterior probability distributing of attribute weights. In computing the Bayesian weights, our measurement model assumes that the weight ratios produced by each respondent's judgments are subject to random error and an unknown scale compatibility bias. Ratios are log-transformed and analyzed by a Bayesian linear model with a noninformative prior distribution. Posterior distributions are then developed for the weights and the bias. We estimate the compatibility bias for each person and, in most cases, it is found to be large and in the predicted direction, suggesting the importance of its consideration in deriving trade-off weights. In addition, the Bayesian framework is shown to be useful for quantifying the value of additional information about multiattribute weights. Finally, a simple heuristic procedure for assessing the, weights appears to be effective in eliminating the bias.</description><author>Anderson, R. M.; Hobbs, B. F.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multistage production to order with rework capability</title><link>http://www.example.com/articles/1</link><description>Grosfeld-Nir, A.; Gerchak, Y.
This study considers multistage production systems where defective units can be reworked repeatedly at every stage. Production, as well as rework, is in lots requiring set-up and variable production costs, and orders need to be filled in their entirety. The yield of each stage is uncertain, so several production runs may need to be attempted until the quantity of finished products is sufficient. The trade-off at each stage is between using small lots, possibly necessitating repeated rework set-ups and large lots, which may result in costly overproduction. Multistage manufacturing facilities with rework capabilities are quite common in practice, but their optimal operation when orders have to be met in full has been virtually unexplored. We, show that a multistage system where only one of the stages requires a setup (a "single-bottleneck system") can be reduced to a single-stage system. Moreover, if it is possible to arrange the operations in any order, we prove that it is best to make the "bottleneck" the first stage of the system. We also develop recursive algorithms for solving two- and three-stage systems, where all stages require set-ups, optimally, Generalizations to systems where rework yields and costs differ from those of initial processing are also discussed.</description><author>Grosfeld-Nir, A.; Gerchak, Y.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information flows in capacitated supply chains with fixed ordering costs</title><link>http://www.example.com/articles/1</link><description>Gavirneni, S.
Many organizations have only recently recognized that sharing information with other members in their supply chain can lead to significant reduction in the total costs. Usually these information flows are incorporated into existing operating policies at the various parties. In this paper we argue that, in some cases, it may be necessary to change the way the supply chain is managed to make complete use of the information flows. We support this argument by analyzing a supply chain containing a capacitated supplier and a retailer facing i.i.d. demands. In addition there are fixed ordering costs between the retailer and the supplier. In this setting, we consider two models: (1) the retailer is using the optimal (s, S) policy and providing the supplier information about her inventory levels; and (2) the retailer, still sharing information on her inventory levels, orders in a period only if by the previous period the cumulative end-customer demand since she last ordered was greater than delta. Thus, in Model 1, information sharing is used to supplement existing policies; while, in Model 2, we have redefined operating policies to make better use of the information flows. We will show, via a detailed computational study, that the total supply chain costs of Model 2 are 10.4% lower, on the average, than that of Model 1. We noticed that this reduction in costs is higher at higher capacities, higher supplier penalty costs, lower retailer penalty costs, moderate values of set-up cost, and at lower end-customer demand variances.</description><author>Gavirneni, S.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A dynamic programming procedure for pricing American-style Asian options</title><link>http://www.example.com/articles/1</link><description>Ben-Ameur, H.; Breton, M.; L'Ecuyer, P.
Pricing European-style Asian options based on the arithmetic average, under the Black and Scholes model, involves estimating an integral (a mathematical expectation) for which no easily computable analytical solution is available. Pricing their American-style counterparts, which provide early exercise opportunities, poses the additional difficulty of solving a dynamic optimization problem to determine the optimal exercise strategy. A procedure for pricing American-style Asian options of the Bermudan flavor, based on dynamic programming combined with finite-element piecewise-polynomial approximation of the value function, is developed here. A convergence proof is provided. Numerical experiments illustrate the consistency and efficiency of the procedure. Theoretical properties of the value function and of the optimal exercise strategy are also established.</description><author>Ben-Ameur, H.; Breton, M.; L'Ecuyer, P.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Adaptive inventory control for nonstationary demand and partial information</title><link>http://www.example.com/articles/1</link><description>Treharne, J. T.; Sox, C. R.
This paper examines several different policies for an inventory control problem in which the demand process is nonstationary and partially observed. The probability distribution for the demand in each period is determined by the state of a Markov chain, the core process. However, the state of this core process is not directly observed, only the actual demand is observed by the decision maker. Given this demand process, the inventory control problem is a composite-state, partially observed Markov decision process (POMDP), which is an appropriate model for a number of dynamic demand problems. In practice, managers often use certainty equivalent control (CEC) policies to solve such a problem. However, this paper presents results that demonstrate that there are other practical control policies that almost always provide much better solutions for this problem than the CEC policies commonly used in practice. The computational results also indicate how specific problem characteristics influence the performance of each of the alternative policies.</description><author>Treharne, J. T.; Sox, C. R.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Process and product improvement in manufacturing systems with correlated stages</title><link>http://www.example.com/articles/1</link><description>Zantek, P. F.; Wright, G. P.; Plante, R. D.
Manufacturing systems typically contain processing and assembly stages whose output quality is significantly affected by the output quality of preceding stages in the system. This study offers and empirically validates a procedure for (1) measuring the effect of each stage's performance on the output quality of subsequent stages including the quality of the final product, and (2) identifying stages in a manufacturing system where management should concentrate investments in process quality improvement. Our proposed procedure builds on the precedence ordering of the stages in the system and uses the information provided by correlations between the product quality measurements across stages. The starting point of our procedure is a computer executable network representation of the statistical relationships between the product quality measurements; execution automatically converts the network to a simultaneous-equations model and estimates the model parameters by the method of least squares. The parameter estimates are used to measure and rank the impact of each stage's performance on variability in intermediate stage and final product quality. We extend our work by presenting an economic model, which uses these results, to guide management in deciding on the amount of investment in process quality improvement for each stage. We report some of the findings from an extensive empirical validation of our procedure using circuit board production line data from a major electronics manufacturer. The empirical evidence presented here highlights the importance of accounting for quality linkages across stages in (a) identifying the sources of variation in product quality and (b) allocating investments in process quality improvement.</description><author>Zantek, P. F.; Wright, G. P.; Plante, R. D.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bounding option prices by semidefinite programming: A cutting plane algorithm</title><link>http://www.example.com/articles/1</link><description>Gotoh, J.; Konno, H.
In a recent article, Bertsimas and Popescu showed that a tight upper bound on a European-type call option price, given the first n moments of the distribution of the underlying security price, can be obtained by solving an associated semidefinite programming problem (SDP). The purpose of this paper is to improve and extend their results. We will show that a tight lower bound can be calculated by solving another SDP. Also, we will show that these problems can be solved very quickly by a newly developed cutting plane algorithm when n is less than six or seven.</description><author>Gotoh, J.; Konno, H.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling and analysis of congestion in the design of facility layouts</title><link>http://www.example.com/articles/1</link><description>Benjaafar, S.
Reducing manufacturing lead times and minimizing work-in-process (WIP) inventories are the cornerstones of popular manufacturing strategies such as Lean, Quick Response, and Just-in-Time Manufacturing. In this paper, we present a model that captures the relationship between facility layout and congestion-related measures of performance. We use the model to introduce a formulation of the facility layout design problem where the objective is to minimize work-in-process (WIP). In contrast to some recent research, we show that layouts obtained using a WIP-based formulation can be very different from those obtained using the conventional quadratic assignment problem (QAP) formulation. For example, we show that a QAP-optimal layout can be WIP-infeasible. Similarly, we show that two QAP-optimal layouts can have vastly different WIP values. In general, we show that WIP is not monotonic in material-handling travel distances. This leads to a number of surprising results. For instance, we show that it is possible to reduce overall distances between departments but increase WIP. Furthermore, we find that the relative desirability of a layout can be affected by changes in material-handling capacity even when travel distances remain the same. We examine the effect of various system parameters on the difference in WIP between QAP- and WIP-optimal layouts. We find that although there are conditions under which the difference in WIP is significant, there are those under which both layouts are WIP-equivalent.</description><author>Benjaafar, S.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note: Ranking DMUs with infeasible super-efficiency DEA models</title><link>http://www.example.com/articles/1</link><description>Xue, M.; Harker, P. T.
It has been suggested in the data envelopment analysis (DEA) literature that it is impossible to obtain a full ranking of decision-making units (DMUs) when infeasible subproblems arise in the so-called super-efficiency IDEA models under different returns to scale (RTS) assumptions other than constant returns to scale (CRS) and consequently the application of the super-efficiency DEA models under different RTS conditions other than CRS should be restricted. The implications of the infeasibility in super-efficiency IDEA models with respect to the efficiency ranking of the DMUs is explored. Based on the analysis, we show that it is still possible to obtain the full ranking of the entire observation set when infeasibilities arise in super-efficiency IDEA models.</description><author>Xue, M.; Harker, P. T.</author><pubDate>Wed, 01 May 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Neuronal substrates for choice under ambiguity, risk, gains, and losses</title><link>http://www.example.com/articles/1</link><description>Smith, K.; Dickhaut, J.; McCabe, K.; Pardo, J. V.
Economic forces shape the behavior of individuals and institutions. Forces affecting individual behavior are attitudes about payoffs (gains and losses) and beliefs about outcomes (risk and ambiguity). Under risk, the likelihoods of alternative outcomes are fully known. Under ambiguity, these likelihoods are unknown. In our experiment, payoffs and outcomes were manipulated independently during a classical choice task as brain activity was measured with positron emission tomography (PET). Here, we show that attitudes about payoffs and beliefs about the likelihood of outcomes exhibit interaction effects both behaviorally and neurally. Participants are risk averse in gains and risk-seeking in losses; they are ambiguity-seeking in neither gains nor losses. Two neural substrates for choice surfaced in the interaction between attitudes and beliefs: a dorsomedial neocortical system and a ventromedial system. This finding reveals that the brain does not honor a prevalent assumption of economics-the independence of the evaluations of payoffs and outcomes. The demonstration of a relationship between brain activity and observed economic choice attests to the feasibility of a neuroeconomic decision science.</description><author>Smith, K.; Dickhaut, J.; McCabe, K.; Pardo, J. V.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of the secondary market on the supply chain</title><link>http://www.example.com/articles/1</link><description>Lee, H.; Whang, S.
This paper investigates the impacts of a secondary market where resellers can buy and sell excess inventories. We develop a two-period model with a single manufacturer and many resellers. At the beginning of the first period resellers order and receive products from the manufacturer, but at the beginning of the second period, they can trade inventories among themselves in the secondary market. We endogenously derive the optimal decisions for the resellers, along with the equilibrium market price of the secondary market. The secondary market creates two interdependent effects-a quantity effect (sales by the manufacturer) and an allocation effect (supply chain performance). The former is indeterminate; i.e., the total sales volume for the manufacturer may increase or decrease, depending on the critical fractile, The latter is always positive; i.e., the secondary market always improves allocative efficiency. The sum of the effects is also unclear-the welfare of the supply chain may or may not increase as a result of the secondary market. Lastly, we study potential strategies for the manufacturer to increase sales in the presence of the secondary market.</description><author>Lee, H.; Whang, S.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Do better customers utilize electronic distribution channels? The case of PC banking</title><link>http://www.example.com/articles/1</link><description>Hitt, L. M.; Frei, F. X.
Many service firms are pursuing electronic distribution strategies to augment existing physical infrastructure for product and service delivery. But little systematic study has been made for whether and how characteristics or behaviors might differ between customers who use electronic delivery systems and those who use traditional channels. We explore these differences by comparing customers who utilize personal-computer-based home banking (PC banking) to other bank customers. Case studies and detailed customer data from four institutions suggest that PC banking customers are apparently more profitable, principally due to unobservable characteristics extant before the adoption of PC banking. Demographic characteristics and changes in customer behavior following adoption of PC banking account for only a small fraction of overall differences. It also appears that retention is marginally higher for customers of the online channel.</description><author>Hitt, L. M.; Frei, F. X.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Privacy protection of binary confidential data against deterministic, stochastic, and insider threat</title><link>http://www.example.com/articles/1</link><description>Garfinkel, R.; Gopal, R.; Goes, P.
A practical model and an associated method are developed for providing consistent, deterministically correct responses to ad-hoc queries to a database containing a field of binary confidential data. COUNT queries, i.e., the number of selected subjects whose confidential datum is positive, are to be answered. Exact answers may allow users to determine an individual's confidential information. Instead, the proposed technique gives responses in the form of a number plus a guarantee so that the user can determine an interval that is sure to contain the exact answer. At the same time, the method is also able to provide both deterministic and stochastic protection of the confidential data to the subjects of the database. Insider threat is defined precisely and a simple option for defense against it is given. Computational results on a simulated database are very encouraging in that most queries are answered with tight intervals, and that the quality of the responses improves with the number of subjects identified by the query. Thus the results are very appropriate for the very large databases prevalent in business and governmental organizations. The technique is very efficient in terms of both time and storage requirements, and is readily scalable and implementable.</description><author>Garfinkel, R.; Gopal, R.; Goes, P.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Breaking through the clutter: Benefits of advertisement originality and familiarity for brand attention and memory</title><link>http://www.example.com/articles/1</link><description>Pieters, R.; Warlop, L.; Wedel, M.
Rising levels of advertising competition have made it increasingly difficult to attract and hold consumers' attention and to establish strong memory traces for the advertised brand. A common communication strategy to break through this competitive clutter is to increase ad originality However, ad originality may have detrimental effects when consumers pay more attention to the ad at the expense of the advertised brand. Moreover, the positive effects of originality may quickly wane when the ad becomes familiar. Surprisingly, no research to date has examined such brand attention and memory effects of ad originality and familiarity The current study aims to fill this void. We use a stochastic model of the influence that ad originality and familiarity have on consumers' eye fixations to the key elements of advertisements-brand, text, and pictorial-and how the information extracted during eye fixations promotes memory for the advertised brand. The model explicitly accounts for heterogeneity due to consumers and advertisements. Infrared eye tracking was applied to collect eye fixation data from 119 consumers who paged through two general-audience magazines containing 58 full-page advertisements. Memory for the advertised brands was assessed with an indirect memory task. The model was estimated using Markov Chain Monte Carlo (MCMC) methods. In support of our hypotheses, original advertisements drew more attention to the advertised brand. More importantly however, advertisements that were both original and familiar attracted the largest amount of attention to the advertised brand, which improved subsequent brand memory. In addition, original and familiar ads were found to promote brand memory directly. Implications of these findings for communication and media planning strategy are discussed.</description><author>Pieters, R.; Warlop, L.; Wedel, M.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>CEO characteristics and firm R&amp;D spending</title><link>http://www.example.com/articles/1</link><description>Barker, V. L.; Mueller, G. C.
Over the past fifteen years, a number of studies have examined the determinants of firm R&amp;D spending. These studies, however, almost invariably focus on the role of firm or external ownership characteristics in predicting R&amp;D spending while overlooking the attributes of the top managers involved in allocating corporate resources. In this study, we change that focus by empirically examining how R&amp;D spending as compared to industry competitors varies at firms based on the characteristics of their CEOs. Using a sample of publicly traded firms, we find that CEO characteristics explain a significant proportion of the sample variance in firm R&amp;D spending even when corporate strategy, ownership structure, and other firm-level attributes are controlled. In terms of individual CEO characteristics, we find that R&amp;D spending is greater at firms where CEOs are younger, have greater wealth invested in firm stock and significant career experience in marketing and/or engineering/R&amp;D. In contrast to existing theory we find that the amount of a CEO's formal education had no significant association with R&amp;D spending once a CEO has attained a college degree. However, significant R&amp;D spending increases are found at firms where CEOs have advanced science-related degrees. From subgroup analyses, we further find that CEO effects on relative R&amp;D spending increase with longer CEO tenure implying that CEOs, over time, may mold R&amp;D spending to suit their own preferences. From these results, we make implications for both research on determinants of R&amp;D spending and managerial practice.</description><author>Barker, V. L.; Mueller, G. C.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A context-dependent model of the gambling effect</title><link>http://www.example.com/articles/1</link><description>Bleichrodt, H.; Schmidt, U.
This paper presents a context-dependent theory of decision under risk. The relevant contextual factor is the presence of a riskless lottery in a preference comparison. The theory only deviates from expected utility if the set of options contains both riskless and risky lotteries, The main motivation for the theory is to explain the gambling effect. Contrary to previous theories of the gambling effect, the present theory is consistent with stochastic dominance. It can, however, violate transitivity. The theory allows for a decomposition of the interaction between risk aversion and gambling aversion and thereby extends the classical Arrow-Pratt measure of risk aversion.</description><author>Bleichrodt, H.; Schmidt, U.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic equilibrium for a pair of competing servers with convex cost and balking</title><link>http://www.example.com/articles/1</link><description>Christ, D.; Avi-Itzhak, B.
A two-person game is formulated for a queuing situation involving a pair of exponential servers competing for arriving customers. The servers have identical characteristics except for their service rates. Each server is free to select its own service rate. The objective of each server is to select a service rate that will maximize its own profit. Arrivals are Poisson. The probability that an arriving customer enters the queue is allowed to depend on the queue length at the time of arrival. The proportion of arrivals to a given server is shown to be strictly concave in the server's own service rate and decreasing in the other service rate. Furthermore, we show that when the cost function is convex and increasing, there exists a unique pure strategy Nash equilibrium point for the resulting game.</description><author>Christ, D.; Avi-Itzhak, B.</author><pubDate>Sat, 01 Jun 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Predicting equity liquidity</title><link>http://www.example.com/articles/1</link><description>Breen, W. J.; Hodrick, L. S.; Korajczyk, R. A.
In this paper we develop a measure of liquidity price impact, which quantifies the change in a firm's stock price associated with its observed net trading volume. For a large set of institutional trades we compare out-of-sample, characteristic-based estimates of price impact to actual price impacts. Predictive predetermined firm characteristics, chosen to proxy for the severity of adverse selection in the equity market, the non-information-based costs of making a market in the stock, and the extent of shareholder heterogeneity, include relative size, historical relative trading volume, institutional holdings, and the inverse of the stock price. We find numerous aspects of trade execution which are significantly related to the price impact forecast error in economically plausible ways: For example, the predicted price impact overestimates the actual price impact for very large trades, for trades executed in a more patient manner, and for trades where the institution pays higher commissions.</description><author>Breen, W. J.; Hodrick, L. S.; Korajczyk, R. A.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Corporate governance, takeovers, and top-management compensation: Theory and evidence</title><link>http://www.example.com/articles/1</link><description>Cyert, R. M.; Kang, S. H.; Kumar, P.
We examine, both theoretically and empirically, top-management compensation in the presence of agency conflicts when shareholders have delegated governance responsibilities to a self-interested Board of Directors (BOD). We develop a theoretical framework that explicitly incorporates the BOD as a strategic player, models the negotiation process between the CEO and the BOD in designing CEO compensation, and considers the impact of potential takeovers by large shareholders monitoring the CEO-BOD negotiations. In equilibrium, internal governance by the BOD and external takeover threats by a large shareholder act as substitutes in imposing managerial control, especially in constraining management's profligacy in awarding equity-based compensation to itself. The model emphasizes factors in the design of compensation contracts that are rarely considered in the literature, such as equity ownership of the largest outside shareholder and the firm's bankruptcy risk. It also provides new perspectives on factors that are often considered in the literature, such as firm size, firm performance, equity ownership of the BOD, and BOD structure. Our empirical tests lend considerable support for our theoretical predictions. Equity ownership of the largest external shareholder, that of the BOD, and the default risk, are strongly negatively related to the size of CEO equity compensation. Consistent with the theoretical model, these factors do not significantly influence the growth of fixed (or non-performance-related) compensation. We also find that the equity ownership of the BOD is more important in managerial compensation control than other BOD related variables, such as BOD size or the proportion of outside directors.</description><author>Cyert, R. M.; Kang, S. H.; Kumar, P.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Controlling information systems development projects: The view from the client</title><link>http://www.example.com/articles/1</link><description>Kirsch, L. J.; Sambamurthy, V.; Ko, D. G.; Purvis, R. L.
Increasingly, business clients are actively leading information systems (IS) projects, often in collaboration with IS professionals, and they are exercising a greater degree of project control. Control is defined as all attempts to motivate individuals to achieve desired objectives, and it can be exercised via formal and informal modes. Much of the previous research investigating the choice of control mode has focused on direct reporting relationships between IS project leaders and their superiors in a hierarchical setting. However, the client-IS relationships may take on a variety of forms, including both hierarchical and lateral settings, Moreover, prior research has found that the knowledge of the systems development process is a key antecedent of control, yet clients are unlikely to be as knowledgeable as IS professionals about this process. It is therefore unclear whether prior findings will generalize to the client-IS pair, and the goal of this research is to examine the exercise of control across this relationship. Data were gathered from a questionnaire survey of 69 pairs of clients and IS project leaders. The results are largely consistent with prior research on the antecedents of formal control modes, but they shed new insight on the choice of informal control modes.</description><author>Kirsch, L. J.; Sambamurthy, V.; Ko, D. G.; Purvis, R. L.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Order-based backorders and their implications in multi-item inventory systems</title><link>http://www.example.com/articles/1</link><description>Song, J. S.
In a multi-item inventory system, such as an assemble-to-order manufacturing system or an online-retailing system, a customer order typically consists of several different items in different amounts. The average order-based backorders are the average number of customer orders that are not yet completely filled, While this is an important measure of customer satisfaction, it has not been widely studied in the operations management literature. This is largely because its evaluation involves the joint distribution of inventory levels of different items and other intricate relations, which is computationally dreadful. Taking a novel approach, this paper develops a tractable way of evaluating this measure exactly. We also develop easy-to-compute bounds, which require the evaluation of item-based backorders only. Numerical experiments indicate that the average of the lower and upper bounds is very effective. The exact results show surprisingly simple structures, which shed light on how system parameters affect the performance. Using these results, we study several examples to gain managerial insights. Questions addressed include: What are the implications of item-based inventory planning decisions on the order-based performance? What is the impact of introducing common components on inventory and service trade-offs? Would order-delivery performance be improved if we restrict the number of choices in product configurations?</description><author>Song, J. S.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quantifying operational synergies in a merger/acquisition</title><link>http://www.example.com/articles/1</link><description>Gupta, D.; Gerchak, Y.
Merger and acquisition activity has increased sharply in the last decade. It seems useful to have models that can help senior managers of bidder firms make informed decisions about the amount of premium, over the target's share prices prevailing prior to merger announcement, that can be justified on the basis of operational synergies. The goal of this article is to capture important parameters from the production side that have a bearing on the valuation of the target's shares. We show that the production characteristics of both the bidder and the target matter in a significant way. For example, if the bidder and target operate in independent markets, the bidder has flexible production facilities but the target's production facilities are inflexible, then an increase in the bidder's demand can make the target less attractive and lower the value of operational synergy.</description><author>Gupta, D.; Gerchak, Y.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price dispersion and differentiation in online travel: An empirical investigation</title><link>http://www.example.com/articles/1</link><description>Clemons, E. K.; Hann, I. H.; Hitt, L. M.
Previous research has examined whether price dispersion exists in theoretically highly efficient Internet markets. However, much of the previous work has been focused on industries with low cost and undifferentiated products. In this paper, we examine the presence of price dispersion and product differentiation using data on the airline ticket offerings of online travel agents (OTAs). We find that different OTAs offer tickets with substantially different prices and characteristics when given the same customer request. Some of this variation appears to be due to product differentiation-different OTAs specialize by systematically offering different trade-offs between ticket price and ticket quality (minimizing the number of connections, matching requested departure and return time). However, even after accounting for differences in ticket quality ticket prices vary by as much as 18% across OTAs. In addition, OTAs return tickets that are strictly inferior to the ticket offered by another OTA for the same request between 2.2% and 28% of the time. Overall, this suggests the presence of both price dispersion and product differentiation in the online travel market.</description><author>Clemons, E. K.; Hann, I. H.; Hitt, L. M.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An approximate dynamic programming approach to multidimensional knapsack problems</title><link>http://www.example.com/articles/1</link><description>Bertsimas, D.; Demir, R.
We present an Approximate Dynamic Programming (ADP) approach for the multidimensional knapsack problem (MKP). We approximate the value function (a) using parametric and nonparametric methods and (b) using a base-heuristic. We propose a new heuristic which adaptively rounds the solution of the linear programming relaxation. Our computational study suggests: (a) the new heuristic produces high quality solutions fast and robustly, (b) state of the art commercial packages like CPLEX require significantly larger computational time to achieve the same quality of solutions, (c) the ADP approach using the new heuristic competes successfully with alternative heuristic methods such as genetic algorithms, (d) the ADP approach based on parametric and nonparametric approximations, while producing reasonable solutions, is not competitive. Overall, this research illustrates that the base-heuristic approach is a promising computational approach for MKPs worthy of further investigation.</description><author>Bertsimas, D.; Demir, R.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Adaptive behavior of impatient customers in tele-queues: Theory and empirical support</title><link>http://www.example.com/articles/1</link><description>Zohar, E.; Mandelbaum, A.; Shimkin, N.
We address the modeling and analysis of abandonments from a queue that is invisible to its occupants. Such queues arise in remote service systems, notably the Internet and telephone call centers; hence, we refer to them as tele-queues. A basic premise of this paper is that customers adapt their patience (modeled by an abandonment-time distribution) to their service expectations, in particular to their anticipated waiting time. We present empirical support for that hypothesis, and propose an M/M/m-based model that incorporates adaptive customer behavior. In our model, customer patience depends on the mean waiting time in the queue. We characterize the resulting system equilibrium (namely, the operating point in steady state), and establish its existence and uniqueness when changes in customer patience are bounded by the corresponding changes in their anticipated waiting time. The feasibility of multiple system equilibria is illustrated when this condition is violated. Finally, a dynamic learning model is proposed where customer expectations regarding their waiting time are formed through accumulated experience. We demonstrate, via simulation, convergence to the theoretically anticipated equilibrium, while addressing certain issues related to censored-sampling that arise because of abandonments.</description><author>Zohar, E.; Mandelbaum, A.; Shimkin, N.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Incorporating workflow interference in facility layout design: The quartic assignment problem</title><link>http://www.example.com/articles/1</link><description>Chiang, W. C.; Kouvelis, P.; Urban, T. L.
Although many authors have noted the importance of minimizing workflow interference in facility layout design, traditional layout research tends to focus on minimizing the distance-based transportation cost. This paper formalizes the concept of workflow interference from a facility layout perspective. A model, formulated as a quartic assignment problem, is developed that explicitly considers the interference of workflow. Optimal and heuristic solution methodologies are developed and evaluated.</description><author>Chiang, W. C.; Kouvelis, P.; Urban, T. L.</author><pubDate>Mon, 01 Apr 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investigating them effects of store-brand introduction on retailer demand and pricing behavior</title><link>http://www.example.com/articles/1</link><description>Chintagunta, P. K.; Bonfrer, A.; Song, I.
Researchers have recently been interested in studying the drivers of store-brand success as well as factors that motivate retailers to introduce store brands. In this paper, we study the effects of the introduction of a store-brand into a particular product category. Specifically, we are interested in the effect of store-brand introduction on the demand as well as on the supply side. On the demand side, we investigate the changes in preferences for the national brands and price elasticities in the category. On the supply side, we study the effects of the new entrant on the interactions between the national brand manufacturers and the retailer introducing the store brand, including how these interactions influence the retailer's pricing behavior. In doing so, we are also able to test whether the observed data are consistent with some of the commonly used assumptions regarding retailer pricing behavior. For the demand specification we use a random coefficients logit model that allows for consumer heterogeneity. The model parameters are estimated using aggregate data while explicitly accounting for endogeneity in retail prices. Our empirical results obtained from the oats product category based on store-level data from a multistore retail chain indicate that the store-brand introduction generates notable changes within the category The store-brand introduction coincides with an increase in the retailer's margins for the national brand. We find that the preferences for the national brand are relatively unaffected, by the introduction of the store-brand. While consumers are, in general, more price sensitive (in terms of elasticities) than they were prior to store-brand introduction, a statistical test of the differences in mean price elasticities across stores and between the two regimes fails to reject the hypothesis of no change in these elasticities. Elasticities in specific stores however, do increase after the store brand is introduced. We also find that there is considerable heterogeneity in the preferences for the store-brand. On the supply side, we test several forms of manufacturer-retailer interactions to identify retailer pricing behavior most consistent with the data. Our results indicate that the data reject several, commonly imposed, forms of interactions. In examining the nature of manufacturer interactions with the retailer, we find-that the manufacturer of the national brand appears to take a softer stance in its interactions with the retailer subsequent to store-brand entry. This finding is consistent with academic research and with articles in the popular press which suggest that the store brand enhances the retailer's bargaining ability vis-A-vis the manufacturers of the national brands. We also provide results from a second product category (frozen pasta) that are largely consistent with those found in the oats category.</description><author>Chintagunta, P. K.; Bonfrer, A.; Song, I.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competitive one-to-one promotions</title><link>http://www.example.com/articles/1</link><description>Shaffer, G.; Zhang, Z. J.
One-to-one promotions are possible when consumers are individually addressable and firms know something about each customer's preferences. We explore the competitive effects of one-to-one promotions in a model with two competing firms where the firms differ in size and consumers have heterogeneous brand loyalty. We find that one-to-one promotions always lead to an increase in price competition (average prices in the market decrease). However, we also find that one-to-one promotions affect market shares. This market-share effect may outweigh the effect of lower prices, benefiting the firm whose market share increases. Our results suggest that of two firms, the firm with the higher-quality product may gain from one-to-one promotions. Our model also has implications for the phenomenon of customer churn, where consumers switch to a less preferred brand due to targeted promotional incentives. We show that churning can arise optimally from firms pursuing a profit-maximizing strategy. Instead of trying to minimize it, the optimal way to manage customer churn is to engage in both offensive and defensive promotions with the relative mix depending on the marginal cost of targeting.</description><author>Shaffer, G.; Zhang, Z. J.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Easy quantification of improved spare parts inventory policies</title><link>http://www.example.com/articles/1</link><description>Thonemann, U. W.; Brown, A. O.; Hausman, W. H.
This paper presents approximate analytical models to quantify the expected improvement in inventory investment when using a system approach to control inventory as opposed to a simpler item approach. A system approach ensures that a demand-weighted average fill rate is achieved at low inventory investment by assigning low fill rates to parts with high costs and high fill rates to parts with low costs. An item approach does not vary fill rates by parts but assigns identical fill rates to all parts. Using single-parameter functional representations of the skewness of unit costs and average demand across all parts in the system, simple approximate analytical expressions for the required inventory investment are derived for both approaches. The accuracy of the approximations is validated using data from a distribution center for computer spare parts. For these data, the solutions obtained by the approximations are very close to the exact values. The results show that inventory investments can be well approximated as a function of only a few cost and demand parameters. These expressions can be used to determine the percentage reduction in inventory investment for a particular target demand-weighted average fill rate when the superior system approach is used instead of the item approach. For increased ease of use, the percentage reduction in inventory when using a system as opposed to an item approach is computed over a range of realistic values for the key parameters of the model and a quadratic expression is fitted to the data. This fitted expression provides rough guidelines for the anticipated improvement with very limited data needed, prior to detailed modeling or implementation.</description><author>Thonemann, U. W.; Brown, A. O.; Hausman, W. H.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information sharing in a supply chain with horizontal competition</title><link>http://www.example.com/articles/1</link><description>Li, L. D.
This paper examines the incentives for firms to share information vertically in a two-level supply chain in which there are an upstream firm (a manufacturer) and many downstream firms (retailers). The retailers are engaged in a Cournot competition and are endowed with some private information. Vertical information sharing has two effects: "direct effect" due to the changes in strategy by the parties involved in sharing the information and "indirect effect" (or "leakage effect") due to the changes in strategy by other competing firms (who may infer the information from the actions of the informed parties). Both changes would affect the profitability of the firms. We show that the leakage effect discourages the retailers from sharing their demand information with the manufacturer while encouraging them to share their cost information. On the other hand, the direct effect always discourages the retailers from sharing their information. When voluntary information sharing is not possible, we identify conditions under which information can be traded and show how price should be determined to facilitate such information exchange. We also examine the impact of vertical information sharing on the total supply chain profits and social benefits.</description><author>Li, L. D.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Deconstructing the pioneer's advantage: Examining vintage effects and consumer valuations of quality and variety</title><link>http://www.example.com/articles/1</link><description>Bohlmann, J. D.; Golder, P. N.; Mitra, D.
Several studies have demonstrated an order-of-entry effect on market share, suggesting that pioneers outperform later entrants. However, other research has pointed out the limitations of these studies and found evidence that many pioneers fail or have low market share. Given this background, the purpose of this research is to understand the conditions under which pioneers are more likely and also less likely to have an advantage. We propose a game-theoretic model that includes important sources of pioneer advantages as well as disadvantages. Specifically, we incorporate a pioneer advantage due to preemption in markets with heterogeneous tastes. In addition, we incorporate a potential pioneer disadvantage due to technology vintage effects, where later entrants utilizing improved technology can have lower costs and higher quality. The model allows us to evaluate the extent of vintage effects necessary to overcome a pioneer's advantage. Key relationships are found between the magnitude of the pioneer advantage or disadvantage and consumer valuations of product attributes (e.g., variety and quality). We empirically validate the model with vintage effect data in 36 product categories, and measures of consumer valuations of product variety and quality for 12 of these 36 categories. The results show that pioneers do better in product categories where variety is more important and worse in categories where product quality is more important. Pioneers in categories with high vintage effects are shown to have lower market shares and higher failure rates. Similar results appear when analyzing persistence of market leadership over time, further validating our model's major implications. We also present two case studies that illustrate key elements of the model.</description><author>Bohlmann, J. D.; Golder, P. N.; Mitra, D.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Should start-up companies be cautious? Inventory policies which maximise survival probabilities</title><link>http://www.example.com/articles/1</link><description>Archibald, T. W.; Thomas, L. C.; Betts, J. M.; Johnston, R. B.
New start-up companies, which are considered to be a vital ingredient in a successful economy, have a different objective than established companies: They want to maximise their chance of long-term survival. We examine the implications for their operating decisions of this different criterion by considering an abstraction of the inventory problem faced by a start-up manufacturing company. The problem is modelled under two criteria as a Markov decision process; the characteristics of the optimal policies under the two criteria are compared. It is shown that although the start-up company should be more conservative in its component purchasing strategy than if it were a well-established company, it should not be too conservative. Nor is its strategy monotone in the amount of capital it has available. The models are extended to allow for interest on investment and inflation.</description><author>Archibald, T. W.; Thomas, L. C.; Betts, J. M.; Johnston, R. B.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing of information products on online servers: Issues, models, and analysis</title><link>http://www.example.com/articles/1</link><description>Jain, S.; Kannan, P. K.
Online information servers that provide access to diverse databases where users can search for, browse through, and download the information they need have been rapidly increasing in number in the past few years. Online vendors have traditionally charged users for information on the based on the length of the time they were connected to the databases. With hardware and software advances, many online servers have recently started changing their pricing strategies to search-based and/or subscription-fee pricing. This paper examines the various issues involved in pricing these information products, and presents an economic approach to analyze conditions under which the various pricing schemes may prove optimal for the online servers. Our results show that the variation in consumer expertise and valuation of information affects the choice of a pricing strategy by the server. We present general conditions under which subscription-fee pricing is optimal even when consumer demand is inelastic. We also find that, given the cost structures characterizing the market, undifferentiated online servers can compete and coexist in the market each making positive profits. We show that in a competitive setting an increase in costs of online servers can sometimes benefit them by enabling them to differentiate themselves. Our results offer insights into the trends in pricing strategies and may provide an explanation as to why many servers may persist with connect-time strategies.</description><author>Jain, S.; Kannan, P. K.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A structural approach to assessing innovation: Construct development of innovation locus, type, and characteristics</title><link>http://www.example.com/articles/1</link><description>Gatignon, H.; Tushman, M. L.; Smith, W.; Anderson, P.
We take a structural approach to assessing innovation. We develop a comprehensive set of measures to assess an innovation's locus, type, and characteristics. We find that the concepts of competence destroying and competence enhancing are composed of two distinct constructs that, although correlated, separately characterize an innovation: new competence acquisition and competence enhancement/destruction. We develop scales to measure these constructs and show that new competence acquisition and competence enhancing/destroying are different from other innovation characteristics including core/peripheral and incremental/radical, as well as architectural and generational innovation types. We show that innovations can be evaluated distinctively on these various dimensions with generally small correlations between them. We estimate the impact these different innovation characteristics and types have on time to introduction and perceived commercial success. Our results indicate the importance of taking a structural approach to describing innovations and to the differential importance of innovation locus, type, and characteristics on innovation outcomes. Our results also raise intriguing questions regarding the locus of competence acquisition (internal vs. external) and both innovation outcomes.</description><author>Gatignon, H.; Tushman, M. L.; Smith, W.; Anderson, P.</author><pubDate>Sun, 01 Sep 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sharing the wealth: When should firms treat customers as partners?</title><link>http://www.example.com/articles/1</link><description>Anderson, E. T.
Marketers often stress the importance of treating customers as partners. A fundamental premise of this perspective is that all parties can be weakly better off if they work together to increase joint surplus and reach Pareto-efficient agreements. For marketing managers, this implies organizing marketing activities in a manner that maximizes total surplus. This logic is theoretically sound when agreements between partners are limitless and costless. In most consumer marketing contexts (business-to-consumer), this is typically not true. The question I ask is should one still expect firms to partner with consumers and reach Pareto-efficient agreements? In this paper, I use the example of a firm's choice of product configuration to demonstrate two effects. First, I show that a firm may configure a product in a manner that reduces total surplus but increases firm profits. Second, one might conjecture that increased competition would eliminate this effect, but I show that in a duopoly firm profits may be increasing in the cost of product completion. This second result suggests that firms may prefer to remain inefficient and/or stifle innovations. Both results violate a fundamental premise of partnering-that firms and consumers should work together to increase total surplus and reach Pareto-efficient agreements. The model illustrates that Pareto-efficient agreements are less likely to occur if negotiation with individual partners is infeasible or costly, such as in business-to-consumer contexts. Consumer marketers in one-to-many marketing environments should be wary of treating customers as partners because Pareto-efficient agreements may not be optimal for their firm.</description><author>Anderson, E. T.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>David vs. Goliath: An analysis of asymmetric mixed-strategy games and experimental evidence</title><link>http://www.example.com/articles/1</link><description>Amaldoss, W.; Jain, S.
Mixed strategies are widely used to model strategic situations in diverse fields such as economics, marketing, political science, and biology. However, some of the implications of asymmetric mixed-strategy solutions are counterintuitive. We develop a stylized model of patent race to examine some of these implications. In our model two firms compete to develop a product and obtain a patent. However, one firm values the patent more because of its market advantages, such as brand reputation and distribution network. Contrary to some intuition, we find that the firm that values the patent less is likely to invest more aggressively in developing the product and will also win the patent more often. We argue that the reason for these counterintuitive results is inherent in the very concept of mixed strategy solution. In a laboratory test, we examine whether subjects' behavior conforms to the equilibrium predictions. We find that the aggregate behavior of our subjects is consistent with the game-theoretic predictions. With the help of the experience-weighted attraction (EWA) learning model proposed by Camerer and Ho (1999), we show that adaptive learning can account for the investment behavior of our subjects. We find that the EWA learning model tracks the investment decisions of our subjects well, whether we hold out trials or an entire group of subjects.</description><author>Amaldoss, W.; Jain, S.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Augmenting conjoint analysis to estimate consumer reservation price</title><link>http://www.example.com/articles/1</link><description>Jedidi, K.; Zhang, Z. J.
Consumer reservation price is a key concept in marketing and economics. Theoretically, this concept has been instrumental in studying consumer purchase decisions, competitive pricing strategies, and welfare economics. Managerially, knowledge of consumer reservation prices is critical for implementing many pricing tactics such as bundling, target promotions, nonlinear pricing, and one-to-one pricing, and for assessing the impact of marketing strategy on demand. Despite the practical and theoretical importance of this concept, its measurement at the individual level in a practical setting proves elusive. We propose a conjoint-based approach to estimate consumer-level reservation prices. This approach integrates the preference estimation of traditional conjoint with the economic theory of consumer choice. This integration augments the capability of traditional conjoint such that consumers' reservation prices for a product can be derived directly from the individual-level estimates of conjoint coefficients. With this augmentation, we can model a consumer's decision of not only which product to buy, but also whether to buy at all in a category. Thus, we can simulate simultaneously three effects that a change in price or the introduction of a new product may generate in a market: the customer switching effect, the cannibalization effect, and the market expansion effect. We show in a pilot application how this approach can aid product and pricing decisions. We also demonstrate the predictive validity of our approach using data from a commercial study of automobile batteries.</description><author>Jedidi, K.; Zhang, Z. J.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Prospect theory: Much ado about nothing?</title><link>http://www.example.com/articles/1</link><description>Levy, M.; Levy, H.
Prospet theory is a paradigm challenging the expected utility paradigm. One of the fundamental components of prospect theory is the S-shaped value function. The value function is mainly justified by experimental investigation of the certainty equivalents of prospects confined either to the negative or to the positive domain, but not of mixed prospects, which characterize most actual investments. We conduct an experimental study with mixed prospects, using, for the first time, recently developed investment criteria called Prospect Stochastic Dominance (PSD) and Markowitz Stochastic Dominance (MSD). We reject the S-shaped value function, showing that at least 62%-76% of the subjects cannot be characterized by such preferences. We find support for the Markowitz utility function, which is a reversed S-shaped function-exactly the opposite of the prospect theory value function. It is possible that the previous results supporting the S-shaped value function are distorted because the prospects had only positive or only negative outcomes, presenting hypothetical situations which individuals do not usually face, and which are certainly not common in financial markets.</description><author>Levy, M.; Levy, H.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competition and outsourcing with scale economies</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.; Harker, P. T.
Scale economies are commonplace in operations, yet because of analytical challenges, relatively little is, known about how firms should compete in their presence. This paper presents a model of competition between two firms that face scale economies; (i.e., each firm's cost per unit of demand is decreasing in demand). A general framework is used, which incorporates competition between two service providers with price- and time-sensitive demand (a queuing game), And competition between two retailers with fixed-ordering costs and price-sensitive consumers (an Economic Order Quantity game). Reasonably general conditions are provided under which there exists at most one equilibrium, with both firms participating in the market. We demonstrate, in the context of the queuing game, that the lower cost firm in equilibrium may have a higher market share and a higher price, an enviable situation. We also allow each firm to outsource their production process to a supplier. Even if the supplier's technology is no better, than the firms' technology and,the supplier is required to establish dedicated capacity (so the suppliers; scale can be no greater than either firm's scale), we show that the firms strictly prefer to outsource. We conclude that scale economies provide a strong motivation for outsourcing that has not previously been identified in the literature.</description><author>Cachon, G. P.; Harker, P. T.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic and operational benefits of electronic integration in B2B procurement processes</title><link>http://www.example.com/articles/1</link><description>Mukhopadhyay, T.; Kekre, S.
Our goal is to assess the strategic and, operational benefits of electronic integration for industrial procurement. We conduct a field study with an industrial supplier and examine the drivers of performance of the procurement process. Our research quantifies both the operational and strategic impacts of electronic integration in a B2B procurement environment for a supplier. Additionally, we show that the customer also obtains substantial benefits from efficient procurement transaction processing. We isolate the performance impact of technology choice and ordering processes on both the trading partners. A significant finding is that the supplier derives large strategic benefits when the customer initiates the system-and the supplier enhances the system's capabilities. With respect to operational benefits, we find that when suppliers have advanced electronic linkages, the order-processing system significantly increases benefits to both parties.</description><author>Mukhopadhyay, T.; Kekre, S.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Aspiration-level adaptation in an American financial services organization: A field study</title><link>http://www.example.com/articles/1</link><description>Mezias, S. J.; Chen, Y. R.; Murphy, P. R.
Using field data from an American financial services organization, we examined the effects of three important variables in Cyert and March's (1963) initial conceptualization. of the aspiration-level adaptation process: The previous aspiration level, performance feedback, and social comparison. Past findings obtained in controlled contexts (Glynn et al. 1991; Lant 1992) have provided empirical support for the attainment discrepancy model (Lewin et al. 1944), which includes variables of the previous aspiration level and attainment discrepancy (i.e., performance feedback). We replicated these findings in the field: The effects of the previous aspiration level and attainment discrepancy on the current aspiration levels were significant and positive. In addition, we investigated the effect of social comparison using a variable based on the difference between the performance of the focal unit and the performance of comparable others (Greve 1998). Based on the assumption that decision makers. in organizations will expect to observe similar performance levels among those in the same comparison group (Wood 1989), we posited that the effect of social comparison would be negative, reflecting managerial efforts to reduce performance discrepancies among similar units. The empirical results supported the prediction from this reasoning. We conclude by discussing implications of our findings for theory and research in organizational learning and the behavioral theory of the firm.</description><author>Mezias, S. J.; Chen, Y. R.; Murphy, P. R.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>New product innovation with multiple features and technology constraints</title><link>http://www.example.com/articles/1</link><description>Gjerde, K. A. P.; Slotnick, S. A.; Sobel, M. J.
We model a firm's decisions about product innovation, focusing on the extent to which features should be improved or changed in the succession of models that comprise a life cycle. We show that the structure of the internal and external environment in which a firm operates suggests when to innovate to the technology frontier. The criterion is maximization of the expected present value of profits during the life cycle. Computational studies complement the theoretical results and lead to insights about when to bundle innovations across features. The formalization was influenced by extensive interviews with managers in a high-technology firm that dominates its industry.</description><author>Gjerde, K. A. P.; Slotnick, S. A.; Sobel, M. J.</author><pubDate>Tue, 01 Oct 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain coordination under channel rebates with sales effort effects</title><link>http://www.example.com/articles/1</link><description>Taylor, T. A.
A channel rebate is a payment from a manufacturer to a retailer based on retailer sales to end consumers. Two common forms of channel rebates are linear rebates, in which the rebate is paid for each unit sold, and target rebates, in which the rebate is paid for each unit sold beyond a specified target level. When demand is not influenced by sales effort, a properly designed target rebate achieves channel coordination and a win-win outcome. Coordination cannot be achieved by a linear rebate in a way that is implementable. When demand is influenced by retailer sales effort, a properly designed target rebate and returns contract achieves coordination and a win-win outcome. Other contracts, such as linear rebate and returns or target rebate alone, cannot achieve coordination in a way that is implementable. Contrary to the view expressed in the literature that accepting returns weakens incentives for retailer sales effort, we find that the provision of returns strengthens incentives for effort.</description><author>Taylor, T. A.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On uncertainty, ambiguity, and complexity in project management</title><link>http://www.example.com/articles/1</link><description>Pich, M. T.; Loch, C. H.; De Meyer, A.
This article develops a model of a project as a payoff function that depends on the state of the world and the choice of a sequence of actions. A causal mapping, which may be incompletely known by the project team, represents the impact of possible actions on the states of the world. An underlying probability space represents available information about the state of the world. Interactions among actions and states of the world determine the complexity of the payoff function. Activities are endogenous, in that they are the result of a policy that maximizes the expected project payoff. A key concept is the adequacy of the available information about states of the world and action effects. We express uncertainty, ambiguity and complexity in terms of information adequacy. We identify three fundamental project management strategies: instructionism, learning, and selectionism. We show that classic project management methods emphasize adequate information and instructionism, and demonstrate how modern methods fit into the three fundamental strategies. The appropriate strategy is contingent on the type of uncertainty present and the complexity of the project payoff function. Our model establishes a rigorous language that allows the project manager to judge the adequacy of the available project information at the outset, choose an appropriate combination of strategies, and set a supporting project infrastructure-that is, systems for planning, coordination and incentives, and monitoring.</description><author>Pich, M. T.; Loch, C. H.; De Meyer, A.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The market evolution and sales takeoff of product innovations</title><link>http://www.example.com/articles/1</link><description>Agarwal, R.; Bayus, B. L.
In contrast to the prevailing supply-side explanation that price decreases are the key driver of a sales takeoff, we argue that outward shifting supply and demand curves lead to market takeoff. Our fundamental idea is that sales in new markets are initially low because the first commercialized forms of new innovations are primitive. Then, as new firms enter, actual and perceived product quality improves (and prices possibly drop), which leads to a takeoff in sales. To provide empirical evidence for this explanation, we explore the relationship between takeoff times, price decreases, and firm entry for a sample of consumer and industrial product innovations commercialized in the United States over the past 150 years. Based on a proportional hazards analysis of takeoff times, we find that new firm entry dominates other factors in explaining observed sales takeoff times. We interpret these results as supporting the idea that demand shifts during the early evolution of a new market due to nonprice factors is the key driver of a sales takeoff.</description><author>Agarwal, R.; Bayus, B. L.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Performance assessment of the lead user idea-generation process for new product development</title><link>http://www.example.com/articles/1</link><description>Lilien, G. L.; Morrison, P. D.; Searls, K.; Sonnack, M.; von Hippel, E.
Traditional idea generation techniques based on customer input usually collect information on new product needs from a random or typical set of customers. The "lead user process" takes a different approach. It collects information about both needs and solutions from users at the leading edges of the target market, as well as from users in other markets that face similar problems in a more extreme form. This paper reports on a natural experiment conducted within the 3M Company on the effect of the lead user (LU) idea-gene ration process relative to more traditional methods. 3M is known for its innovation capabilities-and we find that the LU process appears to improve upon those capabilities. Annual sales of LU product ideas generated by the average LU project at 3M are conservatively projected to be $146 million after five years-more than eight times higher than forecast sales for the average contemporaneously conducted "traditional" project. Each funded LU project is projected to create a new major product line for a 3M division. As a direct result, divisions funding LU project ideas are projecting their highest rate of major product line generation in the past 50 years.</description><author>Lilien, G. L.; Morrison, P. D.; Searls, K.; Sonnack, M.; von Hippel, E.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An evaluation of research on Integrated Product Development</title><link>http://www.example.com/articles/1</link><description>Gerwin, D.; Barrowman, N. J.
Integrated Product Development (IPD) creates overlap and interaction between activities in the new product development process and, because this increases the need to coordinate, compensates through other aspects of the new product development process (e.g., integrated tools), product definitions (e.g., incremental development), organizational context (e.g., reduced task specialization), and teaming (e.g., cross-functional teams). Since IPD has become an important new standard for managing new product development, this paper's general aim is to evaluate the research that has been conducted on it. Our three specific objectives include first critiquing the IPD literature by identifying problems with empirical research and recommending solutions. There are concerns about the overall approach, conceptualizing and operationalizing IPD characteristics, and selecting performance objectives. Second, we conduct a meta-analysis to evaluate relationships between specific IPD characteristics and project performance. We indicate where relationships do or do not exist and identify variables that may moderate these relationships. Third, we offer suggestions for extending IPD research into studies of (a) the hierarchy of teams working on a project, (b) one company managing a portfolio of projects over time, and (c) two or more firms collaborating in a strategic alliance.</description><author>Gerwin, D.; Barrowman, N. J.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The valuation of American options for a class of diffusion processes</title><link>http://www.example.com/articles/1</link><description>Detemple, J.; Tian, W. D.
W e present an integral equation approach for the valuation of American-style derivatives when the underlying asset price follows a general diffusion process and the interest rate is stochastic. Our contribution is fourfold. First, we show that the exercise region is determined by a single exercise boundary under very general conditions on the interest rate and the dividend yield. Second, based on this result, We derive a recursive integral equation for the exercise boundary and provide a parametric representation of the American option price: Third, we apply the results to models with stochastic volatility or stochastk interest rate, and to American bond options in one-factor models: For the cases studied, explicit parametric valuation formulas are obtained. Finally, we extend esults on American capped options to general diffusion prices. Numerical schemes based on approximations of the optimal stopping time (such as approximations based on a lower bound, or on a combination of lower and upper bounds) are shown to be valid in this context.</description><author>Detemple, J.; Tian, W. D.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managerial insights into the effects of interactions on replacing members of a team</title><link>http://www.example.com/articles/1</link><description>Solow, D.; Vairaktarakis, G.; Piderit, S. K.; Tsai, M. C.
A mathematical model is presented for studying the effects of interactions among team members on the process of replacing members of a team in an organization. The model provides the ability to control the number of members that interact with each individual on the team. Through the use of analysis and computer simulations, it is shown how the amount of interaction affects the tradeoff between the expected performance and the number of replacements and interviews needed to find a good team using various replacement policies. New managerial insights into this process-such as the fact that it is not necessarily optimal to replace the worst-performing team member-are provided.</description><author>Solow, D.; Vairaktarakis, G.; Piderit, S. K.; Tsai, M. C.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Preferred by "all" and preferred by "most" decision makers: Almost stochastic dominance</title><link>http://www.example.com/articles/1</link><description>Leshno, M.; Levy, H.
While "most" decision makers may prefer one uncertain prospect over another, stochastic dominance rules as well as other investment criteria, will not reveal this preference due to some extreme utility functions in the case of even a very small violation of these rules. Such strict rules relate to "all" utility functions in a given class including extreme ones which presumably rarely represents investors' preference. In this paper we establish almost stochastic dominance (ASD) rules which formally reveal a preference for "most" decision makers, but not for "all" of them. The ASD rules reveal that choices which probably conform with "most" decision makers also solve some debates, e.g., showing, as practitioners claim, an ASD preference for a higher proportion of stocks in the portfolio as the investment horizon increases, a conclusion which is not implied by the well-known stochastic dominance rules.</description><author>Leshno, M.; Levy, H.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A jump-diffusion model for option pricing</title><link>http://www.example.com/articles/1</link><description>Kou, S. G.
Brownian motion and normal distribution have been widely used in the Black-Scholes option-pricing framework to model the return of assets. However, two puzzles emerge from many empirical investigations: the leptokurtic feature that the return distribution of assets may have a higher peak and two (asymmetric) heavier tails than those of the normal distribution, and an empirical phenomenon called "volatility smile" in option markets. To incorporate both of them and to strike a balance between reality and tractability, this paper proposes, for the purpose of option pricing, a double exponential jump-diffusion model. In particular, the model is simple enough to produce analytical solutions for a variety of option-pricing problems, including call and put options, interest rate derivatives, and path-dependent options. Equilibrium analysis and a psychological interpretation of the model are also presented.</description><author>Kou, S. G.</author><pubDate>Thu, 01 Aug 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Misperceiving interactions among complements and substitutes: Organizational consequences</title><link>http://www.example.com/articles/1</link><description>Siggelkow, N.
Systems composed of activity choices that interact in nonsimple ways can allow firms to create and sustain a competitive advantage. However, in complex systems, decision makers may not always have a precise understanding of the exact strength of the interaction between activities. Likewise, incentive and accounting systems may lead decision makers to ignore or misperceive interactions. This paper studies formally the consequences of misperceiving interaction effects between activity choices. Our results suggest that misperceptions with respect to complements are more costly than with respect to substitutes. As a result, firms should optimally invest more to gather information about interactions among complementary activities-e.g., concerning network effects-than about interactions among substitute activities. Similarly, the use of division-based incentive schemes appears to be more advisable for divisions whose products are substitutes than for divisions that produce complements. It is further shown that system fragility is not necessarily positively correlated with the strength of the interaction between choices. While systems of complements become increasingly fragile as the strength of interaction increases, systems of substitutes can become increasingly stable.</description><author>Siggelkow, N.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Shifting innovation to users via toolkits</title><link>http://www.example.com/articles/1</link><description>von Hippel, E.; Katz, R.
In the traditional new product development process, manufacturers first explore user needs and then develop responsive products. Developing an accurate understanding of a user need is not simple or fast or cheap, however. As a result, the traditional approach is coming under increasing strain as user needs change more rapidly, and as firms increasingly seek, to serve "markets of one." Toolkits for user's innovation is an emerging alternative approach in which manufacturers actually abandon the attempt to understand user needs in detail in favor of transferring, need-related aspects of product and service development to users. Experience in fields where. the toolkit approach has been pioneered show custom products being developed much more quickly and at a lower cost. In this paper we explore toolkits for user innovation and explain why and how they work.</description><author>von Hippel, E.; Katz, R.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>What do we know about variance in accounting profitability?</title><link>http://www.example.com/articles/1</link><description>McGahan, A. M.; Porter, M. E.
In this paper, we analyze the variance of accounting profitability among a broad cross-section of firms in the American economy from 1981 to 1994. The purpose of the analysis is to identify the importance of year, industry, corporate-parent, and business-specific effects on accounting profitability among operating businesses across sectors. The findings indicate that industry and corporate-parent effects are important and related to one another. As expected, business-specific effects, which arise from competitive positioning and other factors, have a large influence on performance. The analysis reconciles the results of previous studies by exploring differences in method and data. We also identify the broad contributions and limitations of the research, and suggest avenues for further study. New approaches are necessary to generate significant insights about the relationships between industry, corporate-parent, and business influences on firm profitability.</description><author>McGahan, A. M.; Porter, M. E.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organization design</title><link>http://www.example.com/articles/1</link><description>Harris, M.; Raviv, A.
This paper attempts to explain organization structure based on optimal coordination of interactions among activities. The main idea is that each manager is capable of detecting and coordinating interactions only within his limited area of expertise. Only the CEO can coordinate companywide interactions. The optimal design of the organization trades off the costs and benefits of various configurations of managers. Our results consist of classifying the characteristics of activities and managerial costs that lead to the matrix organization, the functional hierarchy, the divisional hierarchy, or a flat hierarchy. We also investigate the effect of changing the costs of various managers on the nature of the optimal organization, including the extent of centralization.</description><author>Harris, M.; Raviv, A.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The performance effects of congruence between product competitive strategies and purchasing management design</title><link>http://www.example.com/articles/1</link><description>David, J. S.; Hwang, Y. C.; Pei, B. K. W.; Reneau, J. H.
The objective of this study is to examine a performance contingency effect between product competitive strategy and organization design using an archival approach. Specifically, this study examines a sample of 194 firms from 20 industries based on the data collected by Center for Advanced Purchasing Studies (CAPS) in its benchmarking surveys between 1989-1994 and links the benchmarking data to the COMPUSTAT (Standard &amp; Poor's) financial data of these firms. The results of the study reveal a contingency relationship among product competitive strategies, purchasing design characteristics, and overall firm financial performance (return on assets). Specifically, the nature of this contingency relationship suggests that a firm's product competitive strategy must be enabled with a complementary design in purchasing management to promote firm performance. Given the growing practice of benchmarking at the functional level, this study also examines whether or not a firm achieving a congruency in product strategy and design will necessarily enjoy higher operational efficiency at the purchasing management level. The results show that this is true only under specific conditions. The implications of the preceding findings are discussed accordingly.</description><author>David, J. S.; Hwang, Y. C.; Pei, B. K. W.; Reneau, J. H.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Project assignment rights and incentives for eliciting ideas</title><link>http://www.example.com/articles/1</link><description>Arya, A.; Glover, J.; Routledge, B. R.
In this paper, we study an incentive problem that arises between a principal and two agents because they value a real option differently. The real option in our model is a timing option. The agents have limited capacity to undertake projects, and each agent's capacity can be filled now or later. Because the principal cares about capacity in the aggregate but each agent cares only about his own capacity, the agents assign a higher value to the option to wait. As a result, agents sometimes withhold ideas from the principal. We show that decentralization can be a solution to this problem. Delegating assignment rights to an agent reduces the option value of waiting for the other agent sufficiently that he is willing to reveal his ideas.</description><author>Arya, A.; Glover, J.; Routledge, B. R.</author><pubDate>Mon, 01 Jul 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An improved batch means procedure for simulation output analysis</title><link>http://www.example.com/articles/1</link><description>Steiger, N. M.; Wilson, J. R.
We formulate and evaluate the Automated Simulation Analysis Procedure (ASAP), an algorithm for steady-state simulation output analysis based on the method of nonoverlapping batch means (NOBM). ASAP delivers a confidence interval for an expected response that is centered on the sample mean of a portion of a simulation-generated time series and satisfies a user-specified absolute or relative precision requirement. ASAP operates as follows: The batch size is progressively increased until either (a) the batch means pass the von Neumann test for independence, and then ASAP delivers a classical NOBM confidence interval; or (b) the batch means pass the Shapiro-Wilk test for multivariate normality, and then ASAP delivers a correlation-adjusted confidence interval. The latter adjustment is based on an inverted Cornish-Fisher expansion for the classical NOBM t-ratio, where the terms of the expansion are estimated via an autoregressive-moving average time series model of the batch means. After determining the batch size and confidence-interval type, ASAP sequentially increases the number of batches until the precision requirement is satisfied. An extensive experimental study demonstrates the performance improvements achieved by ASAP versus well-known batch means procedures, especially in confidence-interval coverage probability.</description><author>Steiger, N. M.; Wilson, J. R.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge seeking and location choice of foreign direct investment in the United States</title><link>http://www.example.com/articles/1</link><description>Chung, W.; Alcacer, J.
To what extent do firms go abroad to access technology available in other locations? This paper examines whether and when state technical capabilities attract foreign investment in manufacturing from. 1987-1993. We find that on average state R&amp;D intensity does not attract foreign direct investment. Most investing firms are in lower-tech industries and locate in low R&amp;D intensity states, suggesting little interest in state technical capabilities. In contrast, we find that firms in research-intensive industries are more likely to locate in states with high R&amp;D intensity. Foreign firms in the pharmaceutical industry value state R&amp;D intensity the most, at a level twice that of firms in the semiconductor industry, and four times that of electronics firms. Interestingly, not only firms from technically lagging nations, but also some firms from technically leading nations are attracted to R&amp;D intensive states. This suggests that beyond catching up, firms use knowledge-seeking investments also to source technical diversity.</description><author>Chung, W.; Alcacer, J.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The transfer of experience in groups of organizations: Implications for performance and competition</title><link>http://www.example.com/articles/1</link><description>Ingram, P.; Simons, T.
Groups of organizations are pervasive, although there is little systematic knowledge about how they affect their members. We examine one dimension of the operation of organization groups, the transfer of experience. Our Fore,argument is that organization groups may create benefits for their members, but problems-for those outside the group. Within the group they can facilitate the transfer of experience among their members by creating mechanisms for communication, incentives for helping, and by promoting, understanding. The predicted pattern of experience transfer should improve performance of those within the group, but also has implications for those outside it. Experience accumulated, in one organization group strengthens the competitiveness of its organizations, and thereby harms competitors outside the group. Thus, organization groups are fundamental both for the functioning of their members and the competitive dynamics of their industries. Our longitudinal analysis of the profitability of kibbutz agriculture supports both these claims. Between 1954 and 1965 (the years of this study), almost all kibbutzim were part of organization groups. Kibbutzim became more profitable as a function of the experience of others in their group was reduced, however, as a function of experience of others outside. their Their profitability group.</description><author>Ingram, P.; Simons, T.</author><pubDate>Sun, 01 Dec 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technology selection and commitment in new product development: The role of uncertainty and design flexibility</title><link>http://www.example.com/articles/1</link><description>Krishnan, V.; Bhattacharya, S.
Selecting the right technologies to incorporate in new products is a particularly challenging aspect of new product definition and development. While newer advanced technologies may offer improved performance, they also make the product development process more risky and challenging. In this paper, we focus on the problem of technology selection and commitment under uncertainty, a major challenge to firms in turbulent environments. We argue that the ''pizza-bin'' approach of rejecting prospective technologies outright may not serve firms well when the pressure to differentiate products is enormous. After motivating the challenges and decisions facing firms using a real-life application from Dell Computer Corporation, we formulate a mathematical model of a firm that must define its products in the presence of technology uncertainty. Specifically, the firm faces two options: (i) a proven technology that is known to be viable and (ii) a prospective technology that offers superior price to performance results but whose viability is not a fully certain outcome. To minimize the impact of technology uncertainty, we consider two approaches to design flexibility termed parallel path and sufficient design, which allow the firm to concurrently develop its products while the technology is being validated. Our analysis helps understand appropriateness of the different flexible design approaches. We illustrate our model with the Dell portable computer example and note the managerial implications of our analysis.</description><author>Krishnan, V.; Bhattacharya, S.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Aggregate social discount rate derived from individual discount rates</title><link>http://www.example.com/articles/1</link><description>Reinschmidt, K. F.
In the economic evaluation of large public-sector projects, an aggregate social discount rate may be used in present worth comparison of alternatives. This paper uses the assumptions that individual discount rates are constant over time and approximately Normally distributed across the affected population, with mean mu and variance sigma(2), to derive an aggregate discount function that is exponential in form but with time-dependent aggregate discount rate p(t) = mu - sigma(2)t/2, where t is the time of occurrence of the cost or benefit. This equation agrees with numerical simulations. If sigma(2) &gt; 0, then the aggregate discount rate is less than the mean individual discount rate, and use of the time-dependent aggregate discount rate p(t) = mu - sigma(2)t/2 instead of the constant discount factor p(t) = mu would result in larger discounted present values for public-sector projects for which the benefits lie far in the future. This could mean that public-sector investments that would be rejected under the assumption p(t) = mu might be justified using the time-dependent aggregate discount rate p(t) = mu - sigma(2)t/2.</description><author>Reinschmidt, K. F.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The inventory benefit of shipment coordination and stock rebalancing in a supply chain</title><link>http://www.example.com/articles/1</link><description>Cheung, K. L.; Lee, H. L.
In this paper, we examine two information-based supply-chain efforts that are often linked to Vendor-Managed Inventory (VMI) programs. Specifically, we consider a supplier serving Multiple retailers located in a close proximity. The first effort uses information on the retailers' inventory positions to coordinate shipments from the supplier to enjoy economies of scale in shipments, such as full truckloads. The second effort uses the same information for eventual unloading of the shipments to the retailers to rebalance their stocking positions. How much benefit do we gain from such initiatives? What are the relative benefits of the two initiatives? What are the drivers of such benefits? This paper seeks answers to these questions.</description><author>Cheung, K. L.; Lee, H. L.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploring the locus of profitable pollution reduction</title><link>http://www.example.com/articles/1</link><description>King, A.; Lenox, M.
In this paper, we explore the locus of profitable pollution reduction. We propose that managers underestimate the full value of some means of pollution reduction and so under exploit these means. Based on evidence from previous studies, we argue that waste prevention often provides unexpected innovation offsets, and that onsite waste treatment often provides unexpected cost. We use statistical methods to test the direction and significance of the relationship between the various means of pollution reduction and profitability. We find strong evidence that waste prevention leads to financial gain, but we find no evidence that firms profit from reducing pollution by other means. Indeed, we find evidence that the benefits of waste prevention alone are responsible for the observed association between lower emissions and profitability.</description><author>King, A.; Lenox, M.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing capacity and inventory jointly in manufacturing systems</title><link>http://www.example.com/articles/1</link><description>Bradley, J. R.; Glynn, P. W.
In this paper, we develop approximations that yield insight into the joint optimization of capacity and inventory, and how the optimal inventory policy varies with capacity investment in a single-product, single-station, make-to-stock manufacturing system in which inventory is managed through a base-stock policy. We allow for a correlated demand stream as we analyze our models in an asymptotic regime, in which the penalty and holding costs are small relative to the cost of capacity Although our approximations are asymptotically correct, our Brownian approximation is accurate even under moderate traffic intensity.</description><author>Bradley, J. R.; Glynn, P. W.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investment, capital structure, and complementarities between debt and new equity</title><link>http://www.example.com/articles/1</link><description>Stenbacka, R.; Tombak, M.
We study simultaneous investment and financing decisions made by incumbent owners in the presence of capital market imperfections, We present a theory for how the optimal combination of debt and equity financing depends on the firm's internal funds. We identify complementarities between the two financial instruments. We test these predictions empirically with panel data on 3,119 corporations in the COMPUSTAT database. Our estimates using instrumental variable techniques support our theoretical predictions regarding the link between internal funds and capital investments, as well as the interaction effects between debt and new equity. We explore implications for managers, financiers, and policy makers.</description><author>Stenbacka, R.; Tombak, M.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Make to order or make to stock: Model and application</title><link>http://www.example.com/articles/1</link><description>Rajagopalan, S.
Some firms make all their products to order while others make them to stock. There are a number of firms that maintain a middle ground, where some items are made to stock and others are made to order. This paper was motivated by a consumer product company faced with the decision about which items to make to stock and which ones to make to order, and the inventory and production policy for the make-to-stock items. The production environment is characterized by multiple items, setup times between the production of consecutive items, limited capacity, and congestion effects. In such an environment, making an item to order reduces inventory costs for that item, but might increase the lot size and inventory costs for the items made to stock. Also, lead times increase because of congestion effects, resulting in higher safety stocks for make-to-stock items and lower service levels for make-to-order items, thus leading to a complex trade-off. We develop a nonlinear, integer programming formulation of the problem. We present an efficient heuristic to solve the problem, which was motivated by key results for a special case of the problem without congestion effects that can be solved optimally. We also develop a lower bound to evaluate the performance of the heuristic. A computational study indicates that the heuristic performs well. We discuss the application of the model in a large firm and the resulting insights. We also provide insights into the impact of various problem parameters on the make-to-order versus make-to-stock decisions using a computational study. In particular, we find that the average number of setups of an item selected for make-to-stock production is always less than half the average number of setups of the item if it were to be made to order. Also, factors other than an item's demand, such as its setup time, processing time, and unit holding cost, impact the make-to-order versus make-to-stock decision.</description><author>Rajagopalan, S.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A dynamic IT adoption model for the SOHO market: PC generational decisions with technological expectations</title><link>http://www.example.com/articles/1</link><description>Kim, N.; Han, J. K.; Srivastava, R. K.
The small-office/home-office (SOHO) professionals comprise the fastest growing segment in the labor force today. Typically being a one-person business based at home, SOHO owners mostly rely on office information technology to single handedly run their entire operation. Despite the segment's ostensibly growing dependence and influence on the information technology (IT) industry, still very little is known about the dynamics between SOHO and IT products. With the purpose of addressing this void, we investigate the SOHO professionals' adoption patterns of multigenerational IT products. Accordingly, we develop and empirically estimate an individual SOHO-level initial- and repeat-purchase logit model that captures the procurement patterns for successive generations of technological products, namely the PC category. Specifically, we find that SOHO professionals' procurement choices are influenced by a number of salient dimensions (i.e., income, performance, price, interpurchase time, network externalities). Furthermore, some SOHO owners are found to have a preference for a future (expected) generation (over a currently available one), which is explained via their business dispositions (i.e., technology orientation, result orientation, search orientation) toward accepting technological incertitude.</description><author>Kim, N.; Han, J. K.; Srivastava, R. K.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing demand and sales dynamics in new product diffusion under supply constraint</title><link>http://www.example.com/articles/1</link><description>Ho, T. H.; Savin, S.; Terwiesch, C.
The Bass diffusion model is a well-known parametric approach to estimating new product demand trajectory over time. This paper generalizes the Bass model by allowing for a supply constraint. In the presence of a supply constraint, potential customers who are not able to obtain the new product join the waiting queue, generating backorders. and potentially reversing their adoption decision, resulting in lost sales. Consequently, they do not generate the positive "word-of-mouth" that is typically assumed in the Bass model, leading to significant changes in the new product diffusion dynamics. We study how a firm should manage its supply processes in a new product diffusion environment with backorders and lost sales. We consider a make-to-stock production environment and use optimal control theory to establish that it is never optimal to delay demand fulfillment. This result is interesting because immediate fulfillment may accelerate the diffusion process and thereby result in a greater loss of customers in the future. Using this result, we derive closed-form expressions for the resulting demand and sales dynamics over the product life cycle. We then use these expressions to investigate how the firm should determine the size of its capacity and the time to market its new product. We show that delaying a product launch to build up an initial inventory may be optimal and can be used as a substitute for capacity. Also, the optimal time to market and capacity increase with the coefficients of innovation and imitation in the adoption population. We compare our optimal capacity and time to market policies with those resulting from exogeneous demand forecasts in order to quantify the value of endogenizing demand.</description><author>Ho, T. H.; Savin, S.; Terwiesch, C.</author><pubDate>Fri, 01 Feb 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal control of a paired-kidney exchange program</title><link>http://www.example.com/articles/1</link><description>Zenios, S. A.
Organ exchanges are expected to increase the utilization of living donors and to alleviate the critical shortage of organs for transplantation. The typical arrangement involves a direct exchange between two blood-type incompatible donor-candidate pairs. An alternate possibility is an indirect exchange between one such pair and the highest priority candidate on the regular waiting list for cadaveric organs. This paper focuses on the mix of direct and indirect exchanges that maximizes the expected total discounted quality-adjusted life years (QALY) of the candidates in the participating pairs. Direct exchanges are preferable because the candidate receives a living-donor organ instead of the inferior cadaveric organ an indirect exchange provides. However, the latter involves a shorter wait. To capture this tradeoff, we develop a double-ended queueing model for an exchange system with two types of donor-candidate pairs, and obtain an optimal dynamic exchange policy by invoking a Brownian approximation. The policy takes the form of a two-sided regulator in which new pairs will join the exchange system to wait for a direct exchange if and only if the process modeling the exchange system is within the regulator's two barriers. In all other circumstances, new pairs will participate in an indirect exchange. Expressions for the optimal barriers are obtained under a variety of assumptions about the objective function, including one of complete candidate autonomy. The analysis identifies three design principles that will amplify the likelihood of an exchange program's success. First, exchange programs must involve the coordinated activities of multiple local transplant centers to enjoy the substantial benefits of resource pooling. Second, participant wait must be controlled through indirect exchanges. Third, the program must respect participants' autonomy and weigh that autonomy against the broader goal of maximizing their overall welfare.</description><author>Zenios, S. A.</author><pubDate>Fri, 01 Mar 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational endowments and the performance of university start-ups</title><link>http://www.example.com/articles/1</link><description>Shane, S.; Stuart, T.
The question of how initial resource endowments-the stocks of resources that entrepreneurs contribute to their new ventures at the time of founding-affect organizational life chances is one of significant interest in organizational ecology, evolutionary theory, and entrepreneurship research. Using data on the life histories of all 134 firms founded to exploit MIT-assigned inventions during the 1980-1996 period, the study analyzes how resource endowments affect the likelihood of three critical outcomes: that new ventures attract venture capital financing, experience initial public offerings, and fail. Our analysis focuses on the role of founders' social capital as a determinant of these outcomes. Event history analyses show that new ventures with founders having direct and indirect relationships with venture investors are most likely to receive venture funding and are less likely to fail. In turn, receiving venture funding is the single most important determinant of the likelihood of IPO. We conclude that the social capital of company founders represents an important endowment for early-stage organizations.</description><author>Shane, S.; Stuart, T.</author><pubDate>Tue, 01 Jan 2002 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>