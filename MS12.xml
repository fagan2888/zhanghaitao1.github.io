<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>MS12</title><link>http://www.example.com/rss</link><description>This is the feed for items from my zotero.</description><language>en-US</language><lastBuildDate>Sun, 08 Dec 2019 22:27:10 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Intuitions about combining opinions: Misappreciation of the averaging principle (vol 52, pg 111, 2006)</title><link>http://www.example.com/articles/1</link><description>Larrick, R. P.; Soll, J. B.
nan</description><author>Larrick, R. P.; Soll, J. B.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Proactive and reactive product line strategies: Asymmetries between market leaders and followers</title><link>http://www.example.com/articles/1</link><description>Shankar, V.
To what extent do firms engage in product line actions simultaneously with actions in other marketing variables? What are the determinants of product line actions? To what extent are product line actions proactive? To what degree are they reactive? How can a firm's product line action elasticity (percent change in product line length with respect to percent change in competitor's past and anticipated actions) be decomposed into reaction and anticipation elasticities? Are product line actions and elasticities symmetric across market leaders and followers? To address these questions, we develop a conceptual framework comprising determinants of product line and other marketing actions in a single framework. We formulate hypotheses about the asymmetries between market leaders and followers regarding product line actions based on extended expectancy-valence and competitive demand elasticity theories. We develop a simultaneous equation model of demand and supply with product line and other marketing actions, which can be used to identify reaction and anticipation elasticities through the rational expectations approach. We estimate the model using data from the computer printer market comprising the market leader, Hewlett Packard (HP), and followers: Epson, Canon, and Lexmark. The results show that the market leader practices a product-proliferation strategy and rarely fights on price. In contrast, market followers adopt a price-fighting strategy. A firm is more likely to engage in product line actions when its competitors changed their product lines in the past, when the firm is large, and when its price is high. Product line reaction and anticipation elasticities are asymmetric between themselves and across the firms. For the market leader (followers), product line reaction elasticity is higher (lower) than product line anticipation elasticity. These differences are related to product line demand elasticities, which are higher for the market leader than they are for the followers.</description><author>Shankar, V.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A contract and balancing mechanism for sharing capacity in a communication network</title><link>http://www.example.com/articles/1</link><description>Anderson, E.; Kelly, F.; Steinberg, R.
We propose a method for determining how much to charge users of a communication network when they share bandwidth. Our approach can be employed either when a network owner wishes to sell bandwidth for a specified period of time to a number of different users, or when users cooperate to build a network to be shared among themselves. Our proposed contract and balancing mechanism can mediate between rapidly fluctuating prices and the longer time scales over which bandwidth contracts may be traded. An advantage of the process is that it avoids perverse incentives for a capacity provider to increase congestion.</description><author>Anderson, E.; Kelly, F.; Steinberg, R.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Testing the statistical significance of linear programming estimators</title><link>http://www.example.com/articles/1</link><description>Horsky, D.; Nelson, P.
Linear programming-based estimation procedures are used in a variety of arenas. Two notable areas are multiattribute utility models (LINMAP) and production frontiers (data envelopment analysis (DEA)). Both LINMAP and DEA have theoretical and managerial advantages. For example, LINMAP treats ordinal-scaled preference data as such in uncovering individual-level attribute weights, while regression treats these preferences as interval scaled. DEA produces easy-to-understand efficiency measures, which allow for improved productivity benchmarking. However, acceptance of these techniques is hindered by the lack of statistical significance tests for their parameter estimates. In this paper, we propose and evaluate such parameter significance tests. Two types of tests are forwarded. The first examines whether a model's fit is significantly reduced when an explanatory variable is deleted. The second is based on generating a standard deviation or distribution for the parameter estimate using nonparametric jackknife or bootstrap techniques. We demonstrate through simulations that both types of tests reliably identify both significant and insignificant parameters. The availability of these tests, especially the relatively simple and easy-to-use tests of the first type, should enhance the utilization of linear programming-based estimation.</description><author>Horsky, D.; Nelson, P.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Intuitions about combining opinions: Misappreciation of the averaging principle</title><link>http://www.example.com/articles/1</link><description>Larrick, R. P.; Soll, J. B.
Averaging estimates is an effective way to improve accuracy when combining expert judgments, integrating group members' judgments, or using advice to modify personal judgments. If the estimates of two judges ever fall on different sides of the truth, which we term bracketing, averaging must outperform the average judge for convex loss functions, such as mean absolute deviation (MAD). We hypothesized that people often hold incorrect beliefs about averaging, falsely concluding that the average of two judges' estimates would be no more accurate than the average judge. The experiments confirmed that this misconception was common across a range of tasks that involved reasoning from summary data (Experiment 1), from specific instances (Experiment 2), and conceptually (Experiment 3). However, this misconception decreased as observed or assumed bracketing rate increased (all three studies) and when bracketing was made more transparent (Experiment 2). Experiment 4 showed that flawed inferential rules and poor extensional reasoning abilities contributed to the misconception. We conclude by describing how people may face few opportunities to learn the benefits of averaging and how misappreciating averaging contributes to poor intuitive strategies for combining estimates.</description><author>Larrick, R. P.; Soll, J. B.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing American-style derivatives with European call options</title><link>http://www.example.com/articles/1</link><description>Laprise, S. B.; Fu, M. C.; Marcus, S. I.; Lim, A. E. B.; Zhang, H. J.
We present a new approach to pricing American-style derivatives that is applicable to any Markovian setting (i.e., not limited to geometric Brownian motion) for which European call-option prices are readily available. By approximating the value function with an appropriately chosen interpolation function, the pricing of an American-style derivative with arbitrary payoff function is converted to the pricing of a portfolio of European call options, leading to analytical expressions for those cases where analytical European call prices are available (e.g., the Merton jump-diffusion process). Furthermore, in many settings, the approach yields upper and lower analytical bounds that provably converge to the true option price. We provide computational results to illustrate the convergence and accuracy of the resulting estimators.</description><author>Laprise, S. B.; Fu, M. C.; Marcus, S. I.; Lim, A. E. B.; Zhang, H. J.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Less likely to fail: Low performance, firm size, and factory expansion in the shipbuilding industry</title><link>http://www.example.com/articles/1</link><description>Audia, P. G.; Greve, H. R.
The behavioral theory of the firm and prospect theory predict that performance below an aspiration level increases risk taking, but researchers also propose that performance below an aspiration level decreases risk taking. These conflicting predictions primarily hinge on whether decision makers perceive negative performance as a repairable gap or as a threat to firm survival. This study examines a boundary condition of these conflicting predictions. We argue that a firm's resource endowment affects decision makers' risk tolerance: Managers in firms with large stocks of resources are buffered from the threat of failure and conform to the prediction of greater risk taking in response to performance decreases; managers in firms with limited resources view low performance as a step closer to failure and decrease risk taking in response to performance decreases. Using data on the risky decision of factory expansion in shipbuilding firms and firm size as an indicator of the stock of tangible resources, we find that performance below the aspiration level reduces risk taking in small firms, but either does not affect risk taking or increases risk taking in large firms. These findings are largely consistent with our predictions and also suggest that large firms are more inert than small firms.</description><author>Audia, P. G.; Greve, H. R.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>In search of complementarity in innovation strategy: Internal R&amp;D and external knowledge acquisition</title><link>http://www.example.com/articles/1</link><description>Cassiman, B.; Veugelers, R.
Empirical research on complementarity between organizational design decisions has traditionally focused on the question of existence of complementarity. In this paper, we take a broader approach to the issue, combining a "productivity" and an "adoption" approach, while including a search for contextual variables in the firm's strategy that affects complementarity. Analysis of contextual variables is not only interesting per se, but also improves the productivity test for the existence of complementarity. We use our empirical methodology to analyze complementarity between innovation activities: internal research and development (R&amp;D) and external knowledge acquisition. Our results suggest that internal R&amp;D and external knowledge acquisition are complementary innovation activities, but that the degree of complementarity is sensitive to other elements of the firm's strategic environment. We identify reliance on basic R&amp;D-the importance of universities and research centers as an information source for the innovation process-as an important contextual variable affecting complementarity between internal and external innovation activities.</description><author>Cassiman, B.; Veugelers, R.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supplier commitment and production decisions under a forecast-commitment contract</title><link>http://www.example.com/articles/1</link><description>Durango-Cohen, E. J.; Yano, C. A.
Manufacturing firms in capital-intensive industries face inherent demand volatility for their products and the inability to change their capacity quickly. To cope with these challenges, manufacturers often enter into contracts with their customers that offer greater certainty of supply in return for more predictable orders. In this paper, we study a "forecast-commitment" contract in which the customer provides a forecast, the supplier makes a production commitment to the customer based on the forecast, and the customer's minimum order quantity is a function of the forecast and committed quantities. We provide a complete analysis of the supplier's decisions when there is a single customer facing uncertain demand. We first show that the supplier has two dominant commitment strategies: committing to the forecast or committing to the production quantity. We then characterize the jointly optimal commitment and production strategy for the supplier and extend the results to consider a capacity constraint. We show that the proposed contract can moderate the supplier's motivation to underproduce, and due to the structure of the contract and the form of the supplier's optimal strategy, also limits the customer's incentive to overforecast. We also provide results for a capacitated two-customer example, which show that the supplier's choice of production quantity for each customer is not necessarily nondecreasing in the total available capacity.</description><author>Durango-Cohen, E. J.; Yano, C. A.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Price competition with reduced consumer switching costs: the case of "wireless number portability" in the cellular phone industry</title><link>http://www.example.com/articles/1</link><description>Shi, M. Z.; Chiang, J. W.; Rhee, B. D.
Wireless number portability (WNP) is a telecommunication regulatory policy that requires cellular phone service providers to allow customers who switch service subscriptions to retain their original phone numbers. The right to retain the number lowers the switching cost for a consumer. Thus, the purpose of the policy is to induce more competition and facilitate the growth of new or small service providers. In this paper, we show that WNP drives market price downward as expected but with a surprising twist-rather than helping the smaller firms grow, the policy may accelerate the process of market concentration. We find that the main contributing factor to this peculiarity is the discriminatory pricing scheme prevalent in the industry-that is, a service provider charges a lower per-minute fee for the calls initiated and received within the same network than for the calls connected across two networks. Under this pricing scheme, a consumer who subscribes to a larger network would benefit more than if subscribing to a smaller network, despite the relatively higher fixed access fee that the former may charge. By lowering the barrier of switching, WNP creates a market condition conducive for a larger network to gain market share. We support our analysis with the empirical evidence gathered from Hong Kong where WNP was adopted in March 1999.</description><author>Shi, M. Z.; Chiang, J. W.; Rhee, B. D.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Managing new and remanufactured products</title><link>http://www.example.com/articles/1</link><description>Ferrer, G.; Swaminathan, J. M.
We study a firm that makes new products in the first period and uses returned cores to offer remanufactured products, along with new products, in future periods. We introduce the monopoly environment in two-period and multiperiod scenarios to identify thresholds in remanufacturing operations. Next, we focus our attention on the duopoly environment where an independent operator (10) may intercept cores of products made by the original equipment manufacturer (OEM) to sell remanufactured products in future periods. We characterize the production quantities associated with self-selection and explore the effect of various parameters in the Nash equilibrium. Among other results, we find that if remanufacturing is very profitable, the original-equipment manufacturer may forgo some of the first-period margin by lowering the price and selling additional units to increase the number of cores available for remanufacturing in future periods. Further, as the threat of competition increases, the OEM is more likely to completely utilize all available cores, offering the remanufactured products at a lower price.</description><author>Ferrer, G.; Swaminathan, J. M.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reverse channel design: The case of competing retailers</title><link>http://www.example.com/articles/1</link><description>Savaskan, R. C.; Van Wassenhove, L. N.
The economical and environmental benefits of product remanufacturing have been widely recognized in the literature and in practice. In this paper, we focus on the interaction between a manufacturer's reverse channel choice to collect postconsumer goods and the strategic product pricing decisions in the forward channel when retailing is competitive. To this end, we model a direct product collection system, in which the manufacturer collects used products directly from the consumers (e.g., print and copy cartridges) and an indirect product collection system, in which the retailers act as product return points (e.g., single-use cameras, cellular phones). We first examine how the allocation of product collection to retailers impacts their strategic behavior in the product market, and we discuss the economic trade-offs the manufacturer faces while choosing an optimal reverse channel structure. When a direct collection system is used, channel profits are driven by the impact of scale of returns on collection effort, whereas in the indirect reverse channel, supply chain profits are driven by the competitive interaction between the retailers. Subsequently, we show that the buy-back payments transfered to the retailers for postconsumer goods provide a wholesale pricing flexibility that can be used to price discriminate between retailers of different profitability</description><author>Savaskan, R. C.; Van Wassenhove, L. N.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Patent protection, complementary assets, and firms' incentives for technology licensing</title><link>http://www.example.com/articles/1</link><description>Arora, A.; Ceccagnoli, M.
This paper analyzes the relationship between technology licensing and the effectiveness of patent protection. Using the 1994 Carnegie Mellon survey on industrial research and development (R&amp;D) in the United States, we develop and test a simple structural model in which the patenting and licensing decisions are jointly determined. We find that increases in the effectiveness of patent protection increases licensing propensity, but only when the firm lacks specialized complementary assets required to commercialize new technologies. In contrast, for firms with specialized complementary assets, increases in patent effectiveness increase patenting propensity but reduce the propensity to license. We present systematic cross-industry empirical support for the proposition that intellectual property protection is a key determinant of the vertical boundaries of the firm and the market for technology but that its impact is mediated by a firm's ownership of specialized complementary assets.</description><author>Arora, A.; Ceccagnoli, M.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal project rejection and new firm start-ups</title><link>http://www.example.com/articles/1</link><description>Cassiman, B.; Ueda, M.
We study the decision of an established firm to commercialize innovations. An innovation can be exploited by the established firm as an internal venture, pursued by a new firm start-up as an external venture, or not commercialized at all. The limited commercialization capacity of the established firm in the short run results in an option value of waiting. In this setup, start-up firms emerge when the established firm is generating many innovations or is selective because the option value of waiting is high, or both. The model predicts that innovations commercialized through internal ventures are characterized by a higher fit with the internal resources of the established firm, a higher cannibalization of the established firm's existing businesses, and a lower profitability than innovations commercialized through external ventures. The model furthermore generates predictions on the relation between firm performance and spin-off performance.</description><author>Cassiman, B.; Ueda, M.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Continual corporate entrepreneurial search for long-term growth</title><link>http://www.example.com/articles/1</link><description>Bhardwaj, G.; Camillus, J. C.; Hounshell, D. A.
This paper examines how established firms conduct continual entrepreneurial search for possibilities for long-term growth. Drawing on comprehensive internal documents of the DuPont Company over a 20-year period, we develop a search process that is a departure from frequent depictions of search as local or random. Longitudinal field data show that corporate entrepreneurs follow a "moving, anchored search" for growth possibilities. Employing this framework as lens, we develop propositions. We find that corporate entrepreneurs are more likely to conduct search in new domains following events that cause them to expect future performance to change significantly and lastingly. This is in contrast to the literature that has typically modeled the initiation of search as a response to poor past performance. Because new domains are unexplored territories for corporate entrepreneurs, they utilize transitional levers that they perceive will facilitate the move from existing domains to new ones. These perceived transitional levers, however, typically prove inaccurate or incomplete. Content within domains is searched using anchors whose locations and numbers change. The combination of search process and content searched influences the particular growth possibilities discovered and created. Search and pursuit of growth possibilities is accompanied by the creation of new knowledge and new capabilities.</description><author>Bhardwaj, G.; Camillus, J. C.; Hounshell, D. A.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the focused issue on entrepreneurship</title><link>http://www.example.com/articles/1</link><description>Shane, S.
This article is an introduction to the focused issue on entrepreneurship. It provides motivation for greater scholarly investigation of the phenomenon of entrepreneurship, explains the evolution of the focused issue, offers an overview of the seven papers in the issue, and offers the editor's thoughts on the relationship of the papers in the focused issue to research on entrepreneurship in general.</description><author>Shane, S.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Social capital, geography, and survival: Gujarati immigrant entrepreneurs in the US lodging industry</title><link>http://www.example.com/articles/1</link><description>Kalnins, A.; Chung, W.
Immigrant entrepreneurs often rely on their group's local social capital in their new home market to establish and maintain their businesses. In particular, immigrant entrepreneurs with few resources of their own receive help from those possessing more resources. Supporting these arguments using the empirical setting of Gujarati immigrant entrepreneurs in the lodging industry, we find that the likelihood of survival of an immigrant entrepreneur's hotel increases when surrounded by higher counts of branded hotels (representing high-resource establishments) owned by individuals from their ethnic group but is unaffected by unbranded motels (representing low-resource establishments) owned by members of their ethnic group or by branded hotels owned by individuals from other ethnic groups. These results isolate and reinforce the importance of social capital not only for immigrant entrepreneurs but also more generally for any entrepreneurs belonging to ethnic, professional, religious, or social groups.</description><author>Kalnins, A.; Chung, W.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multistage selection and the financing of new ventures</title><link>http://www.example.com/articles/1</link><description>Eckhardt, J. T.; Shane, S.; Delmar, F.
Using a random sample of 221 new Swedish ventures initiated in 1998, we examine why some new ventures are more likely than others to successfully be awarded capital from external sources. We examine venture financing as a staged selection process in which two sequential selection events systematically winnow the population of ventures and influence which ventures receive financing. For a venture to receive external financing its founders must first select it as a candidate for external funding, and then a financier must fund it. We find evidence that founders select ventures as candidates for external finance based on their perceptions of market competition, market growth, and employment growth, while financiers base funding decisions on objective verifiable indicators of venture development, such as the completion of organizing activities, marketing activities, and the level of sales of the venture. Our findings have implications for venture financing and evolutionary theories of social processes.</description><author>Eckhardt, J. T.; Shane, S.; Delmar, F.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Venture capitalists and cooperative start-up commercialization strategy</title><link>http://www.example.com/articles/1</link><description>Hsu, D. H.
This paper examines the possible impact of venture capital (VC) backing on the commercialization direction of technology-based start-ups by asking: To what extent (if at all) do VC-funded start-ups engage in cooperative commercialization strategies (strategic alliances or technology licensing, or both) relative to a comparable set of start-ups, and with what consequences? To address these questions, I assemble a novel data set that matches firms receiving a federal research and development subsidy through the U.S. Small Business Innovative Research program to VC-funded firms by observable characteristics in five technology-intensive industries. These data allow decoupling of cooperative activity resulting from start-up development via the passage of calendar time from that due to association with VCs. An analysis of the 696 start-ups in the sample (split by an external funding source) suggests substantial boosts in both cooperative activity associated with VC-backed firms and in the likelihood of an initial public offering.</description><author>Hsu, D. H.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The personal computer and entrepreneurship</title><link>http://www.example.com/articles/1</link><description>Fairlie, R. W.
In contrast to the large and rapidly growing literature on information technology (IT) investments and firm productivity, we know very little about the role of personal computers in business creation. Using matched data from the 1997-2001 Computer and Internet Usage Supplements to subsequent Outgoing Rotation Group files from the Current Population Survey (CPS), I explore the relationship between computer ownership and entrepreneurship. Trends over the past two decades provide some evidence of a positive relationship between home computers and entrepreneurship rates, but the evidence is not clear. In contrast, an analysis of the relationship between computer ownership and entrepreneurship at the individual level provides evidence that individuals who had access to a home computer are substantially more likely to become entrepreneurs over the following 12-15 months. Probit and bivariate probit regressions also provide evidence of a strong positive relationship between computer ownership and entrepreneurship among women, but only limited evidence for men. Further, estimates from the CPS indicate that entrepreneurs who had prior access to home computers create a large variety of types of businesses and not only those in the IT industry.</description><author>Fairlie, R. W.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Overoptimism and the performance of entrepreneurial firms</title><link>http://www.example.com/articles/1</link><description>Lowe, R. A.; Ziedonis, A. A.
Recent theoretical and empirical research on cognitive bias in decision making suggests that overoptimism critically influences entrepreneurs' decisions to establish and sustain new ventures. We investigate whether such cognitive bias influences entrepreneurial venture performance using data on commercialization efforts for university inventions. In contrast to prior studies, our results suggest that entrepreneurial overoptimism does not appear to be the determining factor in the decision to found a firm. We do find that entrepreneurs continue unsuccessful development efforts for longer periods of time than do established firms, which is consistent with entrepreneurial overoptimism in the development of technologies with uncertain market prospects. This latter finding is also consistent with rationality-based models of decision-making behavior, however. We find that the economic returns associated with many of the technologies in our sample are realized after the start-up has been acquired by an established firm, suggesting that start-ups may serve as a transitional organizational form in the market for technology commercialization.</description><author>Lowe, R. A.; Ziedonis, A. A.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A hubris theory of entrepreneurship</title><link>http://www.example.com/articles/1</link><description>Hayward, M. L. A.; Shepherd, D. A.; Griffin, D.
This paper develops a hubris theory of entrepreneurship to explain why so many new ventures are created in the shadow of high venture failure rates: More confident actors are moved to start ventures, and then act on such confidence when deciding how to allocate resources in their ventures. Building on theory and evidence from the behavioral decision-making literature, we describe how founders' socially constructed confidence affects the manner in which they interpret information about their prior and current ventures. We then link founders' propensity to be overconfident to their decisions to allocate, use, and attain resources. In our model, founders with greater socially constructed confidence tend to deprive their ventures of resources and resourcefulness and, therefore, increase the likelihood that their ventures will fail.</description><author>Hayward, M. L. A.; Shepherd, D. A.; Griffin, D.</author><pubDate>Wed, 01 Feb 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Designing a family of development-intensive products</title><link>http://www.example.com/articles/1</link><description>Krishnan, V.; Zhu, W.
Faced with fragmented markets, saturated and demanding customers, and global competition, firms increasingly must design and offer a line of innovative, quality-differentiated products to target customers with differing willingness to pay (WTP). In this context, designing a special class of products that we term development-intensive products (DIPs)-for which the fixed costs of development far outweigh the unit-variable costs-presents some unique managerial challenges. Examples of such development-intensive offerings abound in a number of industries, including the pharmaceutical, information, and entertainment sectors of the economy. Our contributions in this paper are threefold: (a) to show that managerial insights from the traditional approach to product-line design developed for unit-variable cost-intensive products do not carry over to DIPs, (b) to present new mechanisms and managerial guidelines for designing a family of products for which development costs cannot be ignored, and (c) to illustrate the insights with an extended industry example. We find that the design approach based on degrading (or subtracting value from) a high-end product to obtain a subsumed low-end edition, shown in the literature to be an effective approach for designing unit cost-intensive products, can be inappropriate for DIPs. This limitation of value subtraction has implications for the number of variants and the sequence in which they are developed. As an alternative to a subsumed product-design strategy, we propose and examine the overlapped product-design approach, in which a low-end product is not completely subsumed within its high-end counterpart, but differentiated on additional vertical quality dimensions. Our results both explain the recent challenges of firms with subsumed low-end products and guide them in designing a product line to successfully address emerging low-end market segments.</description><author>Krishnan, V.; Zhu, W.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Supply chain choice on the Internet</title><link>http://www.example.com/articles/1</link><description>Netessine, S.; Rudi, N.
Internet companies extensively use the practice of drop-shipping, where the wholesaler stocks and owns the inventory and ships products directly to customers at retailers' request. Under the drop-shipping arrangement, the supply chain benefits from risk pooling because the inventory for multiple retailers is stocked at the same location, the wholesaler's. Another more traditional channel alternative on the Internet is one in which retailers stock and own the inventory These two supply chain structures, which predominate on the Internet, result in different inventory risk allocation, stocking decisions, and profits for channel members. Moreover, the two channel alternatives can be combined into a dual strategy whereby the retailer uses local inventory as a primary source and relies on drop-shipping as a backup. We model the dual strategy as a noncooperative game among the retailers and the wholesaler, analyze it, and obtain insights into the structural properties of the equilibrium solution to facilitate development of recommendations for practicing managers. Finally, we characterize situations in which each of three channels is preferable by specifying appropriate ranges of critical parameters, including demand variability, the number of retailers in the channel, wholesale prices, and transportation costs.</description><author>Netessine, S.; Rudi, N.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Technological development and the boundaries of the firm: A knowledge-based examination in semiconductor manufacturing</title><link>http://www.example.com/articles/1</link><description>Macher, J. T.
This paper examines how the knowledge-based view (KBV) can be applied to firm boundary decisions and the performance implications of those decisions. At the center of the paper is a theoretical and empirical examination of how firms most efficiently organize to solve different types of problems related to technological development, using the semiconductor industry as the empirical setting. Measures that capture important dimensions of performance support the proposition that organization affects performance in problem solving related to knowledge development. Integrated firms realize performance advantages when problem solving in technological development is ill structured and complex, while the same is true for specialized firms when problem solving in technological development is well structured and simple. Performance differences also arise from the presence of scale economies and scope economies.</description><author>Macher, J. T.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A bargaining model for a first-time interaction under asymmetric beliefs of supply reliability</title><link>http://www.example.com/articles/1</link><description>Gurnani, Haresh; Shi, Mengze
We consider the case of a first-time interaction between a buyer and a supplier who is unreliable in delivery The supplier declares her estimate of the ability to meet the order obligations, but the buyer may have a different estimate, which may be higher or lower than the supplier's estimate. We derive the Nash bargaining solution and discuss the role of using a down-payment or nondelivery penalty in the contract. For the case of buyer overtrust, the down-payment contract maximizes channel profits when the supplier's estimate is public information. If the supplier's estimate is private information, a nonsymmetric contract is shown to be efficient and incentive compatible. For the case of buyer undertrust, the contract structure is quite different as both players choose not to include down-payments in the contract. When delivery estimates are public information, a nondelivery penalty contract is able to maximize channel profits if the buyer uses the supplier's estimate in making the ordering decision. If estimates are private information, channel profits are maximized only if the true estimates of both players are not far part. We also discuss the effect of different risk profiles on the nature of the bargaining solution. In three extensions of the model, we consider the following variants of the basic problem. First, we analyze the effect of early versus late negotiation on the bargaining solution. Then, we study the case of endogenous supply reliability, and finally, for the case of repeated interactions, we discuss the impact of updating delivery estimates on the order quantity and negotiated prices of future orders.</description><author>Gurnani, Haresh; Shi, Mengze</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Procuring fast delivery: Sole sourcing with information asymmetry</title><link>http://www.example.com/articles/1</link><description>Cachon, G. P.; Zhang, F. Q.
This paper studies a queuing model in which a buyer sources a good or service from a single supplier chosen from a pool of suppliers. The buyer seeks to minimize the sum of her procurement and operating costs, the latter of which depends on the supplier's lead time. The selected supplier can regulate his lead time, but faster lead times are costly. Although the buyer selects the supplier to source from (possibly via an auction) and dictates the contractual terms, the buyer's bargaining power is limited by asymmetric information: The buyer only has an estimate of the suppliers' costs, while the suppliers know their costs precisely. We identify a procurement mechanism that minimizes the buyer's total cost (procurement plus operating). This mechanism is not simple: It is a numerically derived nonlinear menu of contracts. Therefore, we study several simpler mechanisms: e.g., one that charges a late fee and one that specifies a fixed lead-time requirement (no menus, no nonlinear functions). We find that simple mechanisms are nearly optimal (generally within 1% of optimal) because asymmetric information conveys significant protection to the supplier, i.e., the supplier is able to retain most of the benefit of having a lower cost. Renegotiation is another concern with the optimal mechanism: Because it does not minimize the supply chain's cost, the firms can be both better off if they throw away the contract and start over. Interestingly, we find that the potential gain from renegotiation is relatively small with either the optimal or our simple mechanisms. We conclude that our simple mechanisms are quite attractive along all relevant dimensions: buyer's performance, supply chain performance, simplicity, and robustness to renegotiation.</description><author>Cachon, G. P.; Zhang, F. Q.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Competitive advertising strategies and market-size dynamics: A research note on theory and evidence</title><link>http://www.example.com/articles/1</link><description>Nguyen, D.; Shi, L.
This paper analyzes competing firms' advertising strategies in markets characterized by dynamic market sizes. For new product innovations, a large literature has been developed on the basis of the celebrated Bass diffusion process, which captures what we call market-size dynamics. However, in spite of the significant literature incorporating competitiveness into the Bass model, few if any analytical results have been obtained, especially those with normative implications. On the other hand, important theoretical and empirical insights have recently been obtained using the famous Lanchester warfare model of competition, in which market-share dynamics are governed by advertising activities for mature products whose market sizes are typically fixed or stable. Our paper seeks to address the issue of optimal advertising strategies incorporating both market-share dynamics and market-size dynamics. In doing so, we seek to take advantage of the theoretical and normative insights currently available in the Lanchester formulation for mature products to obtain normative, analytical results on competitive advertising strategies for new product innovations whose markets evolve around the Bass process. We first obtain analytical solutions to the problem at hand and offer theoretical implications for competitive advertising on the Bass diffusion model. The model's parameters are then estimated using a set of data on sales and advertising expenditures of Polaroid Corp. and Eastman Kodak Co. in instant photography during the 1976-1985 period; optimal trajectories for the firms' advertising spending are then calculated, and other marketing and managerial implications of optimal advertising strategies are explored.</description><author>Nguyen, D.; Shi, L.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Personal taxes, endogenous default, and corporate bond yield spreads</title><link>http://www.example.com/articles/1</link><description>Liu, Sheen X.; Qi, Howard; Wu, Chunchi
Term structure models have often been criticized for failing to explain satisfactorily the yield spread between corporate and Treasury bonds. A potential problem is that the personal tax effect is ignored in these models. In this paper, we employ a structural model to investigate the role of personal taxes on both debt and equity returns in capital structure decisions and assess their impact on corporate bond yield spreads. It is shown that personal taxes affect the firm's optimal capital structure, and the tax premium explains a substantial portion of yield spreads, especially for high-grade bonds. The predictive ability of the model for yield spreads is much improved when personal tax effects are accounted for. In controlling for the liquidity effect, we obtain implied personal income tax rates closely in line with Graham's (1999) estimates.</description><author>Liu, Sheen X.; Qi, Howard; Wu, Chunchi</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reclaiming quasi-Monte Carlo efficiency in portfolio value-at-risk simulation through Fourier transform</title><link>http://www.example.com/articles/1</link><description>Jin, X.; Zhang, A. X.
Quasi-Monte Carlo methods overcome the problem of sample clustering in regular Monte Carlo simulation and have been shown to improve simulation efficiency in the derivatives pricing literature when the price is expressed as a multidimensional integration and the integrand is suitably smooth. For portfolio value-at-risk (VaR) problems, the distribution of portfolio value change is based on the expectation of an indicator function, hence the integrand is discontinuous. The purpose of this paper is to smooth the expectation estimation of an indicator function via Fourier transform so that the faster convergence rate of quasi-Monte Carlo methods can be reclaimed theoretically. Under fairly mild assumptions, the simulation of portfolio value-at-risk is fast and accurate. Numerical examples elucidate the advantage of the proposed approach over regular Monte Carlo and quasi-Monte Carlo methods.</description><author>Jin, X.; Zhang, A. X.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Identifying sources of heterogeneity for empirically deriving strategic types: A constrained finite-mixture structural-equation methodology</title><link>http://www.example.com/articles/1</link><description>DeSarbo, Wayne S.; Di Benedetto, C. Anthony; Jedidi, Kamel; Song, Michael
The resource-based view (RBV) of the firm suggests that strategic deployment of capabilities allows strategic business units (SBUs) to exploit distinctive competencies and create sustainable competitive advantage. Following the RBV, we propose a new predictive methodology for deriving typologies of SBUs that accommodates heterogeneity among SBUs with respect to their strategic capabilities, how effectively they are employed, and performance. Statistically, we devise a constrained finite-mixture structural-equation procedure that simultaneously accounts for firm capabilities, performance outcomes, and the relationships between them. The procedure allows for a comprehensive modeling and grouping of entities, and simultaneously provides a diagnosis of the sources of heterogeneity via the flexibility of estimating a series of nested models. Managerially, our proposed methodology is grounded in the strategic type and RBV literature and can capture the effects of environmental and industry-specific factors. Using data obtained from 216 SBUs in the United States for illustration, the results show that our derived four mixed-type solution dominates the four-group, Prospectors-Analyzers-Defenders-Reactors classification as well as a number of other nested model solutions in terms of objective statistical fit criteria for this data set, suggesting a more contingency-driven strategic stance adopted by these SBUs. We conclude with a discussion of the theoretical and managerial benefits of an improved methodology for empirically deriving strategic typologies.</description><author>DeSarbo, Wayne S.; Di Benedetto, C. Anthony; Jedidi, Kamel; Song, Michael</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploring decision makers' use of price information in a speculative market</title><link>http://www.example.com/articles/1</link><description>Johnson, Johnnie E. V.; Jones, Owen; Tang, Leilei
We explore the extent to which the decisions of participants in a speculative market effectively account for information contained in prices and price movements. The horse race betting market is an ideal environment to explore these issues. A conditional logit model is constructed to determine winning probabilities based on bookmakers' closing prices and the time-indexed movement of prices to the market close. We incorporate a technique for extracting predictors from price (odds) curves using orthogonal polynomials. The results indicate that closing prices do not fully incorporate market price information, particularly information that is less readily discernable by market participants.</description><author>Johnson, Johnnie E. V.; Jones, Owen; Tang, Leilei</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An empirical examination of the decision to invest in fulfillment capabilities: A study of Internet retailers</title><link>http://www.example.com/articles/1</link><description>Randall, T.; Netessine, S.; Rudi, N.
Internet technology has allowed for a higher degree of decoupling between the information-intensive sales process and the physical process of inventory management than its brick-and-mortar counterpart. As a result, some Internet retailers choose to outsource inventory and back-end operations to focus on the sales/marketing aspects of e-commerce. Nonetheless, many retailers keep fulfillment capabilities in-house. In this paper, we identify and empirically test factors that persuade firms to integrate inventory and fulfillment capabilities with virtual storefronts. Based on the extant literature and previous research in e-commerce, we formulate nine theoretical predictions. We then use data from a sample of over 50 public Internet retailers to test whether empirical data are consistent with these hypotheses. Finally, given the strategic importance and financial magnitude of the inventory investment decision, we analyze the effect of this decision on the economic success of Internet retailers during the period of study. We find that there are many circumstances in which it is prudent to own fulfillment capabilities and inventory. Empirical data are consistent with hypotheses that this tendency is higher for older firms selling small, high-margin products, offering lower levels of product variety; and facing lower demand uncertainty. We also discover that firms making inventory ownership decisions that are consistent with an empirical benchmark derived from environmental and strategic factors are less likely to go bankrupt than those making inconsistent inventory choices.</description><author>Randall, T.; Netessine, S.; Rudi, N.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Allocation of service time in a multiserver system</title><link>http://www.example.com/articles/1</link><description>El-Taha, M.; Maddah, B.
Reducing congestion is a primary concern in the design and analysis of queueing networks, especially in systems where sources of randomness are characterized by high variability. This paper considers a multiserver first-come, first-served (FCFS) queueing model where we arrange servers in two stations in series. All arrivals join the first service center, where they receive a maximum of T units of service. Arrivals with service requirements that exceed the threshold T join the second queue, where they receive their remaining service. For a variety of heavy tail service time distributions, characterized by large coefficient of variations, analytical and numerical comparisons show that our scheme provides better system performance than the standard parallel multiserver model in the sense of reducing the mean delay per customer in heavy traffic systems. Our model is likely to be useful in systems where high variability is a cause for degradation and where numerous service interruptions are not desired.</description><author>El-Taha, M.; Maddah, B.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note-sole entrant, co-optor, or component supplier: Optimal end-product strategies for manufacturers of proprietary component brands</title><link>http://www.example.com/articles/1</link><description>Venkatesh, R.; Chintagunta, P.; Mahajan, V.
We propose the optimal strategies in the end-product market for manufacturers of proprietary component brands' MPCBs for short. MPCBs can pursue one of three end-product roles: sole entrant, as with 3Com that, until recently, offered its Palm operating system only as part of its own Palm handheld devices; co-optor, as with Canon, whose specialty motors go into its own and HP laser printers; component supplier, as with Intel, which has refrained from making its own PCs. Applying extant theoretical and modeling perspectives, especially from branding, spatial competition, and channels, we show that although each of the three roles has its unique domain of optimality, the co-optor role is the most widely optimal for the MPCB; it is profit maximizing even when the end products are strong (but not perfect) substitutes, an alternative component is available and the downstream market is saturated. Optimal prices under the co-optor role are higher than under the other roles. We provide an illustrative application of the model to three real-world settings.</description><author>Venkatesh, R.; Chintagunta, P.; Mahajan, V.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Machine learning for direct marketing response models: Bayesian networks with evolutionary programming</title><link>http://www.example.com/articles/1</link><description>Cui, G.; Wong, M. L.; Lui, H. K.
Machine learning methods are powerful tools for data mining with large noisy databases and give researchers the opportunity to gain new insights into consumer behavior and to improve the performance of marketing operations. To model consumer responses to direct marketing, this study proposes Bayesian networks learned by evolutionary programming. Using a large direct marketing data set, we tested the endogeneity bias in the recency, frequency, monetary value (RFM) variables using the control function approach; compared the results of Bayesian networks with those of neural networks, classification and regression tree (CART), and latent class regression; and applied a tenfold cross-validation. The results suggest that Bayesian networks have distinct advantages over the other methods in accuracy of prediction, transparency of procedures, interpretability of results, and explanatory insight. Our findings lend strong support to Bayesian networks as a robust tool for modeling consumer response and other marketing problems and for assisting management decision making.</description><author>Cui, G.; Wong, M. L.; Lui, H. K.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>E-sourcing in procurement: Theory and behavior in reverse auctions with noncompetitive contracts</title><link>http://www.example.com/articles/1</link><description>Engelbrecht-Wiggans, R.; Katok, E.
One of the goals of procurement is to establish a competitive price while affording the buyer some flexibility in selecting the suppliers to deal with. Reverse auctions do not have this flexibility, because it is the auction rules and not the buyer that determines the winner. In practice, however, hybrid mechanisms that remove some suppliers and a corresponding amount of demand from the auction market are quite common. We find that in theory such hybrid mechanisms increase competition and make buyers better off as long as suppliers are wilting to accept noncompetitive contracts. It turns out that suppliers often do because under a wide variety of conditions, these contracts have a positive expected profit. Our theory relies on two behavioral assumptions: (1) bidders in a multiunit uniform-price reverse auction will follow the dominant strategy of bidding truthfully, and (2) the suppliers who have been removed from the market will accept noncompetitive contracts that have a positive expected profit. Our experiment demonstrates that bidders in the auction behave very close to following the dominant strategy regardless of whether this auction is a stand-alone or a part of a hybrid mechanism. We also find that suppliers accept noncompetitive contracts sufficiently often (although not always) to make the hybrid mechanism outperform the reverse auction in the laboratory as well as in theory.</description><author>Engelbrecht-Wiggans, R.; Katok, E.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Downside loss aversion and portfolio management</title><link>http://www.example.com/articles/1</link><description>Jarrow, R.; Zhao, F.
Downside loss-averse preferences have seen a resurgence in the portfolio management literature. This is due to the increasing use of derivatives in managing equity portfolios and the increased use of quantitative techniques for bond portfolio management. We employ the lower partial moment as a risk measure for downside loss aversion and compare mean-variance (M-V) and mean-lower partial moment (M-LPM) optimal portfolios under nonnormal asset return distributions. When asset returns are nearly normally distributed, there is little difference between the optimal M-V and M-LPM portfolios. When asset returns are nonnormal with large left tails, we document significant differences in M-V and M-LPM optimal portfolios. This observation is consistent with industry usage of M-V theory for equity portfolios but not for fixed-income portfolios.</description><author>Jarrow, R.; Zhao, F.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risk management with benchmarking</title><link>http://www.example.com/articles/1</link><description>Basak, S.; Shapiro, A.; Tepla, L.
Portfolio theory must address the fact that, in reality, portfolio managers are evaluated relative to a benchmark, and therefore adopt risk management practices to account for the benchmark performance. We capture this risk management consideration by allowing a prespecified shortfall from a target benchmark-linked return, consistent with growing interest in such practice. In a dynamic setting, we demonstrate how a risk-averse portfolio manager optimally under- or overperforms a target benchmark under different economic conditions, depending on his attitude towards risk and choice of the benchmark. The analysis therefore illustrates how investors can achieve their desired performance profile for funds under management through an appropriate combined choice of the benchmark and money manager. We consider a variety of extensions, and also highlight the ability of our setting to shed some light on documented return patterns across segments of the money management industry.</description><author>Basak, S.; Shapiro, A.; Tepla, L.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Affect, empathy, and regressive mispredictions of others' preferences under risk</title><link>http://www.example.com/articles/1</link><description>Faro, D.; Rottenstreich, Y.
Making effective decisions under risk often requires making accurate predictions of other people's decisions under risk. We experimentally assess the accuracy of people's predictions of others' risky choices. In four studies, we find evidence of systematic inaccuracy: predictions of others' choices are too regressive. That is, people predict that others' choices will be closer to risk neutrality than those choices actually are. Where people are risk seeking, they predict that others will be risk seeking but substantially less so; likewise, where people are risk averse, they predict that others will be risk averse but substantially less so. Put differently, people predict that others' choices will reveal a more muted form of prospect theory's fourfold pattern of risk preferences than actually prevails. Two psychological concepts, the notion of risk-as-feelings and of an empathy gap, help account for regressive mispredictions. We explore several debiasing techniques suggested by these notions and also find that self-reported ratings of empathy moderate the magnitude of regressive mispredictions.</description><author>Faro, D.; Rottenstreich, Y.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A general, analytic method for generating robust strategies and narrative scenarios</title><link>http://www.example.com/articles/1</link><description>Lempert, R. J.; Groves, D. G.; Popper, S. W.; Bankes, S. C.
Robustness is a key criterion for evaluating alternative decisions under conditions of deep uncertainty However, no systematic, general approach exists for finding robust strategies using the broad range of models and data often available to decision makers. This study demonstrates robust decision making (RDM), an analytic method that helps design robust strategies through an iterative process that first suggests candidate robust strategies, identifies clusters of future states of the world to which they are vulnerable, and then evaluates the trade-offs in hedging against these vulnerabilities. This approach can help decision makers design robust strategies while also systematically generating clusters of key futures interpretable as narrative scenarios. Our study demonstrates the approach by identifying robust, adaptive, near-term pollution-control strategies to help ensure economic growth and environmental quality throughout the 21st century.</description><author>Lempert, R. J.; Groves, D. G.; Popper, S. W.; Bankes, S. C.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Confidence in imitation: Niche-width strategy in the UK automobile industry</title><link>http://www.example.com/articles/1</link><description>Rhee, M.; Kim, Y. C.; Han, J.
Our study examines the imitation behavior of UK automobile manufacturers from 1894 to 1981 and supports previous studies on interorganizational imitation by showing that manufacturers tend to imitate other manufacturers that are similar. We also find that the degree of confidence manufacturers have in their imitating behavior affects the intensity of that behavior, where an organization's confidence is determined by the variance of the routines used by its reference group and the number of firms in the reference group. Our results show that (1) manufacturers whose reference groups showed large variance in niche-width changes during the previous year are less likely to imitate (the mean of) those changes, (2) manufacturers who have large reference groups are more likely to imitate the changes, and (3) the negative effect of variance on the imitating behavior is strengthened as the number of reference organizations increases.</description><author>Rhee, M.; Kim, Y. C.; Han, J.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Does past success lead analysts to become overconfident?</title><link>http://www.example.com/articles/1</link><description>Hilary, G.; Menzly, L.
This paper provides evidence that analysts who have predicted earnings more accurately than the median analyst in the previous four quarters tend to be simultaneously less accurate and further from the consensus forecast in their subsequent earnings prediction. This phenomenon is economically and statistically meaningful. The results are robust to different estimation techniques and different control variables. Our findings are consistent with an attribution bias that leads analysts who have experienced a short-lived success to become overconfident in their ability to forecast future earnings.</description><author>Hilary, G.; Menzly, L.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The firm specificity of individual performance: Evidence from cardiac surgery</title><link>http://www.example.com/articles/1</link><description>Huckman, R. S.; Pisano, G. P.
In many settings, firms rely on independent contractors, or freelancers, for the provision of certain services. The benefits of such relationships for both firms and workers are often understood in terms of increased flexibility. Less understood is the impact of freelancing on individual performance. While it is often presumed that the performance of freelancers is largely portable across organizations, it is also possible that a given worker's performance may vary across organizations if he or she develops firm-specific skills and knowledge over time. We examine this issue empirically by considering the performance of cardiac surgeons, many of whom perform operations at multiple hospitals within narrow periods of time. Using patient mortality as an outcome measure, we find that the quality of a surgeon's performance at a given hospital improves significantly with increases in his or her recent procedure volume at that hospital but does not significantly improve with increases in his or her volume at other hospitals. Our findings suggest that surgeon performance is not fully portable across hospitals (i.e., some portion of performance is firm specific). Further, we provide preliminary evidence suggesting that this result may be driven by the familiarity that a surgeon develops with the assets of a given organization.</description><author>Huckman, R. S.; Pisano, G. P.</author><pubDate>Sat, 01 Apr 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the value of mitigation and contingency strategies for managing supply chain disruption risks</title><link>http://www.example.com/articles/1</link><description>Tomlin, Brian
We study a single-product setting in which a firm can source from two suppliers, one that is unreliable and another that is reliable but more expensive. Suppliers are capacity constrained, but the reliable supplier may possess volume flexibility. We prove that in the special case in which the reliable supplier has no flexibility and the unreliable supplier has infinite capacity, a risk-neutral firm will pursue a single disruption-management strategy: mitigation by carrying inventory, mitigation by single-sourcing from the reliable supplier, or passive acceptance. We find that a supplier's percentage uptime and the nature of the disruptions (frequent but short versus rare but long) are key determinants of the optimal strategy. For a given percentage uptime, sourcing mitigation is increasingly favored over inventory mitigation as disruptions become less frequent but longer. Further, we show that a mixed mitigation strategy (partial sourcing from the reliable supplier and carrying inventory) can be optimal if the unreliable supplier has finite capacity or if the firm is risk averse. Contingent rerouting is a possible tactic if the reliable supplier can ramp up its processing capacity, that is, if it has volume flexibility. We find that contingent rerouting is often a component of the optimal disruption-management strategy, and that it can significantly reduce the firm's costs. For a given percentage uptime, mitigation rather than contingent rerouting tends to be optimal if disruptions are rare.</description><author>Tomlin, Brian</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Up close and personal: Investor sophistication and the disposition effect</title><link>http://www.example.com/articles/1</link><description>Dhar, R.; Zhu, N.
This paper analyzes the trading records of a major discount brokerage house to investigate the disposition effect, the tendency to sell stocks that have appreciated in price (winners) sooner than stocks that trade below the purchase price (losers). In contrast to previous research that has demonstrated the disposition effect by aggregating across investors, our main objective is to identify differences in the disposition bias across individuals and explain this in terms of underlying investor characteristics. Building on the findings in experimental economics and social psychology, we hypothesize that differences in investor literacy about financial markets and trading frequency are responsible in part for the variation in individual disposition effect. Using demographic and socioeconomic variables as proxies for investor literacy, we find empirical evidence that wealthier individuals and individuals employed in professional occupations exhibit a lower disposition effect. Consistent with experimental economics, trading frequency also tends to reduce the disposition effect. We provide guidelines for investment advisors, regulators, and investment communities to utilize our findings and help investors make better decisions.</description><author>Dhar, R.; Zhu, N.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Targeted advertising strategies on television</title><link>http://www.example.com/articles/1</link><description>Gal-Or, Esther; Gal-Or, Mordechai; May, Jerrold H.; Spangler, William E.
The personal video recorder (PVR) facilitates the use of targeted advertising by allowing companies to monitor television viewing behavior and to build demographic profiles of viewers from the data that are collected. Our research explores the extent to which an advertiser should allocate resources to increase the quality of its targeting. We present a game-theoretic model that extends the conventional measurement of targeting quality by exploring the trade-off between two measures: accuracy and recognition. Accuracy measures the likelihood that any target segment prediction is correct, while recognition conversely measures the likelihood that any member of the target segment is identified. We find that the relative resources allocated to improving accuracy and recognition depend upon the size of the population of viewers, the propensity of viewers to skip commercials, the overall cost of airing commercials, and the competitive environment. Furthermore, the incentives to improve accuracy are markedly different from those to improve recognition. Although improving accuracy does not affect the extent of price competition, improving recognition leads to intensified price competition and reduced profitability in the product market. Thus, when facing a competitor that pursues a strategy to improve its recognition of potential customers, an advertiser should choose to reduce its investment in recognition and increase its investment in accuracy.</description><author>Gal-Or, Esther; Gal-Or, Mordechai; May, Jerrold H.; Spangler, William E.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Selectively acquiring customer information: A new data acquisition problem and an active learning-based solution</title><link>http://www.example.com/articles/1</link><description>Zheng, Zhiqiang; Padmanabhan, Balaji
This paper presents a new information acquisition problem motivated by business applications where customer data has to be acquired with a specific modeling objective in mind. In the last two decades, there has been substantial work in two different fields-optimal experimental design and machine learning-that has addressed the issue of acquiring data in a selective manner with a specific objective in mind. We show that the problem presented here is different from the classic model-based data acquisition problems considered thus far in the literature in both fields. Building on work in optimal experimental design and in machine learning, we develop a new active learning technique for the information acquisition problem presented in this paper. We demonstrate that the proposed method performs well based on results from applying this method across 20 Web usage and machine learning data sets.</description><author>Zheng, Zhiqiang; Padmanabhan, Balaji</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic catalog mailing policies</title><link>http://www.example.com/articles/1</link><description>Simester, Duncan I.; Sun, Peng; Tsitsiklis, John N.
Deciding who should receive a mail-order catalog is among the most important decisions that mail-order-catalog firms must address. In practice, the current approach to the problem is invariably myopic: firms send catalogs to customers who they think are most likely to order from that catalog. In doing so, the firms overlook the long-run implications of these decisions. For example, it may be profitable to mail to customers who are unlikely to order immediately if sending the current catalog increases the probability of a future order. We propose a model that allows firms to optimize mailing decisions by addressing the dynamic implications of their decisions. The model is conceptually simple and straightforward to implement. We apply the model to a large sample of historical data provided by a catalog firm and then evaluate its performance in a large-scale field test. The findings offer support for the proposed model but also identify opportunities for further improvement.</description><author>Simester, Duncan I.; Sun, Peng; Tsitsiklis, John N.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The contingent effects of transactive memory: When is it more beneficial to know what others know?</title><link>http://www.example.com/articles/1</link><description>Ren, Y. Q.; Carley, K. M.; Argote, L.
Previous studies have provided evidence of the positive impact of transactive memory (TM) on group performance, such as the efficient storage and recall of knowledge and better product quality. This paper aims to unify the experimental research on TM and to extend it to more dynamic and diverse group settings. In this paper, we develop an empirically grounded computational model-ORGMEM-and apply it to explore the contingent effects of TM on group performance. The comparison between virtual experimental results and relevant laboratory experimental results demonstrates the validity of ORGMEM as a useful tool to study memory-related phenomena. Through a series of virtual experiments, we find that TM decreases group response time by facilitating knowledge retrieval processes and improves decision quality by informing task coordination and evaluation. Our results also suggest that the effects of TM are contingent upon group characteristics, such as group size and environment, as well as the dimension along which group performance is assessed. Overall, TM seems to be more beneficial to small groups using quality as the dependent variable, but more beneficial to large groups, groups in a dynamic task environment, and groups in a volatile knowledge environment using time as the dependent variable.</description><author>Ren, Y. Q.; Carley, K. M.; Argote, L.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Data shuffling - A new masking approach for numerical data</title><link>http://www.example.com/articles/1</link><description>Muralidhar, Krishnamurty; Sarathy, Rathindra
This study discusses a new procedure for masking confidential numerical data-a procedure called data shuffling-in which the values of the confidential variables are "shuffled" among observations. The shuffled data provides a high level of data utility and minimizes the risk of disclosure. From a practical perspective, data shuffling overcomes reservations about using perturbed or modified confidential data because it retains all the desirable properties of perturbation methods and performs better than other masking techniques in both data utility and disclosure risk. In addition, data shuffling can be implemented using only rank-order data, and thus provides a nonparametric method for masking. We illustrate the applicability of data shuffling for small and large data sets.</description><author>Muralidhar, Krishnamurty; Sarathy, Rathindra</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Joint production and pricing decisions with setup costs and capacity constraints</title><link>http://www.example.com/articles/1</link><description>Deng, Shiming; Yano, Candace A.
We consider the problem of setting prices and choosing production quantities for a single product over a finite horizon for a capacity-constrained manufacturer facing price-sensitive demands. There is a fixed cost per production run and a variable cost per unit produced, both of which may vary by period. We characterize properties of the optimal solution, considering cases with constant and time-varying capacity, and with and without speculative motive for holding inventory. We show that, counter to intuition, optimal prices may increase as the capacity increases, even when capacity is constant over the horizon. We also show that increases in capacity do not always exhibit diminishing marginal returns. Our procedure produces solutions with much higher profits than can be obtained from a sophisticated sequential procedure in which a well-informed and optimum-seeking marketing department makes pricing decisions and the manufacturing department seeks to satisfy the resulting demands at minimum cost. Our results also suggest that firms with seasonal demand and tight capacity constraints should be more aggressive in setting prices to manage their demands than what is typically done in practice. Finally; we discuss how a decision maker can use our procedure as an aid in solving multiproduct versions of the problem.</description><author>Deng, Shiming; Yano, Candace A.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Interdependency, competition, and the distribution of firm and industry profits</title><link>http://www.example.com/articles/1</link><description>Lenox, Michael J.; Rockart, Scott F.; Lewin, Arie Y.
Coordination of interdependencies among firms' productive activities has been advanced as a promising explanation for sustained heterogeneity in capabilities among firms. In this paper, the extend this line of research to determine the industry structures and patterns of expected firm profits for the case when difficulty optimizing interdependent activities does, in fact, generate and sustain capability heterogeneity among firms. We combine a widely used agent-based model where firms search to discover sets of activities that complement one another (reducing overall costs or raising product quality) with traditional economic models of competition among profit-maximizing firms. The agent-based model produces a distribution of performance (interpreted as variable cost or product quality) among firms and the competition models determine resulting industry outcomes including patterns of entry, exit, and profits. The integration of economic models of competition among firms with an agent-based model of search for improvement by firms reveals a rich relationship between interdependencies in production functions and industry structure, firm profits, and industry average profitability.</description><author>Lenox, Michael J.; Rockart, Scott F.; Lewin, Arie Y.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of new franchisor partnering strategies on franchise system size</title><link>http://www.example.com/articles/1</link><description>Shane, Scott; Shankar, Venkatesh; Aravindakshan, Ashwin
Many young firms use strategic actions to attract partners who help them increase the size of their operations quickly. This article examines the use of strategic actions to attract partners and increase system size in the context of franchising. We build on research in entrepreneurship, marketing, organization theory, strategic management, and finance to develop specific hypotheses about the influences of franchisor pricing policy and strategic control decisions on system size. We test these hypotheses empirically, using panel data on a sample of 1,292 business format franchise systems from 152 industries that were established in the United States between 1979 and 1996 and followed from their inception forward in time. Our model accounts for the endogeneity of strategic decisions, controls for unobserved firm and industry factors, and accounts for selection effects due to system failure. The results show that franchisors that grow larger (1) lower royalty rates as the systems age, (2) have low up-front franchise fees that rise over time, (3) own a small proportion of outlets and lower that percentage over time, (4) keep franchisees' initial investment low, and (5) finance their franchisees.</description><author>Shane, Scott; Shankar, Venkatesh; Aravindakshan, Ashwin</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal mechanisms with finite agent types</title><link>http://www.example.com/articles/1</link><description>Lovejoy, W. S.
In some mechanism design problems, finite-type spaces may be natural ones to consider, yet the current literature is dominated by analyses of continuous-type spaces. This probably derives from the intellectual dominance of the early work in this area. Here we present an analysis of the finite-state case that unifies and generalizes current understanding of these problems. We analyze general quasi-linear utility functions among asymmetric agents with an arbitrary number of finite types, in the context of incentive-compatible direct revelation games. A key part of the analysis is the relationship between expected benefit functions that feature alternative forms of supermodularity that translate into relaxations or restrictions of the original problem. The required features can be attained with a range of assumptions on the model primitives, each of which can support the results. This unified approach can suggest a range of alternative assumption combinations, often more general than their counterparts in the continuous-space literature, and each of which can reduce the problem to a more tractable form. Also, mechanism design problems with finite-type spaces can require conscious attention to how the principal handles ties, which are probability-zero events, and hence innocuous in continuous spaces.</description><author>Lovejoy, W. S.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A note on feedback sequential equilibria in a Lanchester model with empirical application</title><link>http://www.example.com/articles/1</link><description>Breton, M.; Jarrar, R.; Zaccour, G.
We study in this paper dynamic equilibrium advertising strategies in a duopoly with asymmetric information structure and sequential play. The advertising model of Lanchester is used in a game where the relevant solution concept is feedback Stackelberg equilibrium, which is subgame perfect. An algorithm is devised for the computation of this equilibrium, and numerical results are reported and discussed. Using a data set from the cola market, we obtain the resulting advertising strategies and provide a comparison with closed-loop and open-loop Nash equilibria.</description><author>Breton, M.; Jarrar, R.; Zaccour, G.</author><pubDate>Mon, 01 May 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Static mean-variance analysis with uncertain time horizon</title><link>http://www.example.com/articles/1</link><description>Martellini, L.; Urosevic, B.
We generalize Markowitz analysis to the situations involving an uncertain exit time. Our approach preserves the form of the original problem in that an investor minimizes portfolio variance for a given level of the expected return. However, inputs are now given by the generalized expressions for mean and variance-covariance matrix involving moments of the random exit time in addition to the conditional moments of asset returns. Although efficient frontiers in the generalized and the standard Markowitz case may coincide under certain conditions, we demonstrate that, by means of an example, in general that is not true. In particular, portfolios efficient in the standard Markowitz sense can be inefficient in the generalized sense and vice versa. As a result, an investor facing an uncertain time horizon and investing as if her time of exit is certain would in genera I make suboptimal portfolio allocation decisions. Numerical simulations show that a significant efficiency loss can be induced by an improper use of standard mean-variance analysis when time horizon is uncertain.</description><author>Martellini, L.; Urosevic, B.</author><pubDate>Thu, 01 Jun 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Value implications of investments in information technology</title><link>http://www.example.com/articles/1</link><description>Anderson, Mark C.; Banker, Rajiv D.; Ravindran, Sury
The year 2000 (Y2K) countdown provided a uniquely visible instance of spending on information technology (IT) by U.S. companies. With public attention riveted on potential Y2K malfunctions, managers were forced to evaluate their IT and make decisions about whether to modify or replace existing systems. In the aftermath of Y2K, critics charged that the problem was overblown and that companies overspent on IT. In contrast, we posit in this paper that efforts companies made to renew and upgrade their IT may have positioned them to take advantage of new e-business applications. As Y2K approached, managers could invest opportunistically in IT, which would enable them to connect with customers and suppliers in new ways. Contrary to the alleged overspending, we find that firm value increased, on average, with Y2K spending by Fortune 1000 companies. In particular, higher firm value and subsequent earnings were associated with Y2K spending for firms in industries where IT was considered to have a transforming influence-altering traditional ways of doing business by redefining business processes and relationships. We also test whether the positive association between firm value and Y2K spending diminished with Y2K spending by industry peer firms, but we do not find support for this relative investment hypothesis.</description><author>Anderson, Mark C.; Banker, Rajiv D.; Ravindran, Sury</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Opportunity recognition as the detection of meaningful patterns: Evidence from comparisons of novice and experienced entrepreneurs</title><link>http://www.example.com/articles/1</link><description>Baron, Robert A.; Ensley, Michael D.
It is suggested that the recognition of new business opportunities often involves pattern recognition-the cognitive process through which individuals identify meaningful patterns in complex arrays of events or trends. Basic research on pattern recognition indicates that cognitive frameworks acquired through experience (e.g., prototypes) play a central role in this process. Such frameworks provide individuals with a basis for noticing connections between seemingly independent events or trends (e.g., advances in technology, shifts in markets, changes in government policies, etc.), and for detecting meaningful patterns in these connections. We propose that ideas for new products or services often emerge from the perception of such patterns. New business opportunities are identified when entrepreneurs, using relevant cognitive frameworks, "connect the dots" between seemingly unrelated events or trends and then detect patterns in these connections suggestive of new products or services. To obtain evidence on these proposals, we compared the "business opportunity" prototypes of novice (first-time) and repeat (experienced) entrepreneurs-their cognitive representations of the essential nature of opportunities. As predicted, the prototypes of experienced entrepreneurs were more clearly defined, richer in content, and more concerned with factors and conditions related to actually starting and running a new venture (e.g., generation of positive cash flow) than the prototypes of novice, entrepreneurs. These findings offer support for the view that pattern recognition is a key component of opportunity recognition.</description><author>Baron, Robert A.; Ensley, Michael D.</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Entrepreneurial risk and market entry</title><link>http://www.example.com/articles/1</link><description>Wu, Brian; Knott, Anne Marie
This paper attempts to reconcile the risk-bearing characterization of entrepreneurs with the stylized fact that entrepreneurs exhibit conventional risk-aversion profiles. We propose that the disparity arises from confounding two distinct dimensions of uncertainty: demand uncertainty and ability uncertainty. We further propose that entrepreneurs will be risk averse with respect to demand uncertainty, yet "apparent risk seeking" (or overconfident) with respect to ability uncertainty. To examine this view, we construct a reduced-form model of the entrepreneur's entry decision, which we aggregate to the market level, then test empirically. We find that entrepreneurs in aggregate behave as we predict. Accordingly, risk-averse entrepreneurs are willing to bear market risk when the degree of ability uncertainty is comparable to the degree of demand uncertainty. Potential market failures exist in instances where there is a high demand uncertainty but low performance dispersion (insufficient entry), or low demand uncertainty but high performance dispersion (excess entry).</description><author>Wu, Brian; Knott, Anne Marie</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risk assessment for banking systems</title><link>http://www.example.com/articles/1</link><description>Elsinger, Helmut; Lehar, Alfred; Summer, Martin
We propose a new approach to assess systemic financial stability of a banking system using standard tools from modern risk management in combination with a network model of interbank loans. We apply our model to a unique data set of all Austrian banks. We find that correlation in banks' asset portfolios dominates contagion as the main source of systemic risk. Contagion is rare but can nonetheless wipe out a major part of the banking system. Low bankruptcy costs and an efficient crisis resolution policy are crucial to limit the systemwide impact of contagious default events. We compute the "value at risk" for a lender of last resort and find that the funds necessary to prevent contagion are surprisingly small.</description><author>Elsinger, Helmut; Lehar, Alfred; Summer, Martin</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Airlines as baseball players: Another approach for evaluating an equal-safety hypothesis</title><link>http://www.example.com/articles/1</link><description>Czerwinski, David; Barnett, Arnold
Within the First World, observed differences across air carriers in passenger death risk are almost never statistically significant. But given the rarity of fatal crashes, even a lopsided split of crashes across airlines is unlikely to achieve significance. For greater perspective about relative airline safety performance, we supplemented an analysis of overall mortality data with a two-part statistical procedure. The first part-which was inspired by the way the Oakland Athletics Major League baseball team evaluates individual baseball players-allows for a sharp test of a key aspect of an equal-safety hypothesis, related to the frequency with which individual airlines suffer life-threatening events. The second test considers a further aspect of safety, namely the ability to recover from emergencies given that they have occurred. When applied to U.S. domestic jet services over 1983-2002, the test procedures indicate that long-established airlines have been about equally (and hugely) successful in protecting their passengers from accidental death and that established airlines are not demonstrably safer than the "new-entrant" jet carriers formed after U.S. airline deregulation.</description><author>Czerwinski, David; Barnett, Arnold</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Project assignments when budget padding taints resource allocation</title><link>http://www.example.com/articles/1</link><description>Arya, Anil; Mittendorf, Brian
This paper shows that rotation programs can be an effective response to concerns of employee budget padding. Rotation programs naturally create a "portfolio" of assignments for each manager, and the resulting diversification can reduce the downside of resource rationing. In particular, the production versus rents trade-off linked with adverse selection problems can be more efficiently carried out when the firm faces two managers with average information advantages, rather than one with a large advantage and one with a small advantage. Roughly stated, rotation of project assignments is a way of smoothing information across managers. On the other hand, if a firm places a premium on treating different types of projects in distinct ways, specialized assignments can be preferred due to the ability to confine project types to individual managers.</description><author>Arya, Anil; Mittendorf, Brian</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Market valuation and employee stock options</title><link>http://www.example.com/articles/1</link><description>Zhang, Ge
This paper investigates a market-valuation-based hypothesis for employee stock options (ESOs). Given that stock prices do not track fundamental values perfectly, I show that ESOs can be used to sell overvalued stocks and to increase long-term shareholder value: The key cross-sectional prediction of the valuation rationale is that the conditional probability of granting options to employees and the amount of options granted to them are positively correlated with market valuation and volatility. Moreover, for extremely overvalued firms, the correlation between option grant and market valuation is weaker. Firms that use ESOs can save their regular employee compensation costs. I find strong empirical evidence supporting these predictions.</description><author>Zhang, Ge</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Methodologies and algorithms for group-rankings decision</title><link>http://www.example.com/articles/1</link><description>Hochbaum, Dorit S.; Levin, Asaf
The problem of group ranking, also known as rank aggregation, has been studied in contexts varying from sports, to multicriteria decision making, to machine learning, to ranking Web pages, and to behavioral issues. The dynamics of the group aggregation of individual decisions has been a subject of central importance in decision theory. We present here a new paradigm using an optimization framework that addresses major shortcomings that exist in current models of group ranking. Moreover, the framework provides a specific performance measure for the quality of the aggregate ranking as per its deviations from the individual decision-makers' rankings. The new model for the group-ranking problem presented here is based on rankings provided with intensity-that is, the degree of preference is quantified. The model allows for flexibility in decision protocols and can take into consideration imprecise beliefs, less than full confidence in some of the rankings, and differentiating between the expertise of the reviewers. Our approach relaxes frequently made assumptions of: certain beliefs in pairwise rankings; homogeneity implying equal expertise of all decision makers with respect to all evaluations; and full list requirement according to which each decision maker evaluates and ranks all objects. The option of preserving the ranks in certain subsets is also addressed in the model here. Significantly, our model is a natural extension and generalization of existing models, yet it is solvable in polynomial time. The group-rankings models are linked to network flow techniques.</description><author>Hochbaum, Dorit S.; Levin, Asaf</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Enhancing a branch-and-bound algorithm for two-stage stochastic integer network design-based models</title><link>http://www.example.com/articles/1</link><description>Andrade, Rafael; Lisser, Abdel; Maculan, Nelson; Plateau, Gerard
In this paper we present branch-and-bound (B&amp;B) strategies for two-stage stochastic integer network design-based models with integrality constraints in the first-stage variables. These strategies are used within L-shaped decomposition-based B&amp;B framework. We propose a valid inequality in order to improve B&amp;B performance. We use this inequality to implement a multirooted B&amp;B tree. A selective use of optimality cuts is explored in the B&amp;B approach and we also propose a subgradient-based technique for branching on 0-1 feasible solutions. Finally, we present computational results for a fixed-charge network design problem with random demands.</description><author>Andrade, Rafael; Lisser, Abdel; Maculan, Nelson; Plateau, Gerard</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Sequential observation and selection with rank-dependent payoffs: An experimental study</title><link>http://www.example.com/articles/1</link><description>Bearden, J. Neil; Rapoport, Amnon; Murphy, Ryan O.
We consider a class of sequential observation and selection decision problems in which applicants are interviewed one at a time, decision makers only learn the applicant's quality relative to the applicants that have been interviewed and rejected, only a single applicant is selected, and payoffs increase in the absolute quality of the selected applicant. Compared to the optimal decision policy, which we compute numerically, results from two experiments show that subjects terminated their search too early. We competitively test three behavioral decision rules and find that a multithreshold rule, which has the same form as the optimal decision policy but is parameterized differently, best accounts for the data. Results from a probability estimation task show that subjects tend to overestimate the absolute quality of early applicants and give insufficient consideration to the yet-to-be-seen applicants.</description><author>Bearden, J. Neil; Rapoport, Amnon; Murphy, Ryan O.</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Discounting by intervals: A generalized model of intertemporal choice</title><link>http://www.example.com/articles/1</link><description>Scholten, Marc; Read, Daniel
According to most models of intertemporal choice, an agent's discount rate is a function of how far the outcomes are removed from the present, and nothing else. This view has been challenged by recent studies, which show that discount rates tend to be higher the closer the outcomes are to one another (subadditive discounting) and that this can give rise to intransitive intertemporal choice. We develop and test a generalized model of intertemporal choice, the Discounting By Intervals (DBI) model, according to which the discount rate is a function of both how far outcomes are removed from the present and how far the outcomes are removed from one another. The model addresses past challenges to other models, most of which it includes as special cases, as well as the new challenges presented in this paper: Our studies show that when the interval between outcomes is very short, discount rate tends to increase with interval length (superadditive discounting). In the discussion we place our model and evidence in a broader theoretical context.</description><author>Scholten, Marc; Read, Daniel</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Stochastic dominance and cumulative prospect theory</title><link>http://www.example.com/articles/1</link><description>Baucells, Manel; Heukamp, Franz H.
We generalize and extend the second-order stochastic dominance condition for expected utility to cumulative prospect theory. The new definitions include preferences represented by S-shaped value functions, inverse S-shaped probability weighting functions, and loss aversion. The stochastic dominance conditions supply a framework to test different features of cumulative prospect theory. In the experimental part of the paper, we offer a test of several joint hypotheses on the value function and the probability weighting function. Assuming empirically relevant weighting functions, we can reject the inverse S-shaped value function recently advocated by Levy and Levy (2002) in favor of the S-shaped form. In addition, we find generally supporting evidence for loss aversion. Violations of loss aversion can be explained by subjects using the overall probability of winning as a heuristic.</description><author>Baucells, Manel; Heukamp, Franz H.</author><pubDate>Fri, 01 Sep 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exact solutions to task allocation problems</title><link>http://www.example.com/articles/1</link><description>Ernst, Andreas; Jiang, Houyuan; Krishnamoorthy, Mohan
The task allocation problem (TAP) is one where a number of tasks or modules need to be assigned to a set of processors or machines at minimum overall cost. The overall cost includes the communication cost between tasks that are assigned to different processors and other costs such as the assignment cost and the fixed cost of using processors. Processors may have limited or unlimited capacities to perform tasks. Task allocation has been applied to the design of distributed computing systems and also in auto-manufacturing contexts. We present several integer programs and a column generation formulation for the uncapacitated and the capacitated TAP. Computational experiments are carried out to demonstrate computational capabilities of integer programming and the column generation formulations for the uncapacitated TAP (UTAP). Excellent results are obtained for the column generation formulation. We also report some computational experience for the capacitated TAP (CTAP).</description><author>Ernst, Andreas; Jiang, Houyuan; Krishnamoorthy, Mohan</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theory of banks' industry expertise, market power, and credit risk</title><link>http://www.example.com/articles/1</link><description>Stomper, Alex
The author analyzes banks' incentives to acquire expertise in judging the creditworthiness of borrowers in an industry with uncertain business conditions. The analysis shows that industry expertise enables banks to extract rents proportional to their exposure to industry-specific credit risk. This exposure is in turn determined by the number of banks aiming to focus on lending to an industry. In equilibrium, the industry receives funding from a limited number of banks with industry expertise, as well as from a competitive fringe of financiers without such expertise. The equilibrium yields testable predictions about the concentration of bank lending to an industry, and about the correlation between this concentration and the recovery rates, default rates, and interest rates of bank loans.</description><author>Stomper, Alex</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A data disclosure policy for count data based on the COM-Poisson distribution</title><link>http://www.example.com/articles/1</link><description>Kadane, Joseph B.; Krishnan, Ramayya; Shmueli, Galit
Count data arise in various organizational settings. When the release of such data is sensitive, organizations need information-disclosure policies that protect data confidentiality while still providing data access. In contrast to extant disclosure policies, we describe a new policy for count tables that is based on disclosing only the sufficient statistics of a flexible discrete distribution. This distribution, the COM-Poisson, well approximates Poisson counts but also under- and over-dispersed counts. The sufficient statistics mask the exact cell counts and often also the table size. Under the scenario of a data holding agency and a data snooper, we show that this policy has low disclosure risk with no loss of data utility: Usually, many count tables correspond to the disclosed sufficient statistics. Furthermore, these count tables are equally likely to be the undisclosed table. Finding these solutions requires solving a system of linear equations, which are underdetermined for tables with more than three cells, and can be computationally prohibitive for even small tables. We also consider cell-specific interval bounds, a commonly used disclosure limitation policy, and compare them to our policy. We describe several types of snooper knowledge, their integration with the disclosed statistics, and implications. Applying this policy to three real data sets, we illustrate the low associated disclosure risk.</description><author>Kadane, Joseph B.; Krishnan, Ramayya; Shmueli, Galit</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring cross-category price effects with aggregate store data</title><link>http://www.example.com/articles/1</link><description>Song, Inseong; Chintagunta, Pradeep K.
Our objective is to understand the cross-category effects of marketing activities using aggregate store-level scanner data. For this, we provide a framework derived from household utility maximizing behavior which assumes that a household chooses the "bundle" of products with the highest utility. We use a second-order Taylor series approximation to an arbitrary utility function to represent bundle utility. Aggregate sales or shares in each category are derived under the assumption that households are heterogeneous in their preferences and in their sensitivities to marketing activities. Our estimation accounts for potential price endogeneity in demand. Using store-level scanner data on four product categories-liquid laundry detergents, powdered laundry detergents, liquid fabric softeners, and sheet fabric softeners-we find evidence for a complementary relationship between liquid softeners and both forms of detergents. We also find that the magnitude of cross-category elasticities are brand specific, i.e., different brands in a category have a different price impact on the demand for a brand in another category. The results have implications for retailers in terms of the potential need for cross-category management, as well as for manufacturers such as Procter &amp; Gamble that participate in all four categories. We compare our model with a log-log regression specification on three criteria-estimated elasticities, hold-out sample predictions, and retailer cross-category pricing. We find that the proposed model produces more reasonable estimates relative to the log-log model; it predicts better and is more useful for pricing purposes. Further, in a simulation study, we show that our proposed model can recover the elasticities from a data-generating process that simulates household-level joint outcomes across categories even after these data have been aggregated to brand-level shares within each category By contrast, the log-log regression model is unable to do so.</description><author>Song, Inseong; Chintagunta, Pradeep K.</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic manipulation of Internet opinion forums: Implications for consumers and firms</title><link>http://www.example.com/articles/1</link><description>Dellarocas, Chrysanthos
There is growing evidence that consumers are influenced by Internet-based opinion forums before making a variety of purchase decisions. Firms whose products are being discussed in such forums are therefore tempted to manipulate consumer perceptions by posting costly anonymous messages that praise their products. This paper offers a theoretical analysis of the impact of such behavior on firm profits and consumer surplus. There are three main results. First, if every firm's manipulation strategy is a monotonically increasing (decreasing) function of that firm's true quality, strategic manipulation of online forums increases (decreases) the information value of a forum to consumers. This result implies the existence of settings where online forum manipulation benefits consumers. Second, equilibria where strategies are monotonically increasing (decreasing) functions of a firm's true quality exist in settings where the firm's net payoff function, inclusive of the cost of manipulation, is supermodular (submodular) in the firm's quality and manipulation action. Third, in a broad class of settings, if the precision of honest consumer opinions that firms manipulate is sufficiently high, firms of all types, as well as society, would be strictly better off if manipulation of online forums was not possible. Nonetheless, firms are locked into a "rat race" and forced to spend resources on such profit-reducing activities; if they don't, consumer perceptions will be biased against them. The social cost of online manipulation can be reduced by developing "filtering" technologies that make it costlier for firms to manipulate. Interestingly, as the amount of user-contributed online content increases, it is firms, and not consumers, that have most to gain from the development of such technologies.</description><author>Dellarocas, Chrysanthos</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The process of innovation assimilation by firms in different countries: A technology diffusion perspective on e-business</title><link>http://www.example.com/articles/1</link><description>Zhu, Kevin; Kraemer, Kenneth L.; Xu, Sean
This paper extends our previous studies on the assimilation of Internet-based e-business innovations by firms in an international setting. Drawing upon theories on the process and contexts of technology diffusion, we develop an integrative model to examine three assimilation stages: initiation -&gt; adoption -&gt; routinization. The model features technological, organizational, and environmental contexts as prominent antecedents of this three-stage assimilation process. Based on this model, we hypothesize how technology readiness, technology integration, firm size, global scope, managerial obstacles, competition intensity, and regulatory environment influence e-business assimilation at the firm level. A unique data set of 1,857 firms from 10 countries is used to test the conceptual model and hypotheses. To probe deeper into the influence of the environmental context, we compare two subsamples from developed and developing countries. Our empirical analysis leads to several key findings: (1) Competition positively affects initiation and adoption, but negatively impacts routinization, suggesting that too much competition is not necessarily good for technology assimilation because it drives firms to chase the latest technologies without learning how to use existing ones effectively. (2) Large firms tend to enjoy resource advantages at the initiation stage, but have to overcome structural inertia in later stages. (3) We also find that economic environments shape innovation assimilation: Regulatory environment plays a more important role in developing countries than in developed countries. Moreover, while technology readiness is the strongest factor facilitating assimilation in developing countries, technology integration turns out to be the strongest in developed countries, implying that as e-business evolves, the key determinant of its assimilation shifts from accumulation to integration of technologies. Together, these findings offer insights into how innovation assimilation is influenced by contextual factors, and how the effects may vary across different stages and in different environments.</description><author>Zhu, Kevin; Kraemer, Kenneth L.; Xu, Sean</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Characterization of the bullwhip effect in, linear, time-invariant supply chains: Some formulae and tests</title><link>http://www.example.com/articles/1</link><description>Ouyang, Yanfeng
The authors analyze the bullwhip effect in multistage, decentralized supply chains operated with linear and time-invariant inventory management policies; the focus is on robustness. The supply chain is modeled as a single-input, single-output control system driven by arbitrary customer demands. The authors derive robust analytical conditions to predict the presence of the bullwhip effect and bound its magnitude, based only on the way inventories are managed. These results hold independently of the customer demand. The authors also characterize the stream of orders placed at any stage of the chain when the customer demand process is known and ergodic and give an exact formula for the variance of the orders placed. This formula generalizes existing results by broadening the class of inventory replenishment policies and customer demand processes to which it applies. The authors also show that the bullwhip effect can be mitigated by introducing commitments for future orders into the ordering policies.</description><author>Ouyang, Yanfeng</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Assortment planning and inventory decisions under a locational choice model</title><link>http://www.example.com/articles/1</link><description>Gaur, Vishal; Horthon, Dorothee
We consider a single-period assortment planning and inventory management problem for a retailer, using a locational choice model to represent consumer demand. We first determine the optimal variety, product location, and inventory decisions under static substitution, and show that the optimal assortment consists of products equally spaced out such that there is no substitution among them regardless of the distribution of consumer preferences. The optimal solution can be such that some customers prefer not to buy any product in the assortment, and such that the most popular product is not offered. We then obtain bounds on profit when customers dynamically substitute, using the static substitution for the lower bound, and a retailer-controlled substitution for the upper bound. We thus define two heuristics to solve the problem under dynamic substitution and numerically evaluate their performance. This analysis shows the value of modeling dynamic substitution and identifies conditions in which the static substitution solution serves as a good approximation.</description><author>Gaur, Vishal; Horthon, Dorothee</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Partnership in a dynamic production system with unobservable actions and noncontractible output</title><link>http://www.example.com/articles/1</link><description>Plambeck, Erica L.; Taylor, Terry A.
This paper considers two firms that engage in joint production. The prospect of repeated interaction introduces dynamics, in that actions that firms take today influence the costliness and effectiveness of actions in the future. Repeated interaction also facilitates the use of informal agreements (relational contracts) that are sustained not by the court system, but by the ongoing value of the relationship. We characterize the optimal relational contract in this dynamic system with double moral hazard. We show that an optimal relational contract has a simple form that does not depend on the past history. The optimal relational contract may require that the firms terminate their relationship with positive probability following poor performance. We show how process visibility, which allows the firms to better assess who is at fault, can substantially improve system performance. The degree to which process visibility eliminates the need for termination depends on the nature of the dynamics: If the buyer's action does not influence the dynamics, the need for termination is eliminated; otherwise, termination may be required.</description><author>Plambeck, Erica L.; Taylor, Terry A.</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Transshipment of inventories among retailers: Myopic vs. farsighted stability</title><link>http://www.example.com/articles/1</link><description>Sosic, Greys
The author considers a model of a decentralized distribution system consisting of n retailers selling an identical product. Retailers face a stochastic demand and must order their initial inventory before this demand is realized. After demand realization, retailers decide how much of their unsold inventory or unsatisfied demand they want to share with other retailers. This is followed by a transshipment of leftover inventories and distribution of the additional profit generated through inventory sharing. The author addresses the following issue: Suppose that the retailers distribute the profit from inventory sharing according to an allocation rule that induces retailers to share their residuals in a way that maximizes the additional profit, such as the Shapley value, but may not belong to the core. Is it likely that, in this framework, all retailers will jointly share their residuals and not form subcoalitions? Previous research has looked at this problem from a myopic viewpoint and concluded that the grand coalition is not stable. Unlike the prior work, the author looks at stability in a farsighted sense. That is, retailers do not consider only their immediate payoffs but are also concerned with reactions of other retailers to their actions. In a symmetric setting, with identical additional unit revenues from transshipments generated by all retailers, farsighted retailers always maximize their allocations by not defecting from the grand coalition. The author also provides conditions when the same is true for nonidentical additional unit revenues.</description><author>Sosic, Greys</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordinating supply chains with simple pricing schemes: The role of vendor-managed inventories</title><link>http://www.example.com/articles/1</link><description>Bernstein, Fernando; Chen, Fangruo; Federgruen, Awi
We characterize supply chain settings in-which perfect coordination can be achieved with simple wholesale pricing schemes: either retailer-specific constant unit wholesale prices or retailer-specific volume discount schemes. We confine ourselves to two-echelon supply chains with a single supplier servicing a network of retailers who compete with each other by selecting sales quantities. We identify a key sufficient condition, in terms of interdependencies between chain members' operational decisions, under which perfect coordination via simple schemes is feasible, under general cost and demand functions. This condition, which we refer to as echelon operational autonomy (EOA), states that the costs incurred by the supplier for a given vector of sales volumes depends only on operational decisions she controls herself. At the same time, the costs incurred by the retailers may depend on operational decisions controlled by the supplier, in which case, the supplier's operational decisions are made to minimize chainwide costs. We show how vendor-managed inventory (VMI) partnerships create EOA and compare the resulting coordinating pricing schemes with those required in a traditional decentralized setting (without EOA). We also discuss compliance issues with the coordinating schemes in view of the Robinson-Patman act and provide remedies to overcome these issues.</description><author>Bernstein, Fernando; Chen, Fangruo; Federgruen, Awi</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Vendor certification and appraisal: Implications for supplier quality</title><link>http://www.example.com/articles/1</link><description>Hwang, Iny; Radhakrishnan, Suresh; Su, Lixin
We examine the buyer's problem of inducing the supplier's quality effort using two arrangements: the appraisal regime and the certification regime. In the appraisal regime, the buyer inspects the units supplied and either charges a penalty for defective units identified during inspection or pays the unit price for good units. In the certification regime, the supplier obtains vendor certification and the buyer pays the unit price for all units supplied. The inspection technology and the certification process provide noisy information on the supplier's quality effort. In the appraisal regime, the buyer implements the supplier's high-quality and low-inspection. The supplier's expected profit is greater than his reservation profit because of an additional agency cost: The buyer has to prevent the supplier from performing unwanted/preemptive inspection (which gives rise to indirect costs from delay, etc.). This additional agency cost arises precisely when the effectiveness of inspection is high. This provides a moral-hazard-based rationale for the increasing use of certification (such as ISO 9000) in spite of (in fact, because of) the increasing effectiveness of inspection. The potential for additional agency cost incurred by the buyer in the appraisal regime highlights an indirect cost associated with inspection.</description><author>Hwang, Iny; Radhakrishnan, Suresh; Su, Lixin</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Modeling the "pseudodeductible" in insurance claims decisions</title><link>http://www.example.com/articles/1</link><description>Braun, Michael; Fader, Peter S.; Bradlow, Eric T.; Kunreuther, Howard
In many different managerial contexts, consumers "leave money on the table" by, for example, their failure to claim rebates, use available coupons, and so on. This project focuses on a related problem faced by homeowners who may be reluctant to file insurance claims despite the fact their losses are covered. We model this consumer decision by introducing the concept of the "pseudodeductible," a latent threshold above the policy deductible that governs the homeowner's claim behavior. In addition, we show how the observed number of claims can be modeled as the output of three stochastic processes that are separately, and in conjunction, managerially relevant: the rate at which losses occur, the size of each loss, and the choice of the individual to file or not file a claim. By allowing for the possibility of pseudodeductibles, one can sort out (and make accurate inferences about) these three processes. We test this model using a proprietary data set provided by State Farm, the largest underwriter of personal lines insurance in the United States. Using mixtures of Dirichlet processes to capture heterogeneity and the interplay among the three processes, we uncover several relevant "stories" that underlie the frequency and severity of claims. For instance, some customers have a small number of losses, but all are filed as claims, whereas others may experience many more losses, but are more selective about which claims they file. These stories explain several observed phenomena regarding the claims decisions that insurance customers make, and have broad implications for customer lifetime value and market segmentation.</description><author>Braun, Michael; Fader, Peter S.; Bradlow, Eric T.; Kunreuther, Howard</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Tainted knowledge vs. tempting knowledge: People avoid knowledge from internal rivals and seek knowledge from external rivals</title><link>http://www.example.com/articles/1</link><description>Menon, Tanya; Thompson, Leigh; Choi, Hoon-Seok
We compare how people react to good ideas authored by internal rivals (employees at the same organization) versus external rivals (employees at a competitor organization). We hypothesize that internal and external rivals evoke contrasting kinds of threats. Specifically, using knowledge from an internal rival is difficult because it threatens the self and its competence: It is tantamount to being a "follower" and losing status relative to a direct competitor. By contrast, external rivals pose a lower threat to personal status, so people are more willing to use their knowledge. We conducted three studies. Study I showed that internal and external rivalry involved opposite relationships between threat and knowledge valuation: The more threat internal rivals provoked, the more people avoided their knowledge, whereas the more threat external rivals provoked, the more people pursued their knowledge. Study 2 explored the types of threat that insiders and outsiders evoked. In particular, people assumed that they would lose more personal status if they used an internal rival's knowledge and, therefore, reduced their valuation of that knowledge. Finally, Study 3 found that self-affirmation attenuated these patterns. We suggest that the threats and opportunities for affirmation facing the self dictate how people respond to rivals and, ultimately, their willingness to value new ideas.</description><author>Menon, Tanya; Thompson, Leigh; Choi, Hoon-Seok</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On precautionary policies</title><link>http://www.example.com/articles/1</link><description>Barrieu, Pauline; Sinclair-Desgagne, Bernard
In the United States and most industrialized countries, regulatory policies pertaining to food safety, occupational health, and environmental protection are (according to laws and statutes) science based. The complexity of some ecosystems and new technologies, however, makes it increasingly necessary to deal with situations where scientists cannot yet provide a definite picture. In this context, a widely invoked (but debated) rule, known as the Precautionary Principle, says to address potential hazards right away with preventive measures. We develop an intuitive formalization of this rule, which allows us to infer what an appropriate precautionary policy should do. Implications for resource conservation and the regulation of technological risks are then explored.</description><author>Barrieu, Pauline; Sinclair-Desgagne, Bernard</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Coordination in fast-response organizations</title><link>http://www.example.com/articles/1</link><description>Faraj, Samer; Xiao, Yan
Organizational coordination has traditionally been viewed from an organizational-design perspective where rules, modalities, and structures are used to meet the information-processing demands of the environment. Fast-response organizations face unique coordination challenges as they operate under conditions of high uncertainty and fast decision making, where mistakes can be catastrophic. Based on an in-depth investigation of the coordination practices of a medical trauma center where fast-response and error-free activities are essential requirements, we develop a coordination-practice perspective that emphasizes expertise coordination and dialogic coordination. We argue that expertise coordination practices (reliance on protocols, community of practice structuring, plug-and-play teaming, and knowledge sharing) are essential to manage distributed expertise and ensure the timely application of necessary expertise. We suggest that dialogic coordination practices (epistemic contestation, joint sensemaking, cross-boundary intervention, and protocol breaking) are time-critical responses to novel events and ensure error-free operation. However, dialogic coordination</description><author>Faraj, Samer; Xiao, Yan</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Knowledge gathering, team capabilities, and project performance in challenging work environments</title><link>http://www.example.com/articles/1</link><description>Haas, Martine R.
Knowledge gathering can create problems as well as benefits for project teams in work environments characterized by overload, ambiguity, and politics. This paper proposes that the value of knowledge gathering in such environments is greater under conditions that enhance team processing, sensemaking, and buffering capabilities. The hypotheses were tested using independent quality ratings of 96 projects and survey data from 485 project-team members collected during a multimethod field study. The findings reveal that three capability-enhancing conditions moderated the relationship between knowledge gathering and project quality: slack time, organizational experience, and decision-making autonomy. More knowledge gathering helped teams to perform more effectively under favorable conditions but hurt performance under conditions that limited their capabilities to utilize that knowledge successfully. Implications for theory and research on knowledge and learning in organizations, team effectiveness, and organizational design are discussed.</description><author>Haas, Martine R.</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conducting R&amp;D in countries with weak intellectual property rights protection</title><link>http://www.example.com/articles/1</link><description>Zhao, Minyuan
Multinational enterprises (MNEs) are increasingly conducting research and development (R&amp;D) in countries such as China and India, where intellectual property rights (IPR) protection is still far from adequate. This paper examines this seemingly puzzling situation. I argue that weak IPR leads to low returns to innovation and underutilization of innovative talents; MNEs that possess alternative mechanisms for protecting their intellectual properties will therefore find it attractive to conduct R&amp;D at those locations. A theoretical framework is developed to capture the interaction between firm strategy and the institutional environment. The empirical analysis on a sample of 1,567 U.S.-headquartered innovating firms finds results consistent with the hypotheses that (i) technologies developed in countries with weak IPR protection are used more internally, and (ii) technologies developed by firms with R&amp;D in weak IPR countries show stronger internal linkages. The results suggest that firms may use internal organizations to substitute for inadequate external institutions. By doing so, they are able to take advantage of the arbitrage opportunities presented by the institutional gap across countries.</description><author>Zhao, Minyuan</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Time value of commercial product returns</title><link>http://www.example.com/articles/1</link><description>Guide, V. Daniel R.; Souza, Gilvan C.; Van Wassenhove, Luk N.; Blackburn, Joseph D.
Manufacturers and their distributors must cope with an increased flow of returned products from their customers. The value of commercial product returns, which we define as products returned for any reason within 90 days of sale, now exceeds $100 billion annually in-the United States. Although the reverse supply chain of returned products represents a sizeable flow of potentially recoverable assets, only a relatively small fraction of the value is currently extracted by manufacturers; a large proportion of the product value erodes away because of long processing delays. Thus, there are significant opportunities to build competitive advantage from making the appropriate reverse supply chain design choices. In this paper, we present a network flow with delay models that includes the marginal value of time to identify the drivers of reverse supply chain design. We illustrate our approach with specific examples from two companies in different industries and then examine how industry clockspeed generally affects the choice between an efficient and a responsive returns network.</description><author>Guide, V. Daniel R.; Souza, Gilvan C.; Van Wassenhove, Luk N.; Blackburn, Joseph D.</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Agent competition double-auction mechanism</title><link>http://www.example.com/articles/1</link><description>Chu, Leon Yang; Shen, Zuo-Jun Max
This paper proposes an agent competition double-auction mechanism to simplify decision making and promote transactions for the customer-to-customer marketplaces. Under the proposed double-auction mechanism, bidding one's true valuation (private information) is the best strategy for each individual buyer and seller even when shipping costs and sales taxes are different across various possible transactions. The proposed mechanism also achieves budget balance and asymptotic efficiency. Furthermore, these results not only hold for an environment where buyers and sellers exchange identical commodities, but also can be extended to an environment with multiple substitutable commodities.</description><author>Chu, Leon Yang; Shen, Zuo-Jun Max</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Timeshare exchange mechanisms</title><link>http://www.example.com/articles/1</link><description>Wang, Yu; Krishna, Aradhna
This paper focuses on the timeshare industry, where members own timeshare "weeks" and can exchange these weeks among themselves without a medium of exchange (such as money). Timeshare exchanges allow for the weeks to be redistributed among members to better match their preferences and thus increase efficiency. As such, the problem falls within the domain of matching problems, which have recently gained much attention in academia. We demonstrate theoretically that the two major timeshare exchange mechanisms used currently (deposit-first mechanism and request-first mechanism) can cause efficiency loss. We propose an alternate exchange mechanism, the top trading cycles chains and spacebank (TTCCS) mechanism, and show that it can increase the efficiency of the timeshare exchange market because TTCCS is Pareto efficient, individually rational, and strategyproof. We test the three exchange mechanisms in laboratory experiments where we simulate exchange markets with networked "timeshare members." The results of the experiments are robust across four different environments that we construct and strongly support our theory The research focuses on an industry not studied earlier within academia and extends theoretical work on mechanism design to cases where supply of resources is dynamic, but resources can be stored.</description><author>Wang, Yu; Krishna, Aradhna</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic commitments for an optimal capacity decision under asymmetric forecast information</title><link>http://www.example.com/articles/1</link><description>Ozer, Ozalp; Wei, Wei
We study the important problem of how to assure credible forecast information sharing between a supplier and a manufacturer. The supplier is responsible for acquiring the necessary capacity before receiving an order from the manufacturer who possesses private forecast information for her end product. We address how different contracts affect the supplier's capacity decision and, hence, the profitability of the supplier and the manufacturer. We fully develop two contracts (and provide explicit formulae) to enable credible forecast information sharing. The first is a nonlinear capacity reservation contract under which the manufacturer agrees to pay a fee to reserve capacity. The second is an advance purchase contract under which the manufacturer is induced to place a firm order before the supplier secures the component capacity used to build the end product. These contracts serve a strategic role in information sharing. The capacity reservation contract enables the supplier to detect the manufacturer's private forecast information, while the advance purchase contract enables the manufacturer to signal her forecast information. We show that channel coordination is possible even under asymmetric forecast information by combining the advance purchase contract with an appropriate payback agreement. Through our structural and numerical results we also show that the degree of forecast information asymmetry and the risk-adjusted profit margin are two important drivers that determine supply chain efficiency and which contract to adopt.</description><author>Ozer, Ozalp; Wei, Wei</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Financial asset returns, direction-of-change forecasting, and volatility dynamics</title><link>http://www.example.com/articles/1</link><description>Christoffersen, Peter F.; Diebold, Francis X.
We consider three sets of phenomena that feature prominently in the financial economics literature: (1) conditional mean dependence (or lack thereof) in asset returns, (2) dependence (and hence forecastability) in asset return signs, and (3) dependence (and hence forecastability) in asset return volatilities. We show that they are very much interrelated and explore the relationships in detail. Among other things, we show that (1) volatility dependence produces sign dependence, so long as expected returns are nonzero, so that one should expect sign dependence, given the overwhelming evidence of volatility dependence; (2) it is statistically possible to have sign dependence without conditional mean dependence; (3) sign dependence is not likely to be found via analysis of sign autocorrelations, runs tests, or traditional market timing tests because of the special nonlinear nature of sign dependence, so that traditional market timing tests are best viewed as tests for sign dependence arising from variation in expected returns rather than from variation in volatility or higher moments; (4) sign dependence is not likely to be found in very high-frequency (e.g., daily) or very low-frequency (e.g., annual) returns; instead, it is more likely to be found at intermediate return horizons; and (5) the link between volatility dependence and sign dependence remains intact in conditionally non-Gaussian environments, for example, with time-varying conditional skewness and/or kurtosis.</description><author>Christoffersen, Peter F.; Diebold, Francis X.</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Violations of cumulative prospect theory in mixed gambles with moderate probabilities</title><link>http://www.example.com/articles/1</link><description>Baltussen, Guido; Post, Thierry; van Vliet, Pim
In a classroom choice experiment with mixed gambles and moderate probabilities, we find severe violations of cumulative prospect theory (CPT) and of Markowitz stochastic dominance. Our results shed new light on the exchange between Levy and Levy (2002) and Wakker (2003) in this journal.</description><author>Baltussen, Guido; Post, Thierry; van Vliet, Pim</author><pubDate>Tue, 01 Aug 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The promise of research on open source software</title><link>http://www.example.com/articles/1</link><description>von Krogh, Georg; von Hippel, Eric
Breaking with many established assumptions about how innovation ought to work, open source software projects offer eye-opening examples of novel innovation practices for students and practitioners in many fields. In this article we briefly review existing research on the open source phenomenon and discuss the utility of open source software research findings for many other fields. We categorize the research into three areas: motivations of open source software contributors; governance, organization, and the process of innovation in open source software projects; and competitive dynamics enforced by open source software. We introduce the articles in this special issue of Management Science on open source software, and show how each contributes insights to one or more of these areas.</description><author>von Krogh, Georg; von Hippel, Eric</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Understanding the motivations, participation, and performance of open source software developers: A longitudinal study of the Apache projects</title><link>http://www.example.com/articles/1</link><description>Roberts, Jeffrey A.; Hann, Il-Horn; Slaughter, Sandra A.
Understanding what motivates participation is a central theme in the research on open source software (OSS) development. Our study contributes by revealing how the different motivations of OSS developers are interrelated, how these motivations influence participation leading to performance, and how past performance influences subsequent motivations. Drawing on theories of intrinsic and extrinsic motivation, we develop a theoretical model relating the motivations, participation, and. performance of OSS developers. We evaluate our model using survey and archival data collected from a longitudinal field study of software developers in the Apache projects. Our results reveal several important findings. First, we find that developers' motivations are not independent but rather are related in complex ways. Being paid to contribute to Apache projects is positively related to developers' status motivations but negatively related to their use-value motivations. Perhaps surprisingly, we find no evidence of diminished intrinsic motivation in the presence of extrinsic motivations; rather, status motivations enhance intrinsic motivations. Second, we find that different motivations have an impact on participation in different ways. Developers' paid participation and status motivations lead to above-average contribution levels, but use-value motivations lead to below-average contribution levels, and intrinsic motivations do not significantly impact average contribution levels. Third, we find that developers' contribution levels positively impact their performance rankings. Finally, our results suggest that past-performance rankings enhance developers' subsequent status motivations.</description><author>Roberts, Jeffrey A.; Hann, Il-Horn; Slaughter, Sandra A.</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Motivation, governance, and the viability of hybrid forms in open source software development</title><link>http://www.example.com/articles/1</link><description>Shah, Sonali K.
Open source software projects rely on the voluntary efforts of thousands of software developers, yet we know little about why developers choose to participate in this collective development process. This paper inductively derives a framework for understanding participation from the perspective of the individual software developer based on data from two software communities with different governance structures. In both communities, a need for software-related improvements drives initial participation. The majority of participants leave the community once their needs are met, however, a small subset remains involved. For this set of developers, motives evolve over time and participation becomes a hobby. These hobbyists are critical to the long-term viability of the software code: They take on tasks that might otherwise go undone and work to maintain the simplicity and modularity of the code. Governance structures affect this evolution of motives. Implications for firms interested in implementing hybrid strategies designed to combine the advantages of open source software development with proprietary ownership and control are discussed.</description><author>Shah, Sonali K.</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploring the structure of complex software designs: An empirical study of open source and proprietary code</title><link>http://www.example.com/articles/1</link><description>MacCormack, Alan; Rusnak, John; Baldwin, Carliss Y.
This paper reports data from a study that seeks to characterize the differences in design structure between complex software products. We use design structure matrices (DSMs) to map dependencies between the elements of a design and define metrics that allow us to compare the structures of different designs. We use these metrics to compare the architectures of two software products-the Linux operating system and the Mozilla Web browser-that were developed via contrasting modes of organization: specifically, open source versus proprietary development. We then track the evolution of Mozilla, paying attention to a purposeful "redesign" effort undertaken with the intention of making the product more "modular." We find significant differences in structure between Linux and the first version of Mozilla, suggesting that Linux had a more modular architecture. Yet we also find that the redesign of Mozilla resulted in an architecture that was significantly more modular than that of its predecessor and, indeed, than that of Linux. Our results, while exploratory, are consistent with a view that different modes of organization are associated with designs that possess different structures. However, they also suggest that purposeful managerial actions can have a significant impact in adapting a design's structure. This latter result is important given recent moves to release proprietary software into the public domain. These moves are likely to fail unless the product possesses an "architecture for participation."</description><author>MacCormack, Alan; Rusnak, John; Baldwin, Carliss Y.</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic interaction and knowledge sharing in the KDE developer mailing list</title><link>http://www.example.com/articles/1</link><description>Kuk, George
In stark contrast with the fully participative "bazaar" imagery of open source software (OSS) development, some recent empirical research has pointed out that much of the OSS development is carried out by a small percentage of developers. This raises serious concerns that concentration of development effort on a few will limit knowledge sharing and underutilize the available resources. Using the notion of strategic interaction, this paper argues that individual developers often interact strategically with other highly resourceful developers by forming a smaller but better organized structure to intensify the types of epistemic interactions that matter most to the OSS development. A general framework of strategic interaction including participation inequality, conversational interactivity, and cross-thread connectivity is proposed to examine its impact on knowledge sharing, and validated using 128 discussion threads from the K Desktop Environment (KDE) developer mailing list. The findings indicate that strategic interaction has expanded knowledge sharing but with the caveat that extreme concentration of development could have an opposite effect. For researchers, this study dovetails the incentive logic by proposing and validating the strategic aspects of OSS participation to better understand the collective dynamics underpinning OSS development. Practitioners can use this approach to evaluate and better support existing knowledge-sharing initiatives.</description><author>Kuk, George</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Location, location, location: How network embeddedness affects project success in open source systems</title><link>http://www.example.com/articles/1</link><description>Grewal, Rajdeep; Lilien, Gary L.; Mallapragada, Girish
The community-based model for software development in open source environments is becoming a viable alternative to traditional firm-based models. To better understand the workings of open source environments, we examine the effects of network embeddedness-or the nature of the relationship among projects and developers-on the success of open source projects. We find that considerable heterogeneity exists in the network embeddedness of open source projects and project managers. We use a visual representation of the affiliation network of projects and developers as well as a formal statistical analysis to demonstrate this heterogeneity and to investigate how these structures differ across projects and project managers. Our main results surround the effect of this differential network embeddedness on project success. We find that network embeddedness has strong and significant effects on both technical and commercial success, but that those effects are quite complex. We use latent class regression analysis to show that multiple regimes exist and that some of the effects of network embeddedness are positive under some regimes and negative under others. We use project age and number of page views to provide insights into the direction of the effect of network embeddedness on project success. Our findings show that different aspects of network embeddedness have powerful but subtle effects on project success and suggest that this is a rich environment for further study.</description><author>Grewal, Rajdeep; Lilien, Gary L.; Mallapragada, Girish</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Two-sided competition of proprietary vs. open source technology platforms and the implications for the software industry</title><link>http://www.example.com/articles/1</link><description>Economides, Nicholas; Katsamakas, Evangelos
Technology platforms, such as Microsoft Windows, are the hubs of technology industries. We develop a framework to characterize the optimal two-sided pricing strategy of a platform firm; that is, the pricing strategy toward the direct users of the platform as well as toward firms offering applications that are complementary to the platform. We compare industry structures based on a proprietary platform (such as Windows) with those based on an open source platform (such as Linux), and analyze the structure of competition and industry implications in terms of pricing, sales, profitability, and social welfare. We find that, when the platform is proprietary, the equilibrium prices for the platform, the applications, and the platform access fee for applications may be below marginal cost, and we characterize demand conditions that lead to this. The proprietary applications sector of an industry based on an open source platform may be more profitable than the total profits of a proprietary platform industry. When users have a strong preference for application variety, the total profits of the proprietary industry are larger than the total profits of an industry based on an open source platform. The variety of applications is larger when the platform is open source. When a system based on an open source platform with an independent proprietary application competes with a proprietary system, the proprietary system is likely to dominate the open source platform industry both in terms of market share and profitability. This may explain the dominance of Microsoft in the market for PC operating systems.</description><author>Economides, Nicholas; Katsamakas, Evangelos</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic mixed duopoly: A model motivated by Linux vs. Windows</title><link>http://www.example.com/articles/1</link><description>Casadesus-Masanell, Ramon; Ghemawat, Pankaj
This paper analyzes a dynamic mixed duopoly in which a profit-maximizing competitor interacts with a competitor that prices at zero (or marginal cost), with the cumulation of output affecting their relative positions over time. The modeling effort is motivated by interactions between Linux, an open source operating system, and Microsoft's Windows and consequently emphasizes demand-side learning effects that generate dynamic scale economies (or network externalities). Analytical characterizations of the equilibrium under such conditions are offered, and some comparative static and welfare effects are examined.</description><author>Casadesus-Masanell, Ramon; Ghemawat, Pankaj</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Entry strategies under competing standards: Hybrid business models in the open source software industry</title><link>http://www.example.com/articles/1</link><description>Bonaccorsi, Andrea; Giannangeli, Silvia; Rossi, Cristina
The paper analyzes the strategies of software firms that have entered the open source (OS) field. The notion of the OS business model is discussed in the light of a substantial body of theoretical literature concerning strategic management and the economics of innovation, as well as specialized literature on OS. Empirical evidence based on a survey of 146 Italian software firms shows that firms have adapted to an environment dominated by incumbent standards by combining the offering of proprietary and OS software under different licensing schemes, thus choosing a hybrid business model. The paper examines the determinants of the degree of openness toward OS and discusses the stability of hybrid models in the evolution of the industry.</description><author>Bonaccorsi, Andrea; Giannangeli, Silvia; Rossi, Cristina</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Open source software user communities: A study of participation in Linux user groups</title><link>http://www.example.com/articles/1</link><description>Bagozzi, Richard P.; Dholakia, Utpal M.
We conceptualize participation in Linux user groups (LUGs) in terms of group-referent intentional actions and investigate cognitive (attitudes, perceived behavioral control, identification with the open source movement), affective (positive and negative anticipated emotions), and social (social identity) determinants of participation and its consequences on Linux-related behaviors of users. This survey-based study, conducted with 402 active LUG members representing 191 different LUGs from 23 countries and employing structural equation modeling methodology, supports the proposed model. Furthermore, we find that the Linux user's experience level moderates the extent of the LUG's social influence and its impact on the user's participation. We conclude with a consideration of the managerial and research implications of the study's findings.</description><author>Bagozzi, Richard P.; Dholakia, Utpal M.</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The architecture of participation: Does code architecture mitigate free riding in the open source development model?</title><link>http://www.example.com/articles/1</link><description>Baldwin, Carliss Y.; Clark, Kim B.
This paper argues that the architecture of a codebase is a critical factor that lies at the heart of the open source development process. We define two observable properties of an architecture: (1) modularity and (2) option value. Developers can often make informed judgments about modularity and option value from early, partially implemented code releases. We show that codebases that are more modular or have more option value (1) increase developers' incentives to join and remain involved in an open source development effort and (2) decrease the amount of free riding in equilibrium. These effects occur because modularity and option value create opportunities for the exchange of valuable work among developers, opportunities that do not exist in codebases that are not modular or have no option value.</description><author>Baldwin, Carliss Y.; Clark, Kim B.</author><pubDate>Sat, 01 Jul 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Location choices across the value chain: How activity and capability influence collocation</title><link>http://www.example.com/articles/1</link><description>Alcacer, Juan
There has been a recent revival of interest in the geographic component of firm strategy. Recent research suggests that two opposing forces-competition costs and agglomeration benefits-determine whether firms collocate in a given geographic market. Unexplored is (1) whether these forces have different impacts on R&amp;D, production, and sales subsidiaries, leading to diverse collocation levels, and (2) how firm capabilities impact collocation by increasing or decreasing competition costs and agglomeration benefits. I explore these questions using the worldwide location decisions of firms in the cellular handset industry. I find that production and sales subsidiaries are more geographically dispersed, and R&amp;D subsidiaries are more concentrated, than a random distribution would predict. When distinguishing firms by their capabilities, I find that more-capable firms collocate less than less-capable firms, regardless of the activity performed.</description><author>Alcacer, Juan</author><pubDate>Sun, 01 Oct 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Impact of licensing on investment and financing of technology development</title><link>http://www.example.com/articles/1</link><description>Kulatilaka, Nalin; Lin, Lihui
Technology innovations continue to be one of the greatest drivers of economic growth. Realizing the value of such innovations, however, requires substantial follow-on investments in development and commercialization. The value of these investments is difficult to capture because of uncertain demand and potential competition. This often leads to difficulties in obtaining outside financing for these investments. In this paper, we explore how licensing contracts can both dissuade other firms from developing alternative technologies and alleviate the financing problem. We develop a model in which a firm that invests in the development efforts of an innovation can license its technology to a potential competitor. A variety of licensing possibilities is considered, including fixed fees, royalty schedules, and two-part licenses consisting of an up-front payment and a capped royalty schedule. When the firm has no financial constraint, a royalty schedule that depends on realized demand dominates a fixed fee per license. When investment funds are constrained, a royalty cap license with an up-front payment can serve as a source of financing. We also study the investment problem conditional on the licensing and financing decisions.</description><author>Kulatilaka, Nalin; Lin, Lihui</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Efficient Monte Carlo and quasi-Monte Carlo option pricing under the variance gamma model</title><link>http://www.example.com/articles/1</link><description>Avramidis, Athanassios N.; L'Ecuyer, Pierre
We develop and study efficient Monte Carlo algorithms for pricing path-dependent options with the variance gamma model. The key ingredient is difference-of-gamma bridge sampling, based on the representation of a variance gamma process as the difference of two increasing gamma processes. For typical payoffs, we obtain a pair of estimators (named low and high) with expectations that (1) are monotone along any such bridge sampler, and (2) contain the continuous-time price. These estimators provide pathwise bounds on unbiased estimators that would be more expensive (infinitely expensive in some situations) to compute. By using these bounds with extrapolation techniques, we obtain significant efficiency improvements by work reduction. We then combine the gamma bridge sampling with randomized quasi-Monte Carlo to reduce the variance and thus further improve the efficiency. We illustrate the large efficiency improvements on numerical examples for Asian, lookback, and barrier options.</description><author>Avramidis, Athanassios N.; L'Ecuyer, Pierre</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Strategic investments, trading, and pricing under forecast updating</title><link>http://www.example.com/articles/1</link><description>Chod, Jiri; Rudi, Nils
This paper considers, two independent firms that invest in resources such as capacity or inventory based on imperfect market forecasts. As time progresses and new information becomes available, the firms update their forecasts and have the option to trade their resources. The trade contract is determined as the bargaining equilibrium or, alternatively, as the price equilibrium. Assuming a fairly general form of the profit functions, we characterize the Nash equilibrium investment levels, which are first-best under the price equilibrium trade contract, but not under the bargaining equilibrium trade contract. To gain additional insights, we then focus on firms that face stochastic demand functions with constant price elasticity and have contingent pricing power. Assuming a general forecast evolution process, we characterize the impact of the option to trade and the firms' cooperation on equilibrium investments, expected prices, profits, and consumer surplus. Finally, to study the main driving forces of trading, we employ a well-established and empirically tested forecast updating model in which the forecast evolution process follows a two-dimensional geometric Brownian motion. Under this model, we prove that the equilibrium investments, expected prices, profits, and consumer surplus are nondecreasing in the quality and timing of forecast revisions, in market variability, and in foreign exchange volatility, but are nonincreasing in market correlation.</description><author>Chod, Jiri; Rudi, Nils</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Production and inventory control of a single product assemble-to-order system with multiple customer classes</title><link>http://www.example.com/articles/1</link><description>Benjaafar, Saif; ElHafsi, Mohsen
We consider the optimal production and inventory control of an assemble-to-order system with m components, one end-product, and n customer classes. A control policy specifies when to produce each component and, whenever an order is placed, whether or not to satisfy it from on-hand inventory. We formulate the problem as a Markov decision process and characterize the structure of an optimal policy. We show that a base-stock production policy is optimal, but the base-stock level for each component is dynamic and depends on the inventory level of all other components (more specifically, it is nondecreasing). We show that the optimal inventory allocation for each component is a rationing policy with different rationing levels for different demand classes. The rationing levels for each component are dynamic and also nondecreasing in the inventory level of all other components. We compare the performance of the optimal policy to heuristic policies, including the commonly used base-stock policy with fixed base-stock levels, and find them to perform surprisingly well, especially for systems with lost sales.</description><author>Benjaafar, Saif; ElHafsi, Mohsen</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Quality-based competition, profitability, and variable costs</title><link>http://www.example.com/articles/1</link><description>Chambers, Chester; Kouvelis, Panos; Semple, John
We consider the impact of variable production costs on competitive behavior in a duopoly where manufacturers compete on quality and price in a two-stage game. In the pricing stage, we make no assumptions regarding these costs-other than that they are positive and increasing in quality-and no assumptions about whether or not the market is covered. In the quality stage, we investigate a broad family of variable cost functions and show how the shape of these functions impacts equilibrium product positions, profits, and market coverage. We find that seemingly slight changes to the cost function's curvature can produce dramatically different equilibrium outcomes, including the degree of quality differentiation, which competitor is more profitable (the one offering higher or lower quality), and the nature of the market itself (covered or uncovered). Our model helps to predict and explain the diversity of outcomes we see in practice-something the previous literature has been unable to do.</description><author>Chambers, Chester; Kouvelis, Panos; Semple, John</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Common method variance in IS research: A comparison of alternative approaches and a reanalysis of past research</title><link>http://www.example.com/articles/1</link><description>Malhotra, Naresh K.; Kim, Sung S.; Patil, Ashutosh
Despite recurring concerns about common method variance (CMV) in survey research, the information systems (IS) community remains largely uncertain of the extent of such potential biases. To address this uncertainty, this paper attempts to systematically examine the impact of CMV on the inferences drawn from survey research in the IS area. First, we describe the available approaches for assessing CMV and conduct an empirical study to compare them. From an actual survey involving 227 respondents, we find that although CMV is present in the research areas examined, such biases are not substantial. The results also suggest that few differences exist between the relatively new marker-variable technique and other well-established conventional tools in terms of their ability to detect CMV. Accordingly, the marker-variable technique was employed to infer the effect of CMV on correlations from previously published studies. Our findings, based on the reanalysis of 216 correlations, suggest that the inflated correlation caused by CMV may be expected to be on the order of 0.10 or less, and most of the originally significant correlations remain significant even after controlling for CMV. Finally, by extending the marker-variable technique, we examined the effect of CMV on structural relationships in past literature. Our reanalysis reveals that contrary to the concerns of some skeptics, CMV-adjusted structural relationships not only remain largely significant but also are not statistically differentiable from uncorrected estimates. In summary, this comprehensive and systematic analysis offers initial evidence that (1) the marker-variable technique can serve as a convenient, yet effective, tool for accounting for CMV, and (2) common method biases in the IS domain are not as serious as those found in other disciplines.</description><author>Malhotra, Naresh K.; Kim, Sung S.; Patil, Ashutosh</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reconsideration of the winner-take-all hypothesis: Complex networks and local bias</title><link>http://www.example.com/articles/1</link><description>Lee, Eocman; Lee, Jeho; Lee, Jongseok
The literature on network effects has popularized a hypothesis that competition between incompatible technologies results in the "winner-take-all" outcome. For the survival of the firm in this sort of competition, the installed base has been emphasized. We argue that the validity of this hypothesis depends on how customers interact with one another (e.g., if they exchange advice or files). In some interaction networks, customers influenced by their acquaintances may adopt a lagging technology even when a lead technology has built a large installed base. The presence of such a local bias facilitates the persistence of incompatibilities. When local bias cannot be sustained in other interaction networks, one technology corners the market. Our study suggests that overemphasizing the installed base, while ignoring network structure, could mislead practitioners.</description><author>Lee, Eocman; Lee, Jeho; Lee, Jongseok</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of service experiences over time on a supplier's retention of business customers</title><link>http://www.example.com/articles/1</link><description>Bolton, Ruth N.; Lemon, Katherine N.; Bramlett, Matthew D.
This paper examines the link between a supplier's marketing and service operations and its business customers' subsequent repatronage behavior. We develop a dynamic model of service contract renewal for an individual firm, at the contract level, recognizing interdependencies among service contract renewal decisions due to the firm's purchase of multiple contracts from the same supplier. The decision to renew a service contract is modeled as a function of service quality and price, where service quality is measured by the supplier's service operations metrics over time. By incorporating longitudinal data about the supplier's service operations, this study investigates how average service levels, variability in service levels (especially extreme outcomes), and timing of service delivery influence firms' service contract renewal decisions. The study context is support services for high-technology systems in business markets in Germany and the United Kingdom, where service operations metrics over time typically have skewed distributions. Firm behavior is represented by a binary choice model at the contract level, estimated as a binary response model with a complementary log-log link function incorporating random intercepts. The study shows that a firm that has a few extremely favorable experiences for a given service contract is more likely to subsequently renew that service contract, after controlling for average service levels. Firms weigh recent experiences (i.e., within the past year)-rather than earlier experiences-when deciding whether or not to renew, so the timing of service experiences may be critical to the survival of buyer-seller relationships. Overall, the study suggests that models of customer retention should incorporate the extent, variability, and timing of a supplier's service delivery over time at the contract/product level.</description><author>Bolton, Ruth N.; Lemon, Katherine N.; Bramlett, Matthew D.</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal dynamic advertising policy for new products</title><link>http://www.example.com/articles/1</link><description>Krishnan, Trichy V.; Jain, Dipak C.
Advertising is one of the key marketing tools managers have at their disposal to influence their customers into purchasing a new product. The overall objective of new product advertising is to inform and persuade customers. Drawing up an advertising plan for a new product that is under the influence of diffusion phenomenon is not an easy task. Hence, research in this area is very limited. In our research, we use an empirically proven diffusion demand function that explicitly incorporates the advertising component. Our results suggest that optimal advertising is determined by the advertising effectiveness, discount rate, and the ratio of advertisement to profits. Depending upon the interplay among these factors, the optimal advertising takes decrease-increase, increase-decrease, monotonically increasing or monotonically decreasing shape.</description><author>Krishnan, Trichy V.; Jain, Dipak C.</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A theory of volatility spreads</title><link>http://www.example.com/articles/1</link><description>Bakshi, Gurdip; Madan, Dilip
This study formalizes the departure between risk-neutral and physical index return volatilities, termed volatility spreads. Theoretically, the departure between risk-neutral and physical index volatility is connected to the higher-order physical return moments and the parameters of the pricing kernel process. This theory predicts positive volatility spreads when investors are risk averse, and when the physical index distribution is negatively skewed and leptokurtic. Our empirical evidence is supportive of the theoretical implications of risk aversion, exposure to tail events, and fatter left-tails of the physical index distribution in markets where volatility is traded.</description><author>Bakshi, Gurdip; Madan, Dilip</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Multiplicative background risk</title><link>http://www.example.com/articles/1</link><description>Franke, G.; Schlesinger, H.; Stapleton, R. C.
Although there has been much attention in recent years on the effects of additive background risks, the same is not true for its multiplicative counterpart. We consider random wealth of the multiplicative form i, where i and P are statistically independent random variables. We assume that x is endogenous to the economic agent but that P is an exogenous and nontraclable background risk that represents a type of market incompleteness. Our main focus is on how the presence of the multiplicative background risk y affects risk-taking behavior for decisions on the choice of x. We extend the results of Gollier and Pratt (1996) to characterize conditions on preferences that lead to more cautious behavior.</description><author>Franke, G.; Schlesinger, H.; Stapleton, R. C.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A typology of plants in global manufacturing networks</title><link>http://www.example.com/articles/1</link><description>Vereecke, Ann; Van Dierdonck, Roland; De Meyer, Arnoud
The purpose of this paper is to propose a new, empirically derived typology of plants in the international manufacturing network of multinational companies. This typology is based on the knowledge flows between the plants. In our research, network analysis has been used as a methodology for understanding the position of plants in international manufacturing networks. The focus has been primarily on the intangible knowledge network, and secondarily on the physical, logistic network. Our analysis leads to four types of plants with different network roles: the isolated plants, the receivers, the hosting network players, and the active network players. Our analysis shows that the different types of plants play a different strategic role in the company, have a different focus, and differ in age, autonomy, and level of resources and investments. Also, the analysis suggests that the evolution of the plant depends to some extent on the network role of the plant. Finally, two scenarios for the development of a strong network role are identified. The research is useful for the scholar studying the architecture of knowledge networks, as well as for the practitioner who is in charge of an international network of manufacturing units.</description><author>Vereecke, Ann; Van Dierdonck, Roland; De Meyer, Arnoud</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Note on self-restraint as an online entry-deterrence strategy</title><link>http://www.example.com/articles/1</link><description>Liu, Yunchuan; Gupta, Sunil; Zhang, Z. Jolm
We develop a game-theoretical model to show that in the markets where price consistency across channels is critical, an incumbent brick-and-mortar retailer can deter the online entry of a pure-play e-tailer by strategically refraining from entering online. In the markets where price consistency is not a constraint, we find that the incumbent can deter the e-tailer's entry only if it enters online and credibly operates the online channel as an independent profit center. In other words, the incumbent must be willing to cannibalize its own brick-and-mortar business by charging a low online price. We also discuss some social welfare implications of retail online entry and the managerial insights of our analysis.</description><author>Liu, Yunchuan; Gupta, Sunil; Zhang, Z. Jolm</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Some empirical regularities in market shares</title><link>http://www.example.com/articles/1</link><description>Kohli, Rajeev; Sah, Raaj
We present some empirical regularities in the market shares of brands. Our cross-sectional data on market shares consists of 1,171 brands in 91 product categories of foods and sporting goods sold in the United States. One of our results is that the pattern of market shares for each of the categories (many of which are fundamentally dissimilar, such as breakfast cereals and rifles) is represented well by the power law. The power law also does better than an alternative model-namely, the exponential form-which has previously been studied in the literature but without having been compared to any alternative. These two models have sharply different implications; for example, the power law predicts that the ratio of market shares for two successively ranked brands becomes smaller as one progresses from higher-ranked to lower-ranked brands, whereas the exponential form predicts that this ratio is a constant. Our findings have several managerial and research implications, which we summarize.</description><author>Kohli, Rajeev; Sah, Raaj</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Dynamic control of an M/M/1 service system with adjustable arrival and service rates</title><link>http://www.example.com/articles/1</link><description>Ata, Baris; Shneorson, Shiri
We study a service facility in which the system manager dynamically controls the arrival and service rates to maximize the long-run average value generated. We initially consider a rate-setting problem where the service facility is modeled as an M/M/1 queue with adjustable arrival and service rates and solve this problem explicitly. Next, we use this solution to study a price-setting problem, where customers are utility maximizing and price- and delay-sensitive, and the system manager chooses state-dependent service rates and prices. We find that the optimal arrival rate is decreasing and the optimal service rate is increasing in the number of customers in the system; however, the optimal price need not be monotone. We also show that under the optimal policy, the service facility operates as one with a finite buffer. Finally, we study a numerical example to compare the social welfare achieved using a dynamic policy to that achieved using static policies and show the dynamic policy offers significant welfare gains.</description><author>Ata, Baris; Shneorson, Shiri</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A fractiles perspective to the joint price/quantity newsvendor model</title><link>http://www.example.com/articles/1</link><description>Raz, Gal; Porteus, Evan L.
Pricing and quantity decisions are critical to many firms across different industries. We study the joint price/quantity newsvendor model where only a single quantity and price decision is made, such as a fashion or holiday product that cannot be replenished and where the price is advertised nationally and cannot be changed. Demand is uncertain and sensitive to price. We develop a method for easily finding the optimal price and quantity that applies to more general cases than the usual one in which uncertainty is either additive, multiplicative, or a combination of the two. We represent a quantity by its fractile of the probability distribution of demand for a given price. We use a standard approach to approximating a given distribution with a finite number of representative fractiles and assume that these fractile functions are piecewise linear functions of the price. We identify effects that are not usually seen in a joint price/quantity newsvenclor model. For example, although the optimal quantity is a decreasing function of the unit cost, the optimal price can be nonmonotone in the unit cost and we shed insight into why. We illustrate that using a simplified structure of demand uncertainty can result in substantially lower profits.</description><author>Raz, Gal; Porteus, Evan L.</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Measuring and mitigating the costs of stockouts</title><link>http://www.example.com/articles/1</link><description>Anderson, Eric T.; Fitzsimons, Gavan J.; Simester, Duncan
T here is now an extensive theoretical literature investigating optimal inventory policies for retailers. Yet several recent reviews have recognized that these models are rarely applied in practice. One explanation for the paucity of practical applications is the difficulty of measuring how stockouts affect both current and future demand. In this paper, we report the findings of a large-scale field test that measures the short- and long-run opportunity cost of a stockout. The findings confirm that the adverse impact of a stockout extends to both other items in the current order as well as future orders. We show how the findings can be used to provide input to inventory planning models and illustrate how failing to account for the long-run effects of a stockout will lead to suboptimal inventory decisions. We also demonstrate how the findings can be used in a customer lifetime value model. Finally, the study investigates the effectiveness of different responses that firms can offer to mitigate the cost of stockouts. There is considerable variation in the effectiveness of these responses. Offering discounts to encourage customers to backorder rather than cancel their orders is widely used in practice, but that was the least profitable of the responses that we evaluated. The findings have important implications for retailers considering the use of discounts as a response to stockouts.</description><author>Anderson, Eric T.; Fitzsimons, Gavan J.; Simester, Duncan</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Design of robust business-to-business electronic marketplaces with guaranteed privacy</title><link>http://www.example.com/articles/1</link><description>Kalvenes, Joakim; Basu, Amit
Firms that set up electronic marketplaces to enhance their supply and/or distribution channels face challenges in attracting their competitors to participate. A major obstacle is the perception that the owner can unfairly exploit trading information for competitive advantage. In this paper, we propose a marketplace design that shifts the locus of control over trader privacy from the marketplace operator to each individual trader. We show how online transactions between trading partners can be conducted in total privacy, so that not even the marketplace owner/operator can exploit transaction information for strategic purposes. At the same time, our approach includes robust methods for transaction integrity and nonrepudiation, as well as posttransaction dispute resolution.</description><author>Kalvenes, Joakim; Basu, Amit</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Network software security and user incentives</title><link>http://www.example.com/articles/1</link><description>August, Terrence; Tunca, Tunay I.
We study the effect of user incentives on software security in a network of individual users under costly patching and negative network security externalities. For proprietary software or freeware, we compare four alternative policies to manage network security: (i) consumer self-patching (where no external incentives are provided for patching or purchasing); (ii) mandatory patching; (iii) patching rebate; and (iv) usage tax. We show that for proprietary software, when the software security risk and the patching costs are high, for both a welfare-maximizing social planner and a profit-maximizing vendor, a patching rebate dominates the other policies. However, when the patching cost or the security risk is low, self-patching is best. We also show that when a rebate is effective, the profit-maximizing rebate is decreasing in the security risk and increasing in patching costs. The welfare-maximizing rebates are also increasing in patching costs, but can be increasing in the effective security risk when patching costs are high. For freeware, a usage tax is the most effective policy except when both patching costs, and security risk are low, in which case a patching rebate prevails. Optimal patching rebates and taxes tend to increase with increased security risk and patching costs, but can decrease in the security risk for high-risk levels. Our results suggest that both the value generated from software and vendor profits can be significantly improved by mechanisms that target user incentives to maintain software security.</description><author>August, Terrence; Tunca, Tunay I.</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Organizational design and the intensity of rivalry</title><link>http://www.example.com/articles/1</link><description>Vroom, Govert
We analyze the effect of managerial compensation schemes and organizational structure on competitive behavior in imperfectly competitive product markets. Previous research suggests that in cases of strategic substitutability firms tend to choose organizational structures and compensation systems that commit the firm to behaving aggressively in the product market, reducing firm and industry profits. In contrast, we show that while compensation and structure in isolation lead to excessive aggressiveness, the combination of these two internal choice variables may reverse the outcome-organizational design can be used as a commitment device to reduce competitive rivalry Finally, we find that in equilibrium, firms may choose to be different; one firm is decentralized and uses incentives that commit it to being aggressive, while the other is centralized and uses incentives that commit it to being soft. Hence, endogenous firm heterogeneity in the form of organizational differentiation allows firms to avoid a mutually detrimental outcome.</description><author>Vroom, Govert</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction of new technologies to competing industrial customers</title><link>http://www.example.com/articles/1</link><description>Erat, Sanjiv; Kavadias, Stylianos
Motivated by several examples from industry, such as the introduction of a biotechnology-based process innovation in nylon manufacturing, we consider a technology provider that develops and introduces innovations to a market of industrial customers-original equipment manufacturers (OEMs). The technology employed by these OEMs determines the performance quality of the end product they manufacture, which in turn forms the basis of competition among them. Within this context of downstream competition, we examine the technology provider's introduction strategies when improving technologies are introduced sequentially. We develop a two-period game-theoretic framework to account for the strategic considerations of the parties involved (i.e., the technology provider and the OEMs). Our main result indicates that the technology provider may find it beneficial to induce partial adoption of the new technology, depending on the technological progress the provider intends to offer in the future. We analyze many technology-specific and market-related characteristics-such as volume-based pricing for new component technologies, upgrade prices, and OEMs with differing capabilities-that correspond to various business settings. Our key result (i.e., partial adoption) proves to be a robust phenomenon. We also develop additional insights regarding the interactions between adoption and OEM capabilities.</description><author>Erat, Sanjiv; Kavadias, Stylianos</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Exploratory innovation, exploitative innovation, and performance: Effects of organizational antecedents and environmental moderators</title><link>http://www.example.com/articles/1</link><description>Jansen, Justin J. P.; Van den Bosch, Frans A. J.; Volberda, Henk W.
Research on exploration and exploitation is burgeoning, yet our understanding of the antecedents and consequences of both activities remains rather unclear. We advance the growing body of literature by focusing on the apparent differences of exploration and exploitation and examining implications for using formal (i.e., centralization and formalization) and informal (i.e., connectedness) coordination mechanisms. This study further examines how environmental aspects (i.e., dynamism and competitiveness) moderate the effectiveness of exploratory and exploitative innovation. Results indicate that centralization negatively affects exploratory innovation, whereas formalization positively influences exploitative innovation. Interestingly, connectedness within units appears to be an important antecedent of both exploratory and exploitative innovation. Furthermore, our findings reveal that pursuing exploratory innovation is more effective in dynamic environments, whereas pursuing exploitative innovation is more beneficial to a unit's financial performance in more competitive environments. Through this richer explanation and empirical assessment, we contribute to a greater clarity and better understanding of how ambidextrous organizations coordinate the development of exploratory and exploitative innovation in organizational units and successfully respond to multiple environmental conditions.</description><author>Jansen, Justin J. P.; Van den Bosch, Frans A. J.; Volberda, Henk W.</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Recipient choice can address the efficiency-equity trade-off in kidney transplantation: A mechanism design model</title><link>http://www.example.com/articles/1</link><description>Su, Xuanming; Zenios, Stefanos A.
In kidney allocation, transplant candidates may have private information about their propensity to enjoy good outcomes after transplantation or about their relative expected improvement in quality of life after transplantation. This paper develops a mechanism design model to investigate the effect of such information asymmetry on the kidney allocation system. In this model, there are n transplant queues corresponding to n candidate types. Candidate types are only observed by the candidates, and each candidate chooses the queue to join by reporting a type. Kidneys have heterogeneous types, and each kidney will be assigned to one of the queues depending on its type. Candidates report their type strategically to join the queue that maximizes their utility. Candidate utility depends on the type of kidney received and the expected waiting time, which is calculated using fluid approximations. We consider two alternative social welfare functions: aggregate utility (emphasizing efficiency) and minimum utility across all candidates (emphasizing equity). The kidney allocation problem is to divide the organ supply among the different queues so that social welfare is maximized, and this problem is solved explicitly under both objective functions. There are three findings: (1) The allocation mechanism induces truth telling by ensuring that candidates who wait longer receive better kidneys; (2) Information rents are earned by high-risk candidates under the efficiency objective and by low-risk candidates under the equity objective; (3) a choice-based kidney allocation system in which candidates choose the type of queue to join leads to outcomes in the middle of the efficiency-equity spectrum.</description><author>Su, Xuanming; Zenios, Stefanos A.</author><pubDate>Wed, 01 Nov 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Uncertainty aversion with second-order utilities and probabilities</title><link>http://www.example.com/articles/1</link><description>Nau, R. F.
Subjective expected utility theory does not distinguish between attitudes toward uncertainty (ambiguous probabilities) and attitudes toward risk (unambiguous probabilities). Both are explained in terms of nonlinear utility for money rather than properties of events per se, hence, the decision maker displays the same attitude toward all sources of risk and uncertainty. There is ample evidence that real decision makers do not always behave (or even wish to behave) in this way, and instead they often distinguish between risk and uncertainty, as in Ellsberg's (1961) paradox. This paper presents a simple axiomatic model of nonneutral attitudes toward uncertainty and a behavioral test for uncertainty aversion that is applicable even if utility is state dependent. The decision maker may display different degrees of aversion toward gambles on different kinds of events, e.g., being systematically more averse toward gambles on events whose probabilities are more ambiguous. For such a decision maker, the elicitation of preferences among objective gambles may not yield the correct measure of risk aversion for modeling real-world decisions.</description><author>Nau, R. F.</author><pubDate>Sun, 01 Jan 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Information system success: Individual and organizational determinants</title><link>http://www.example.com/articles/1</link><description>Sabherwal, Rajiv; Jeyaraj, Anand; Chowa, Charles
Despite considerable empirical research, results on the relationships among constructs related to information system (IS) success, as well as the determinants of IS success, are often inconsistent. A comprehensive understanding of IS success thus remains elusive. In an attempt to address this situation, which may partly be due to the exclusion of potentially important constructs from prior parsimonious models of IS success, we present and test a comprehensive theoretical model. This model explains interrelationships among four constructs representing the success of a specific IS (user satisfaction, system use, perceived usefulness, and system quality), and the relationships of these IS success constructs with four user-related constructs (user experience with ISs, user training in ISs, user attitude toward ISs, and user participation in the development of the specific IS) and two constructs representing the context (top-management support for ISs and facilitating conditions for ISs). To test the model, we first used meta-analysis to compute a correlation matrix for the constructs in the model based on 612 findings from 121 studies published between 1980 and 2004, and then used this correlation matrix as input for a LISREL analysis of the model. Overall, we found excellent support for the theoretical model. The results underline the importance of user-related and contextual attributes in IS success and raise questions about some commonly believed relationships.</description><author>Sabherwal, Rajiv; Jeyaraj, Anand; Chowa, Charles</author><pubDate>Fri, 01 Dec 2006 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of imprecise probabilities and outcomes in evaluating investment options</title><link>http://www.example.com/articles/1</link><description>Du, N.; Budescu, D. V.
Vagueness attitudes have been used to explain anomalies and irregularities in investment behavior. It is generally assumed (Ellsberg 1961) that decision makers (DMs) dislike vagueness, but this assumption has been challenged by empirical results documenting systematic alternative attitudes to vagueness as a function of its source, the domain of the decisions, and the response mode used. We investigate these three factors in a within-subjects design that was embedded in an investment context. DMs evaluated investment options that varied in terms of their sources of vagueness (probabilities and/or outcomes), in both domains (gains or losses), and employed two response modes (pricing or choice). We confirm that individuals' vagueness attitudes are malleable, contingent on the dimension salience and the reference domain. In particular, we observed three distinct patterns of "reversals of attitudes" towards vagueness. Our results indicate that the ability of vagueness attitudes to predict investment behavior is limited, as decisions can be systematically influenced by task context and/or perceived gain or loss positions. Economic models may be improved by incorporating more flexible assumptions about individuals' attitudes toward vagueness.</description><author>Du, N.; Budescu, D. V.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Management control for market transactions: The relation between transaction characteristics, incomplete contract design, and subsequent performance</title><link>http://www.example.com/articles/1</link><description>Anderson, S. W.; Dekker, H. C.
Using an unusually comprehensive database on 858 transactions for information technology products and accompanying services, we study how close partners who are exposed to opportunistic hazards structure and control a significant transaction. We analyze data on the terms of contracting to determine whether transaction and supplier characteristics that generate opportunistic hazards are related to the formal management control structure. We also examine whether misalignment between transaction and supplier characteristics and the control structure is associated with ex post performance problems. Characteristics associated with hazards are found to be positively related to contract extensiveness. Factor analysis of the use of 24 contract terms reveals four groups of contract terms that are commonly used in combination. We interpret these factors as "dimensions of management control" and label them: assignment of rights, product and price, after-sales service, and legal recourse. Characteristics associated with hazards are positively related to the use of all four dimensions of management control, with different hazards associated with different controls. We then examine the relation between transaction characteristics and ex post transaction problems, demonstrating that even in the presence of mutually agreeable contracts, hazards remain. We conclude that costs of contracting are associated with increased use of contract terms on assignment of rights, after-sales service, and legal recourse. Finally, we present evidence that management control structures that are better aligned with transaction hazards mitigate subsequent performance problems, though at a nontrivial cost of contracting.</description><author>Anderson, S. W.; Dekker, H. C.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Better, faster, cheaper: An experimental analysis of a multiattribute reverse auction mechanism with restricted information feedback</title><link>http://www.example.com/articles/1</link><description>Chen-Ritzo, C. H.; Harrison, T. P.; Kwasnica, A. M.; Thomas, D. J.
The majority of reverse auctions for procurement use a single-attribute (price) format while providing constraints on nonprice attributes such as quality and lead time. Alternatively, a buyer could choose to conduct a multiattribute auction where bidders can specify both a price and levels of nonprice attributes. While such an auction may provide higher theoretical utility to the buyer, it is not clear that this theoretical improvement will be realized given the increased complexity of the auction. In this research, we present an ascending auction mechanism for a buyer whose utility function is known and dependent on three attributes. Motivated by a supply chain procurement problem setting, we consider quality and lead time for the two attributes in addition to price. The auction mechanism provides the bidders with restricted feedback regarding the buyer's utility function. We explore, experimentally, the performance of this multiattribute auction mechanism as compared to a price-only auction mechanism. Compared with the price-only auction, we find that our mechanism design is effective in increasing both buyer utility and bidder (supplier) profits.</description><author>Chen-Ritzo, C. H.; Harrison, T. P.; Kwasnica, A. M.; Thomas, D. J.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of asymmetric bidder size on an auction's performance: Are more bidders always better?</title><link>http://www.example.com/articles/1</link><description>Elmaghraby, W.
One commonly held belief in designing auctions is that increasing the number of bidders makes an auction more competitive. Therefore a buyer who wishes to minimize her procurement costs is better off inviting more suppliers to participate. In this paper, we question the validity of this belief by shedding light on bidders' behavior when bidders experience economies of scale in production and differ in their production capacity. We consider a setting with two different sized bidders, global and small. We assume that global bidders have a large production capacity (can win in more than one auction) and experience economies of scale in production, whereas small bidders can win in at most one auction. In this new setting, we focus on the impact of allowing both global and small suppliers to compete against each other on the performance of an auction.</description><author>Elmaghraby, W.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Investment decisions and time horizon: Risk perception and risk behavior in repeated gambles</title><link>http://www.example.com/articles/1</link><description>Klos, A.
To investigate the effect of time horizon on investment behavior, this paper reports the results of an experiment in which business graduate students provided certainty equivalents and judged various dimensions of the outcome distribution of simple gambles that were played either once or repeatedly for 5 or 50 times. Systematic mistakes in the ex-ante estimations of the distributions of outcomes after (independent) repeated plays were observed. Despite correctly realizing that outcome standard deviation increases with the number of plays, respondents showed evidence of Samuelson's (1963) fallacy of large numbers. Perceived risk judgments showed only low correlations with standard deviation estimates, but were instead related to the anticipated probability of a loss (which was overestimated), mean excess loss, and the coefficient of variation. Implications for future research and practical implications for financial advisors are discussed.</description><author>Klos, A.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Simple models for multiattribute choice with many alternatives: When it does and does not pay to face trade-offs with binary attributes</title><link>http://www.example.com/articles/1</link><description>Hogarth, R. M.; Karelaia, N.
Given the difficulties people experience in making trade-offs, what are the consequences of using simple models that avoid trade-offs? We examine choices by such models in environments where "true" preferences are linear and attributes are characterized by binary attributes. A deterministic elimination-by-aspects (DEBA) model is highly effective over a range of conditions. When preferences are quite compensatory, however, a modified equal weighting (EW) model that uses DEBA to resolve ties is more effective. We explore the sensitivity of results to errors in using DEBA, to different distributions of alternatives, and to error in "true" preferences. Under the conditions examined here, the outcomes of these "boundedly rational" models are highly consistent with "rational" models that explicitly confront trade-offs. We emphasize the importance of binary attributes in reaching these conclusions.</description><author>Hogarth, R. M.; Karelaia, N.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effects of financial risks on inventory policy</title><link>http://www.example.com/articles/1</link><description>Berling, P.; Rosling, K.
The effect of financial risks on (R, Q) inventory policies is analyzed in a real options framework. Simple adjustments of the usual formulas for R and Q are suggested and tested. Stochastic demand and purchase costs are considered, both with known systematic (business-cycle-related) risk. The systematic risk of stochastic demand has typically a negligible effect on the optimal values of R and Q, although an improvement may be achieved by a simple adjustment of R. The systematic risk of the purchase price, c, has a significant effect on R and Q. The capital holding cost should be estimated as r (.) c, where r is the sum of the risk-free interest rate, the expected price decrease, and the risk premium associated with the systematic risk of c. For goods quoted on commodity exchanges, r may be estimated directly from the prices on forward contracts. Its size (and sign) varies considerably for different commodities.</description><author>Berling, P.; Rosling, K.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Skewness preference, risk aversion, and the precedence relations on stochastic changes</title><link>http://www.example.com/articles/1</link><description>Chiu, W. H.
This paper provides a general choice-theoretic characterization of the trade-off between risk and skewness, whose importance in understanding risk-taking behavior is well documented in empirical studies. The condition under which the prudence measure (Kimball 1990) characterizes the strength of an individual's downside-risk aversion against his own risk aversion is identified and interpreted in a unifying framework based on the concept of one stochastic dominant change preceding another and that of the desirability of a stochastic change. The framework is also shown to be useful for a better understanding of the Arrow-Pratt measure, the stronger Ross measure, and the coincidence of the characterizations of downside-risk aversion and prudence, as well as the relationship between stochastic dominances of different degrees.</description><author>Chiu, W. H.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The effect of payoff feedback and information pooling on reasoning errors: Evidence from experimental markets</title><link>http://www.example.com/articles/1</link><description>Budescu, D. V.; Maciejovsky, B.
A vast literature shows that individuals frequently violate normative principles in reasoning. In this paper, we report the results of four studies designed to determine if information dissemination in competitive auctions can reduce, or even eliminate, logical errors in the Wason selection task. Our results show that payoff feedback and exposure to the information flow drive the aggregate behavior toward the normative solution. We also found evidence of spillover effects from informed to uninformed traders in one-sided combinatorial auctions, as well as positive transfer effects from competitive to individual settings.</description><author>Budescu, D. V.; Maciejovsky, B.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Pricing and resource allocation in caching services with multiple levels of quality of service</title><link>http://www.example.com/articles/1</link><description>Hosanagar, K.; Krishnan, R.; Chuang, J.; Choudhary, V.
Network caches are the storage centers in the supply chain for content delivery-the digital equivalent of warehouses. Operated by access networks and other operators, they provide benefits to content publishers in the forms of bandwidth cost reduction, response time improvement, and handling of flash crowds. Yet, caching has not been fully embraced by publishers, because its use can interfere with site personalization strategies and/or collection of visitor information for business intelligence purposes. While recent work has focused on technological solutions to these issues, this paper provides the first study of the managerial issues related to the design and provisioning of incentive-compatible caching services. Starting with a single class of caching service, we find conditions under which the profit-maximizing cache operator should offer the service for free. This occurs when the access networks' bandwidth costs are high and a large fraction of content publishers value personalization and business intelligence. Some publishers will still opt out of the service, i.e., cache bust, as observed in practice. We next derive the conditions under which the profit-maximizing cache operator should provision two vertically differentiated service classes, namely, premium and best effort. Interestingly, caching service differentiation is different from traditional vertical differentiation models, in that the premium and best-effort market segments do not abut. Thus, optimal prices for the two service classes can be set independently and cannibalization does not occur. It is possible for the cache operator to continue to offer the best-effort service for free while charging for the premium service. Furthermore, consumers are better off because more content is cached and delivered faster to them. Finally, we find that declining bandwidth costs will put negative pressure on cache operator profits, unless consumer adoption of broadband connectivity and the availability of multimedia content provide the necessary increase in traffic volume for the caches.</description><author>Hosanagar, K.; Krishnan, R.; Chuang, J.; Choudhary, V.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Perwez Shahabuddin - 1962-2005 - In memoriam</title><link>http://www.example.com/articles/1</link><description>Hopp, W.
nan</description><author>Hopp, W.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Tailored supply chain decision making under price-sensitive stochastic demand and delivery uncertainty</title><link>http://www.example.com/articles/1</link><description>Ray, S. B.; Li, S. L.; Song, Y. Y.
In this paper, we study a serial two-echelon supply chain selling a procure-to-stock product in a price-sensitive market. Our analytical modelling framework incorporates optimal pricing and stocking decisions for both echelons in the presence of stochastic demand and random delivery times. We focus on understanding how these decisions for the chain are affected by its management paradigm (centralized or decentralized), and its business characteristics-price sensitivity, demand uncertainty, and delivery time variability. A novel combination of transformations enables us to analyze the framework and determine the unique optimal choices for centralized and wholesale price-based decentralized supply chains. More detailed investigation reveals that, in general, the business characteristics influence both the behavior and the optimal values of the decision variables, while the management paradigm primarily governs the optimal values. We illustrate the significance of these results in terms of how managers should tailor their decisions to align with their business requirements. Subsequently; comparison of the optimal profits between the channel partners and the management paradigms provides implications for decentralization strategy. A decentralized chain is most inefficient for moderately price-sensitive customers and uncertain environments, but is relatively more effective when dealing with mature products. We propose a contracting scheme that can improve the decentralized chain profit in reliable delivery time settings. The salient modelling insight of this paper is that ignoring the randomness of delivery time trivializes the interaction between pricing and stocking decisions. On the other hand, from a managerial viewpoint, we establish that optimal pricing policies provide the means to increase revenue and also act as strategic tools for tackling uncertainty.</description><author>Ray, S. B.; Li, S. L.; Song, Y. Y.</author><pubDate>Thu, 01 Dec 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Importance sampling for portfolio credit risk</title><link>http://www.example.com/articles/1</link><description>Glasserman, P.; Li, J. Y.
Monte Carlo simulation is widely used to measure the credit risk in portfolios of loans, corporate bonds, and other instruments subject to possible default. The accurate measurement of credit risk is often a rare-event simulation problem because default probabilities are low for highly rated obligors and because risk management is particularly concerned with rare but significant losses resulting from a large number of defaults. This makes importance sampling (IS) potentially attractive. But the application of IS is complicated by the mechanisms used to model dependence between obligors, and capturing this dependence is essential to a portfolio view of credit risk. This paper provides an IS procedure for the widely used normal copula model of portfolio credit risk. The procedure has two parts: One applies IS conditional on a set of common factors affecting multiple obligors, the other applies IS to the factors themselves. The relative importance of the two parts of the procedure is determined by the strength of the dependence between obligors. We provide both theoretical and numerical support for the method.</description><author>Glasserman, P.; Li, J. Y.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrated lot sizing in serial supply chains with production capacities</title><link>http://www.example.com/articles/1</link><description>van Hoesel, S.; Romeijn, H. E.; Morales, D. R.; Wagelmans, A. P. M.
We consider a model for a serial supply chain in which production, inventory, and transportation decisions are integrated in the presence of production capacities and concave cost functions. The model we study generalizes the uncapacitated serial single-item multilevel economic lot-sizing model by adding stationary production capacities at the manufacturer level. We present algorithms with a running time that is polynomial in the planning horizon when all cost functions are concave. In addition, we consider different transportation and inventory holding cost structures that yield improved running times: inventory holding cost functions that are linear and transportation cost functions that are either linear, or are concave with a fixed-charge structure. In the latter case, we make the additional common and reasonable assumption that the variable transportation and inventory costs are such that holding inventories at higher levels in the supply chain is more attractive from a variable cost perspective. While the running times of the algorithms are exponential in the number of levels in the supply chain in the general concave cost case, the running times are remarkably insensitive to the number of levels for the other two cost structures.</description><author>van Hoesel, S.; Romeijn, H. E.; Morales, D. R.; Wagelmans, A. P. M.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Lower bounds for the capacitated facility location problem based on column generation</title><link>http://www.example.com/articles/1</link><description>Klose, A.; Drexl, A.
The capacitated facility location problem (CFLP) is a well-known combinatorial optimization problem with applications in distribution and production planning. A variety of lower bounds based on Lagrangean relaxation and subgradient optimization has been proposed for this problem. However, information about a primal (fractional) solution can be important to solve large or difficult problem instances. Therefore, we study various approaches for solving the master problems exactly The algorithms employ different strategies for stabilizing the column-generation process. Furthermore, a new lower bound for the CFLP based on partitioning the plant set and employing column generation is proposed. Computational results are reported for a set of large problem instances.</description><author>Klose, A.; Drexl, A.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An algorithm for portfolio optimization with transaction costs</title><link>http://www.example.com/articles/1</link><description>Best, M. J.; Hlouskova, J.
We consider the problem of maximizing an expected utility function of n assets, such as the mean-variance or power-utility function. Associated with a change in an asset's holdings from its current or target value is a transaction cost. This cost must be accounted for in practical problems. A straightforward way of doing so results in a 3n-dimensional optimization problem with 3n additional constraints. This higher dimensional problem is computationally expensive to solve. We present a method for solving the 3n-dimensional problem by solving a sequence of n-dimensional optimization problems, which accounts for the transaction costs implicitly rather than explicitly. The method is based on deriving the optimality conditions for the higher-dimensional problem solely in terms of lower-dimensional quantities. The new method is compared to the barrier method implemented in Cplex in a series of numerical experiments. With small but positive transaction costs, the barrier method and the new method solve problems in roughly the same amount of execution time. As the size of the transaction cost increases, the new method outperforms the barrier method by a larger and larger factor.</description><author>Best, M. J.; Hlouskova, J.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Asymptotic properties of Monte Carlo estimators of derivatives</title><link>http://www.example.com/articles/1</link><description>Detemple, J.; Garcia, R.; Rindisbacher, M.
We study the convergence of Monte Carlo estimators of derivatives when the transition density of the underlying state variables is unknown. Three types of estimators are compared. These are respectively based on Malliavin derivatives, on the covariation with the driving Wiener process, and on finite difference approximations of the derivative. We analyze two different estimators based on Malliavin derivatives. The first one, the Malliavin path estimator, extends the path derivative estimator of Broadie and Glasserman (1996) to general diffusion models. The second, the Malliavin weight estimator, proposed by Fournie et al. (1999), is based on an integration by parts argument and generalizes the likelihood ratio derivative estimator. It is shown that for discontinuous payoff functions, only the estimators based on Malliavin derivatives attain the optimal convergence rate for Monte Carlo schemes. Estimators based on the covariation or on finite difference approximations are found to converge at slower rates. Their asymptotic distributions are shown to depend on additional second-order biases even for smooth payoff functions.</description><author>Detemple, J.; Garcia, R.; Rindisbacher, M.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Step-level reasoning and bidding in auctions</title><link>http://www.example.com/articles/1</link><description>Gneezy, U.
Step-level models of reasoning (SLR) proved to be very successful in predicting behavior in the beauty contest game. Recently, a quantified version of the model was suggested as a more general model of thinking. In particular, it was found that, the distribution of choices could be represented by a Poisson distribution. I test the model in stylized first- and second-price common-value sealed-bid auctions. Equilibrium, for both auction types, prescribes that players undercut each other and profits are small. The SLR prediction, on the other hand, is different for the two auctions. Nash equilibrium predicts the outcomes poorly; the SLR model predicts the outcomes well in the second-price auction. However, while bids in the first-price auction could be represented by a Poisson distribution, this could not be attributed to step-level reasoning.</description><author>Gneezy, U.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Accounting conservatism and managerial incentives</title><link>http://www.example.com/articles/1</link><description>Kwon, Y. K.
There are two sources of agency costs under moral hazard: (1) distortions in incentive contracts and (2) implementation of suboptimal decisions. In the accounting literature, the relation between conservative accounting and agency costs of type (1) has received considerable attention (cf. Watts 2002). However, little appears to be known about the effects of accounting conservatism on agency costs of type (2) or trade-offs between agency costs of types (1) and (2). The purpose of this study is to examine this void. In a principal-agent setting in which the principal motivates the agent to expend effort using accounting earnings, this study shows that accounting earnings become more useful for reducing agency costs of type (2) when measured conservatively than when measured aggressively. Combined with the result in Kwon et al. (2001) that agency costs of type (1) decrease with accounting conservatism, this analysis suggests that conservative accounting enhances the incentive value of accounting signals with respect to both types of agency costs.</description><author>Kwon, Y. K.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Antecedents and consequences of group potency: A study of self-managing service teams</title><link>http://www.example.com/articles/1</link><description>de Jong, A.; de Ruyter, K.; Wetzels, M.
This paper proposes and tests a model of antecedents and consequences of group potency in self-managing teams in retail banking. Based on data collected from boundary-spanning service employees organized in 60 teams and their customers, our findings reveal a significant positive impact of group potency on customer-perceived service quality and a negative effect on service profitability. In addition, we find that team consensus regarding group potency positively moderates the effects of group potency, so that for teams with higher levels of potency consensus, the positive impact of group potency on customer-perceived service quality is stronger, whereas the negative impact of group potency on service productivity is weaker. Furthermore, we find significant positive effects of management and interteam support and functional diversity as well as a significant negative effect of team tenure on individual team member beliefs of group potency Finally, social support consensus moderates the effects of management support, interteam. support, and team tenure on group potency, so that the effects of these antecedents are weaker for teams with higher levels of social support consensus. Thus, we conclude that team confidence consensus increases the positive impact of group potency on customer perceptions of service quality and decreases the negative impact on profitability. Thus, team-member perceptual agreement on their team's potency should be stimulated.</description><author>de Jong, A.; de Ruyter, K.; Wetzels, M.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Agency problems in law enforcement: Theory and application to the U.S. Coast Guard</title><link>http://www.example.com/articles/1</link><description>Gawande, K.; Bohara, A. K.
We study two issues in the enforcement of public law. The first is whether the system of inspections and penalties set by the regulator is effective. The second is whether a better system of inspections and penalties can be designed, given the institutional constraints under which the regulator must function. We study these issues in the context of oil spill prevention activities of the U.S. Coast Guard (USCG), the agency entrusted with the enforcement of maritime pollution laws. A theoretically optimal contract that mixes penalties based on the amount of pollution ex post with penalties based on the extent of noncompliance ex ante is derived. The effectiveness of USCG inspections and penalties in reducing oil spills is then econometrically studied using microlevel data on a panel of U.S. flag tank vessels. Whether the optimal penalty can potentially improve the effectiveness of compliance inspections in reducing oil spills is examined in the light of the empirical results and recent developments in the economics and public management literature on effective incentive contracting. Among our findings is the potential for combining unilateral incentive-based methods with cooperative methods based on reciprocity to solve the complex problem of law enforcement.</description><author>Gawande, K.; Bohara, A. K.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Reduce-and-split cuts: Improving the performance of mixed-integer gomory cuts</title><link>http://www.example.com/articles/1</link><description>Andersen, K.; Cornuejols, G.; Li, Y. J.
Mixed-integer Gomory cuts have become an integral part of state-of-the-art software for solving mixed-integer linear programming problems. Therefore, improvements in the performance of these cutting planes can be of great practical value. In this paper, we present a simple and fast heuristic for improving the coefficients on the continuous variables in the mixed-integer Gomory cuts. This is motivated by the fact that in a mixed-integer Gomory cut, the coefficient of an integer variable lies between 0 and 1, whereas for a continuous variable, there is no upper bound. The heuristic tries to reduce the coefficients of the continuous variables. We call the resulting cuts reduce-and-split cuts. We found that on several test problems, reduce-and-split cuts can substantially enhance the performance of a branch-and-bound algorithm.</description><author>Andersen, K.; Cornuejols, G.; Li, Y. J.</author><pubDate>Tue, 01 Nov 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Risky choices and correlated background risk</title><link>http://www.example.com/articles/1</link><description>Tsetlin, I.; Winkler, R. L.
T he analysis of a risky project should take into account not only uncertainties about the return from that project ("project risk"), but also uncertainties associated with other ongoing projects and with exogenous factors that can impact final wealth ("background risk"). The presence of background risk can change the optimal course of action with respect to a project, and ignoring such risk might lead to a poor decision. Most work on background risk assumes that project risk and background risk are independent and are additive in their impact on wealth. However, independence is often unrealistic, and background risk operates in a multiplicative manner in many situations. We relax the independence assumption and consider a model with both additive and multiplicative background risk. The optimal decisions in the correlated setting can be very different than those that would appear optimal if the correlation were ignored. The impact of correlation differs in the additive and multiplicative cases, with positive correlation being beneficial in some cases and negative correlation in others. The analytical and numerical results indicate that in analyzing decision-making problems, it is very important to understand the direction and degree of dependence between project risk and background risks.</description><author>Tsetlin, I.; Winkler, R. L.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Eliciting informative feedback: The peer-prediction method</title><link>http://www.example.com/articles/1</link><description>Miller, N.; Resnick, P.; Zeckhauser, R.
Many recommendation and decision processes depend on eliciting evaluations of opportunities, products, and vendors. A scoring system is devised that induces honest reporting of feedback. Each rater merely reports a signal, and the system applies proper scoring rules to the implied posterior beliefs about another rater's report. Honest reporting proves to be a Nash equilibrium. The scoring schemes can be scaled to induce appropriate effort by raters and can be extended to handle sequential interaction and continuous signals. We also address a number of practical implementation issues that arise in settings such as academic reviewing and online recommender and reputation systems.</description><author>Miller, N.; Resnick, P.; Zeckhauser, R.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An approach to the measurement, analysis, and prediction of brand equity and its sources</title><link>http://www.example.com/articles/1</link><description>Srinivasan, V.; Park, C. S.; Chang, D. R.
The authors propose a new approach for measuring, analyzing, and predicting a brand's equity in a product market. Brand equity is defined as the incremental contribution ($) per year obtained by the brand in comparison to the underlying product (or service) with no brand-building efforts. The incremental contribution is driven by the individual customer's incremental choice probability for the brand in comparison to his choice probability for the underlying product with no brand-building efforts. The approach takes into account three sources of brand equity-brand awareness, attribute perception biases, and nonattribute preference-and reveals how much each of the three sources contributes to brand equity. This is done by taking into account not only the direct effects of these three sources on choice probabilities, but also the indirect effects through enhancing the brand's availability. The method provides what-if analysis capabilities to predict the likely impacts of alternative strategies to enhance a brand's equity The survey-based results from applying the method to the digital cellular phone market in Korea show that the proposed approach has good face validity and convergent validity, with brand awareness playing the largest role, followed by nonattribute preference.</description><author>Srinivasan, V.; Park, C. S.; Chang, D. R.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Subjective probability assessment in decision analysis: Partition dependence and bias toward the ignorance prior</title><link>http://www.example.com/articles/1</link><description>Fox, C. R.; Clemen, R. T.
Decision and risk analysts have considerable discretion in designing procedures for eliciting subjective probabilities. One of the most popular approaches is to specify a particular set of exclusive and exhaustive events for which the assessor provides such judgments. We show that assessed probabilities are systematically biased toward a uniform distribution over all events into which the relevant state space happens to be partitioned, so that probabilities are "partition dependent." We surmise that a typical assessor begins with an "ignorance prior" distribution that assigns equal probabilities to all specified events, then adjusts those probabilities insufficiently to reflect his or her beliefs concerning how the likelihoods of the events differ. In five studies, we demonstrate partition dependence for both discrete events and continuous variables (Studies I and 2), show that the bias decreases with increased domain knowledge (Studies 3 and 4), and that top experts in decision analysis are susceptible to this bias (Study 5). We relate our work to previous research on the "pruning bias" in fault-tree assessment (e.g., Fischhoff et al. 1978) and show that previous explanations of pruning bias (enhanced availability of events that are explicitly specified, ambiguity in interpreting event categories, and demand effects) cannot fully account for partition dependence. We conclude by discussing implications for decision analysis practice.</description><author>Fox, C. R.; Clemen, R. T.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>A partially observed Markov decision process for dynamic pricing</title><link>http://www.example.com/articles/1</link><description>Aviv, Y.; Pazgal, A.
In this paper, we develop a stylized partially observed Markov decision process (POMDP) framework to study a dynamic pricing problem faced by sellers of fashion-like goods. We consider a retailer that plans to sell a given stock of items during a finite sales season. The objective of the retailer is to dynamically price the product in a way that maximizes expected revenues. Our model brings together various types of uncertainties about the demand, some of which are resolvable through sales observations. We develop a rigorous upper bound for the seller's optimal dynamic decision problem and use it to propose an active-learning heuristic pricing policy We conduct a numerical study to test the performance of four different heuristic dynamic pricing policies in order to gain insight into several important managerial questions that arise in the context of revenue management.</description><author>Aviv, Y.; Pazgal, A.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Choice-based elicitation and decomposition of decision weights for gains and losses under uncertainty</title><link>http://www.example.com/articles/1</link><description>Abdellaoui, M.; Vossmann, F.; Weber, M.
This paper reports the results of an experimental parameter-free elicitation and decomposition of decision weights under uncertainty. Assuming cumulative prospect theory, utility functions were elicited for gains and losses at an individual level using the tradeoff method. Subsequently, decision weights were elicited through certainty equivalents of uncertain two-outcome prospects. Furthermore, decision weights were decomposed using observable choice instead of invoking other empirical primitives, as in previous experimental studies. The choice-based elicitation of decision weights allows for a quantitative study of their characteristics, and also allows, among other things, for the examination of the sign-dependence hypothesis for observed choice under uncertainty. Our results confirm concavity of the utility function in the gain domain and bounded subadditivity of decision weights and choice-based subjective probabilities. We also find evidence for sign dependence of decision weights.</description><author>Abdellaoui, M.; Vossmann, F.; Weber, M.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Preferences, identity, and competition: Predicting tie strength from demographic data</title><link>http://www.example.com/articles/1</link><description>Reagans, R.
This research examines the combined influence of three causal mechanisms shaping interpersonal dynamics. Sharing a demographic characteristic has been shown to strengthen a relationship between people. The initial explanation for this positive effect emphasized the importance of individual preferences for social contact. Similar people share common interests, which makes them more attractive as exchange partners. More recent explanations have highlighted the important role of identification and competition. Identification increases the tendency for strong ties to develop among similar people, while competition reduces that tendency I argue that tie strength is a function of all three mechanisms: a baseline level of interpersonal attraction, a positive identification effect, and a negative competition adjustment. Identification and competition vary with the number of people sharing the focal attribute to define a predictable association between sharing the focal attribute and the strength of a network connection. Predictions are tested with tenure and tie-strength data from a small research and development firm. Analysis indicates that having the same tenure has the predicted effect on communication frequency. The implications of the findings are discussed for homophily research in particular and demography research in general.</description><author>Reagans, R.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Four score and seven years from now: The date/delay effect in temporal discounting</title><link>http://www.example.com/articles/1</link><description>Read, D.; Frederick, S.; Orsel, B.; Rahman, J.
We describe a new anomaly in intertemporal choice-the "date/delay effect": discount rates that are imputed when time is described using calendar dates (e.g., on October 17) are markedly lower than those revealed when future outcomes are described in terms of the corresponding delay (e.g., in six months). Date descriptions not only reduce discount rates, but also affect the implied shape of the discount function: When inferred from intertemporal choices between options referenced by calendar dates, the discount function appears markedly less hyperbolic. We discuss potential psychological bases of the date/delay effect, its implications, and other modes of temporal reference.</description><author>Read, D.; Frederick, S.; Orsel, B.; Rahman, J.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Your money or your life: A prescriptive model for health, safety, and consumption decisions</title><link>http://www.example.com/articles/1</link><description>Smith, J. E.; Keeney, R. L.
In this paper we develop a conceptual framework and model for valuing risks to an individual's health and fife and to support decision making about investments in health, quality of life, and safety. Our treatment of health risks in the model builds on the popular quality-adjusted-life-year (QALY) framework that balances health quality and length of life issues. We extend this framework to consider financial concerns as well as health quality and length of life. Our model considers uncertainty in income and health and incorporates the decision maker's ability to adjust consumption over time in response to changes in expectations about health and income. We use this model to study the optimal tradeoffs between financial gains or losses and improvements or reductions in health or longevity and apply it in two example medical decision problems.</description><author>Smith, J. E.; Keeney, R. L.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Transshipment and its impact on supply chain members' performance</title><link>http://www.example.com/articles/1</link><description>Zhang, J.
This note extends some of the key results of Dong and Rudi (2004) to general demand distributions. This generalization is achieved in two steps. First, we build on the analysis of Dong and Rudi to demonstrate that an inventory problem with transshipment is equivalent to a newsvendor problem with an adjusted demand. Then, we study the impact of transshipment by comparing the adjusted demand with the original demand. In addition to extending the results in the existing literature, this note presents a novel approach for analyzing the impact of risk pooling.</description><author>Zhang, J.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Weak-form and semi-strong-form stock return predictability revisited</title><link>http://www.example.com/articles/1</link><description>Ferson, W. E.; Heuson, A.; Su, T.
This paper makes indirect inference about the time variation in expected stock returns by comparing unconditional sample variances to estimates of expected conditional variances. The evidence reveals more predictability as more information is used, and there is no evidence that predictability has diminished in recent years. Semi-strong-form evidence suggests that time variation in expected returns remains economically important.</description><author>Ferson, W. E.; Heuson, A.; Su, T.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The multiobjective discrete optimization problem: A weighted min-max two-stage optimization approach and a bicriteria algorithm</title><link>http://www.example.com/articles/1</link><description>Sayin, S.; Kouvelis, P.
We study the multiple objective discrete optimization (MODO) problem and propose two-stage optimization problems as subproblems to be solved to obtain efficient solutions. The mathematical structure of the first level subproblem has similarities to both Tchebycheff type of approaches and a generalization of the lexicographic max-ordering problem that are applicable to multiple objective optimization. We present some results that enable us to develop an algorithm to solve the bicriteria discrete optimization problem for the entire efficient set. We also propose a modification of the algorithm that generates a sample of efficient solutions that satisfies a prespecified quality guarantee. We apply the algorithm to solve the bicriteria knapsack problem. Our computational results on this particular problem demonstrate that our algorithm performs significantly better than an equivalent Tchebycheff counterpart. Moreover, the computational behavior of the sampling version is quite promising.</description><author>Sayin, S.; Kouvelis, P.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Hub arc location problems: Part II - Formulations and optimal algorithms</title><link>http://www.example.com/articles/1</link><description>Campbell, J. F.; Ernst, A. T.; Krishnamoorthy, M.
Acompanion paper (Campbell et al. 2005) introduces new hub arc location models and analyzes optimal solutions, with special attention to spatial pattens and relationships. This paper provides integer programming formulations and optimal solution algorithms for these problems. We describe two optimal solution approaches in detail and compare their performance, using standard hub location data sets. We present implementation details and show how algorithms can be fine tuned based on characteristics of the data.</description><author>Campbell, J. F.; Ernst, A. T.; Krishnamoorthy, M.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Hub arc location problems: Part I - Introduction and results</title><link>http://www.example.com/articles/1</link><description>Campbell, J. F.; Ernst, A. T.; Krishnamoorthy, M.
Hub networks play an important role in many transportation and telecommunications systems. This paper introduces a new model called the hub arc location model. Rather than locate discrete hub facilities, this model locates hub arcs, which have reduced unit flow costs. Four special cases of the general hub arc location model are examined in detail. We provide motivation for the new models, and present examples and optimal solutions, using data for U.S. air passenger traffic. Results are used to compare optimal costs, hub locations, and hub arc locations with corresponding hub median optimal solutions. The results reveal interesting spatial patterns and help identify promising cities and regions for hubs. A companion paper (Campbell et al. 2005) presents integer programming formulations and solution algorithms for the new hub arc problems. It also provides details and computation times for these solution algorithms.</description><author>Campbell, J. F.; Ernst, A. T.; Krishnamoorthy, M.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal policies for transshipping inventory in a retail network</title><link>http://www.example.com/articles/1</link><description>Wee, K. E.; Dada, M.
Motivated by recent reports of improved customer service facilitated by the use of transshipment to pool inventory, we develop a formal model that focuses on the role of transshipment in a system of n-retailers who stock a good. In addition, some stock may be kept at a warehouse. If a retailer faces a stockout, he makes an attempt to fill backorders by using a transshipment that may come either from the warehouse or from another retailer that has excess stock. We determine that the optimal transshipment policy is described by exactly one of five protocols, and that the choice among them can be made by evaluating a set of easily computable conditions. We also provide other structural results that help identify conditions under which a warehouse should be open: how this decision is influenced by the total number of retailers, and how this decision is related to the degree of correlation of demand between retailers.</description><author>Wee, K. E.; Dada, M.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of duplicate orders on demand estimation and capacity investment</title><link>http://www.example.com/articles/1</link><description>Armony, M.; Plambeck, E. L.
Motivated by a $2.2 billion inventory write-off by Cisco Systems, we investigate how duplicate orders can lead a manufacturer to err in estimating the demand rate and customers' sensitivity to delay, and to make faulty decisions about capacity investment. We consider a manufacturer that sells through two distributors. If a customer finds that his distributor is out of stock, then he will sometimes seek to make a purchase from the other distributor; if the latter is also out of stock, the customer will order from both distributors. When his order is filled by one of the distributors, the customer cancels any duplicate orders. Furthermore, the customer cancels all of his outstanding orders after a random period of time. Assuming that the manufacturer is unaware of duplicate orders, we prove that she will overestimate both the demand rate and the cancellation rate. Surprisingly, failure to account for duplicate orders can cause short-term underinvestment in capacity. However, in long-term equilibrium under stable demand conditions the manufacturer overinvests in capacity. Our results suggest that Cisco's write-off was caused by estimation errors and cannot be blamed entirely on the economic downturn. Finally, we provide some guidance on estimation in the presence of double orders.</description><author>Armony, M.; Plambeck, E. L.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Two-sided network effects: A theory of information product design</title><link>http://www.example.com/articles/1</link><description>Parker, G. G.; Van Alstyne, M. W.
How can firms profitably give away free products? This paper provides a novel answer and articulates trade-offs in a space of information product design. We introduce a formal model of two-sided network externalities based in textbook economics-a mix of Katz and Shapiro network effects, price discrimination, and product differentiation. Externality-based complements, however, exploit a different mechanism than either tying or lock-in even as they help to explain many recent strategies such as those of firms selling operating systems, Internet browsers, games, music, and video. The model presented here argues for three simple but useful results. First, even in the absence of competition, a firm can rationally invest in a product it intends to give away into perpetuity. Second, we identify distinct markets for content providers and end consumers and show that either can be a candidate for a free good. Third, product coupling across markets can increase consumer welfare even as it increases firm profits. The model also generates testable hypotheses on the size and direction of network effects while offering insights to regulators seeking to apply antitrust law to network markets.</description><author>Parker, G. G.; Van Alstyne, M. W.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bundling with customer self-selection: A simple approach to bundling low-marginal-cost goods</title><link>http://www.example.com/articles/1</link><description>Hitt, L. M.; Chen, P. Y.
With declining costs of distributing digital products comes renewed interest in strategies for pricing goods with low marginal costs. In this paper, we evaluate customized bundling, a pricing strategy that gives consumers the right to choose up to a quantity M of goods drawn from a larger pool of N different goods for a fixed price. We show that the complex mixed-bundle problem can be reduced to the customized-bundle problem under some commonly used assumptions. We also show that, for a monopoly seller of low marginal cost goods, this strategy outperforms individual selling (M = 1) and pure bundling (M = N) when goods have a positive marginal cost or when customers have heterogeneous preferences over goods. Comparative statics results also show that the optimal bundle size for customized bundling decreases in both heterogeneity of consumer preferences over different goods and marginal costs of production. We further explore how the customized-bundle solution is affected by factors such as the nature of distribution functions in which valuations are drawn, the correlations of values across goods, and the complementarity or substitutability among products. Altogether, our results suggest that customized bundling has a number of advantages-both in theory and practice-over other bundling strategies in many relevant settings.</description><author>Hitt, L. M.; Chen, P. Y.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Using capital markets as market intelligence: Evidence from the pharmaceutical industry</title><link>http://www.example.com/articles/1</link><description>Markovitch, D. G.; Steckel, J. H.; Yeung, B.
Financial theory posits that capital markets convey through stock prices their expectation of the firm's future performance. We use concepts from principal-agent theory and prospect theory to provide a theoretical explanation for the role stock price variation plays in managerial decision making. We then empirically investigate what specific decisions managers undertake in response to stock price variation. We perform our empirical analyses in the context of the pharmaceutical industry. We find that drug firms whose stock underperformed the industry react differently than drug firms with high-performing stocks. Specifically, laggards tend to implement more changes to their current product portfolio and distribution than high-performing firms. The more laggards underperform, the more they implement acquisitions aimed to produce immediate improvement in the firm's product portfolio. In contrast, drug firms whose stocks outperform the industry tend to make fewer changes to their current portfolio and distribution. Instead, they focus more on long-term research and development and marketing of existing products. We interpret these findings in light of industry key success factors.</description><author>Markovitch, D. G.; Steckel, J. H.; Yeung, B.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Conspicuous consumption and sophisticated thinking</title><link>http://www.example.com/articles/1</link><description>Amaldoss, W.; Jain, S.
Consumers purchase conspicuous goods to satisfy not only material needs but also social needs such as prestige. In an attempt to meet these social needs, producers of conspicuous goods like cars, perfumes, and watches, highlight the exclusivity of their products. In this paper, we propose a monopoly model of conspicuous consumption using the rational expectations framework, and then examine how purchase decisions are affected by the desire for exclusivity and conformity. We show that snobs can have an upward-sloping demand curve but only in the presence of consumers who are (weakly) followers. Laboratory tests lend support for this model prediction and for the rational expectations framework. The experimental results suggest that subjects. used some degree of sophisticated thinking to arrive at their first-period decisions. Their behavior in the subsequent trials, however, can be adequately captured by a purely adaptive learning mechanism. We discuss the implications of consumer learning for optimal dynamic pricing policy by a monopolist.</description><author>Amaldoss, W.; Jain, S.</author><pubDate>Sat, 01 Oct 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Three new tests of independence that differentiate models of risky decision making</title><link>http://www.example.com/articles/1</link><description>Birnbaum, M. H.
This paper tests new "independence" properties to compare three models of risky decision making. According to the rank-affected multiplicative (RAM) weights model, all three properties should be satisfied; according to the transfer of attention exchange (TAX) model, two should be satisfied and one can be violated. However, according to cumulative prospect theory (CPT), all three properties will be violated if the probability weighting function is nonlinear. Although CPT is flexible enough to accommodate violations of these properties, its predicted violations based on previously estimated parameters failed to materialize. In 14 choices for which the CPT model disagreed with TAX, CPT correctly predicted the modal choice in only one case and TAX predicted the modal choice in the other 13. New versions of three other paradoxes were also tested and found to refute CPT.</description><author>Birnbaum, M. H.</author><pubDate>Thu, 01 Sep 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Slow Dutch auctions</title><link>http://www.example.com/articles/1</link><description>Carare, O.; Rothkopf, M.
Theorists have long believed that Dutch auctions are strategically equivalent to standard sealed bidding. However, in recent controlled experiments with actual Dutch and sealed-bid Internet auctions of collectibles, the Dutch auctions produced significantly more revenue. We believe that this happened, in part, because the Internet Dutch auctions are a slow process in which bidders incur incremental transaction costs if they delay bidding. This paper presents models of slow Dutch auctions that include these costs and explain this belief. We first present a decision-theoretic model of a slow Dutch auction. While simple, the decision-theoretic model is fairly general and provides the basic intuition underlying our revenue results. We then develop a game-theoretic model of a slow Dutch auction. We derive two symmetric, payoff-equivalent equilibria of the game in the absence of a cost of return and then consider the more general case of costly return. When the cost of return is in an appropriate range, the seller's expected revenue is an increasing function of that cost.</description><author>Carare, O.; Rothkopf, M.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Online haggling at a name-your-own-price retailer: Theory and application</title><link>http://www.example.com/articles/1</link><description>Terwiesch, C.; Savin, S.; Hann, I. H.
We present a formal model of haggling between a name-your-own-price retailer and a set of individual buyers. Rather than posting a price, the retailer waits for potential buyers to submit offers for a given product and then chooses to either accept or reject them. Consumers whose offers have been rejected can invest in additional haggling effort and increment their offers. This pricing model allows the name-your-own-price retailer to engage in price discrimination: As haggling is costly for the potential buyer, customers with a high willingness to haggle will achieve lower transaction prices. However, because haggling is costly, it reduces overall welfare and diminishes the benefits of price discrimination. Our study is motivated by several name-your-own-price retailers that have recently emerged on the Internet. Based on detailed transaction data of a large German name-your-own-price retailer, we present a model of consumer haggling. We then show how this model can be used to improve the decision making of the retailer, who needs to choose a threshold price above which all offers are accepted. Another decision variable for the retailer lies in the user interface design, which allows the retailer to either facilitate or to hinder the haggling of the consumer.</description><author>Terwiesch, C.; Savin, S.; Hann, I. H.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Naive bidding</title><link>http://www.example.com/articles/1</link><description>Deltas, G.; Engelbrecht-Wiggans, R.
This paper presents an equilibrium explanation for the persistence of naive bidding. Specifically, we consider a common value auction in which a "naive" bidder (who ignores the winner's curse) competes against a fully rational bidder. We show that the naive bidder earns higher equilibrium profits than the rational bidder when the signal distribution is symmetric and unimodal. We then consider a sequence of such auctions with randomly selected participants from a population of naive and rational bidders, with the proportion of bidder types in the population evolving in response to. their relative payoffs in the auctions. We show that the evolutionary equilibrium contains a strictly positive proportion of naive bidders. Finally, we consider more general examples in which a naive bidder matched against a rational bidder does (i) worse than his rational opponent, but (ii) better than a rational bidder matched against another rational bidder. Again, the evolutionary equilibrium contains a strictly positive proportion of naive bidders. The results suggest that overconfident recent entrants in Internet and other low transaction-cost auctions of items that appeal to a mass audience may earn higher payoffs than their experienced competitors and, thus, will not eventually be driven from the market.</description><author>Deltas, G.; Engelbrecht-Wiggans, R.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The landscape of electronic market design</title><link>http://www.example.com/articles/1</link><description>Anandalingam, G.; Day, R. W.; Raghavan, S.
This paper presents an introductory survey for this special issue of Management Science on electronic markets. We acquaint the reader with some fundamental concepts in the study of electronic market mechanisms, while simultaneously presenting a survey and summary of the essential literature in this area. Along the way, we position each of the papers presented in this. special issue within the existing literature demonstrating the deep impact of these 14 articles on an already broad body of knowledge.</description><author>Anandalingam, G.; Day, R. W.; Raghavan, S.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Introduction to the special issue, on electronic markets</title><link>http://www.example.com/articles/1</link><description>Anandalingam, G.; Raghavan, S.
nan</description><author>Anandalingam, G.; Raghavan, S.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Back to the St. Petersburg paradox?</title><link>http://www.example.com/articles/1</link><description>Blavatskyy, P. R.
The conventional parameterizations of cumulative prospect theory do not explain the St. Petersburg paradox. To do so, the power coefficient of an individual's utility function must be lower than the power coefficient of an individual's probability weighting function.</description><author>Blavatskyy, P. R.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Research note - Cost uncertainty is bliss: The effect of competition on the acquisition of cost information for pricing new products</title><link>http://www.example.com/articles/1</link><description>Christen, M.
We examine the optimal acquisition of information about a common uncertain cost factor by two competing firms seeking to price a new product. We show that existing findings regarding the acquisition of demand information or the acquisition of either cost or demand information related to quantity decisions do not extend to this case. For cost information with price competition, the information acquisition strategies are strategic substitutes, even though the price decisions that are based on the information are strategic complements. Competition decreases the expected value of cost information. Moreover, when competition is intense and the cost of information low, identical firms do not acquire the same amount of cost information - even when information is free. Cost uncertainty acts like a "fog" that lessens the destructive effect of price competition when products are close substitutes, and thus increases expected profits. Buyers, on the other hand, are better off when competing firms are informed about cost. Even though the expected value of cost information strictly decreases with competition, the optimal price for industry-specific cost information set by an information vendor increases with competition when the firms' products are sufficiently substitutable.</description><author>Christen, M.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bundling information goods of decreasing value</title><link>http://www.example.com/articles/1</link><description>Geng, X. J.; Stinchcombe, M. B.; Whinston, A. B.
Consumers' average value for information goods, websites, weather forecasts, music, and news declines with the number consumed. This paper provides simple guidelines to optimal bundling marketing strategies in this case. If consumers' values do not decrease too quickly, we show that bundling is approximately optimal. If consumers' values to subsequent goods decrease quickly, we show by example that one should expect bundling to be suboptimal.</description><author>Geng, X. J.; Stinchcombe, M. B.; Whinston, A. B.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Optimal allocation of proposals to reviewers to facilitate effective ranking</title><link>http://www.example.com/articles/1</link><description>Cook, W. D.; Golany, B.; Kress, M.; Penn, M.; Raviv, T.
Peer review of research proposals and articles is an essential element in research and development processes worldwide. Here we consider a problem that, to the best of our knowledge, has not been addressed until now: how to assign subsets of proposals to reviewers in scenarios where the reviewers supply their evaluations through ordinal ranking. The solution approach we propose for this assignment problem maximizes the number of proposal pairs that will be evaluated by one or more reviewers. This new approach should facilitate meaningful aggregation of partial rankings of subsets of proposals by multiple reviewers into a consensus ranking. We offer two ways to implement the approach: an integer-programming set-covering model and a heuristic procedure. The effectiveness and efficiency of the two models are tested through an extensive simulation experiment.</description><author>Cook, W. D.; Golany, B.; Kress, M.; Penn, M.; Raviv, T.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The corporate digital divide: Determinants of Internet adoption</title><link>http://www.example.com/articles/1</link><description>Forman, C.
The diffusion of Internet technology among firms is widely considered to be one of the primary factors behind the rapid economic growth of the 1990s. However, little systematic study has examined the variation in firm decisions to adopt the Internet. I explore the sources of this variation by examining Internet adoption decisions in a very large sample of organizations in the finance and services sector in 1998. I show how prior information technology (IT) investments land workplace organization decisions affect the returns to adopting simple and complex Internet technologies. I show that recent investments in client/server (C/S) networking applications have competing effects on the likelihood of Internet adoption. Such investments can slow adoption by acting as a short-run substitute or by creating "switching costs." Geographic dispersion of employees is complementary with Internet adoption, suggesting that Internet technology lowered internal coordination costs. Increases in organization size and external pressure also increase the likelihood of adoption.</description><author>Forman, C.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>The impact of contractor behavior on the client's payment-scheduling problem</title><link>http://www.example.com/articles/1</link><description>Szmerekovsky, J. G.
Recent approaches to modeling the client's payment-scheduling problem allow the client to control both the timing of payments and the completion times of activities. In practice, the activity schedule is typically determined by the contractor rather than the client. This paper addresses this drawback by considering a model in which the client selects the payment activities and the contractor selects the activity schedule, each to maximize his own net present value (NPV). In addition, the contractor has the option to reject the project if it does not provide a minimum NPV. The new model is shown to be NP-hard in the strong sense and a branch-and-bound procedure is provided for its solution. Results are obtained for the new model that contradict those of the older models. The benefits to the client are no,longer increasing with retention and no longer decreasing with the number of payments. Furthermore, it is no longer ideal for the client to focus payments at the beginning and end of the project. Empirical evidence based on a randomly generated data set suggests that the market for the contractor's services, the timing of benefits received by the client, and the project duration drive these new results.</description><author>Szmerekovsky, J. G.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Integrated scheduling of production and distribution operations</title><link>http://www.example.com/articles/1</link><description>Chen, Z. L.; Vairaktarakis, G. L.
Motivated by applications in the computer and food catering service industries, we study an integrated scheduling model of production and distribution operations. In this model, a set of jobs (i.e,, customer orders) are first processed in a processing facility (e.g., manufacturing plant or service center) and then delivered to the customers directly without intermediate inventory. The problem is to find a joint schedule of production and distribution such that an objective function that takes into account both customer service level and total distribution cost is optimized. Customer service level is measured by a function of the times when the jobs are delivered to the customers. The distribution cost of a delivery shipment consists of a fixed charge and a variable cost proportional to the total distance of the route taken by the shipment. We study two classes of problems under this integrated scheduling model. In the first class of problems, customer service is measured by the average time when the jobs are delivered to the customers; in the second class, customer service is measured by the maximum time when the jobs are delivered to the customers. Two machine configurations in the processing facility-single machine and parallel machine-are considered. For each of the problems studied, we provide an efficient exact algorithm, or a proof of intractability accompanied by a heuristic algorithm with worst-case and asymptotic performance analysis. Computational experiments demonstrate that the heuristics developed are capable of generating near-optimal solutions. We also investigate the possible benefit of using the proposed integrated model relative to a sequential model where production and distribution operations are scheduled sequentially and separately. Computational tests show that in many cases a significant benefit can be achieved by integration.</description><author>Chen, Z. L.; Vairaktarakis, G. L.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Bertrand supertraps</title><link>http://www.example.com/articles/1</link><description>Cabral, L. M. B.; Villas-Boas, M.
We study oligopoly price competition between multiproduct firms-firms whose products interact in the profit function. Specifically, we focus on the impact of intrafirm product interactions on the level of equilibrium profits. This impact may be decomposed in two different ways: (a) a direct effect (keeping the competitors' actions fixed) plus a strategic effect (i.e., through the competitors' actions); or, alternatively, (b) a competitive advantage effect (change in firm i only) plus an imitation effect (change in all other firms). We derive conditions such that (a) the strategic effect more than outweighs the direct effect, and conditions such that (b) the imitation effect more than outweighs the competitive advantage effect: Bertrand supertraps. For example, an increase in the degree of economies of scope would increase profits if prices were fixed or if the change were limited to firm i's cost function. However, if all firms increase the degree of economies of scope then all firms receive lower profits. A variety of other applications is considered, including learning curves, core competencies, demand synergies, systems competition, compatibility bundling, network effects, switching costs, durable goods, long-term contracts.</description><author>Cabral, L. M. B.; Villas-Boas, M.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Diversification and the optimal construction of basis portfolios</title><link>http://www.example.com/articles/1</link><description>Lehmann, B. N.; Modest, D. M.
Nontrivial diversification possibilities arise when a factor model describes security returns. This paper catalogs the merits of alternative strategies for constructing basis portfolios to mimic the common factors. We show how to use the chi(2) statistic for the joint significance of mean basis portfolio returns to rank alternative procedures and the bootstrap to perform inferences on the disparity between chi(2) statistics across portfolio formation procedure, estimation method, cross-section size, and number of factors. Our main conclusion is that maximum likelihood factor analysis coupled with minimum idiosyncratic risk portfolio formation yields economically and statistically superior basis portfolios compared with those derived from asymptotic principal components.</description><author>Lehmann, B. N.; Modest, D. M.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Option pricing with downward-sloping demand curves: The case of supply chain options</title><link>http://www.example.com/articles/1</link><description>Burnetas, A.; Ritchken, P.
This article investigates the role of option contracts in a supply chain when the demand curve is downward sloping. We consider call (put) options that provide the retailer with the right to reorder (return) goods at a fixed price. We show that the introduction of option contracts causes the wholesale price to increase and the volatility of the retail price to decrease. In general, options are not zero-sum games. Conditions are derived under which the manufacturer prefers to use options. When this happens the retailer is also better off, if the uncertainty in the demand curve is low. However, if the uncertainty is sufficiently high, then the introduction of option contracts alters the equilibrium prices in a way that hurts the retailer.</description><author>Burnetas, A.; Ritchken, P.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>On the benefits of pooling in production-inventory systems</title><link>http://www.example.com/articles/1</link><description>Benjaafar, S.; Cooper, W. L.; Kim, J. S.
We study inventory pooling in systems with symmetric costs where supply lead times are endogenously generated by a finite-capacity production system. We investigate the sensitivity of the cost advantage of inventory pooling to various system parameters, including loading, service levels, demand and production time variability, and structure of the production system. The analysis reveals differences in how various parameters affect the cost reduction from pooling and suggests that these differences stem from the manner in which the parameters influence the induced correlation between lead-time demands of the demand streams. We compare these results with those obtained for pure inventory systems, where lead times are exogenous. We also compare inventory pooling with several forms of capacity pooling.</description><author>Benjaafar, S.; Cooper, W. L.; Kim, J. S.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Inventory sharing and rationing in decentralized dealer networks</title><link>http://www.example.com/articles/1</link><description>Zhao, H.; Deshpande, V.; Ryan, J. K.
An increasing number of manufacturers have started to pursue a strategy that promotes inventory sharing among the dealers in their distribution network. In this paper we analyze a decentralized dealer network in which each independent dealer is given the flexibility to share his inventory. We model inventory sharing as a multiple demand classes problem in which each dealer faces his own customer demand with high priority, and inventory-sharing requests from other dealers with low priority. Assuming that each dealer uses a base-stock and threshold-rationing policy for his inventory-stocking and inventory-sharing decisions, we explicitly model the interactions between the dealers through inventory sharing and obtain a closed-form cost function for each dealer based on the steady-state distribution of the inventory levels at the two dealers. We then provide a detailed supermodularity analysis of the inventory-sharing and inventory-rationing game in which each dealer has a two-dimensional strategy set (stocking level and rationing level). We show that the full-sharing game (in which dealers precommit to sharing all of their on-hand inventory) and the fixed-sharing-level game (in which dealers precommit to sharing a portion of their on-hand inventory) are supermodular, and thus a pure-strategy Nash equilibrium is guaranteed to exist. For the rationing game (in which dealers precommit to their stocking levels), we show that there exists a dominant strategy equilibrium on the dealers' sharing (rationing) levels. Finally, a comprehensive computational study is conducted to highlight the impact of the manufacturer's incentives, subsidies, and/or transshipment fees on the dealers' sharing behavior.</description><author>Zhao, H.; Deshpande, V.; Ryan, J. K.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Trust building among strangers</title><link>http://www.example.com/articles/1</link><description>Ho, T. H.; Weigelt, K.
The trust-building process is basic to social science. We investigate it in a laboratory setting using a novel multistage trust game where social gains are achieved if players trust each other in each stage. In each stage, also, players have an opportunity to appropriate these gains or be trustworthy by sharing them. Players are strangers because they do not know the identity of others and, they will not play them again. Thus, there is no prospect of future interaction to induce trusting behavior, and, we study the trust-building process where there is little scope for social relations and networks. Standard game theory, which assumes all players are opportunistic and untrustworthy and thus should have zero trust for others, is used to construct a null hypothesis. We test whether people are trusting or trustworthy and examine how inferring the intentions of those who trust affects trustworthiness. We also investigate the effect of stake on trust, and study the evolution of trust. Results show subjects exhibit some degree of trusting behavior, although a majority of them are not trustworthy and claim the entire social gain. Players are more reluctant to trust in later stages than in earlier ones and are more trustworthy if they are certain of the trustee's intention. Surprisingly, subjects are more trusting and trustworthy when the stake size increases. Finally, we find the subpopulation that invests in initiating the trust-building process modifies its trusting behavior based on the relative fitness of trust.</description><author>Ho, T. H.; Weigelt, K.</author><pubDate>Fri, 01 Apr 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Emotional bidders - An analytical and experimental examination of consumers/behavior in a priceline-like reverse auction</title><link>http://www.example.com/articles/1</link><description>Ding, M.; Eliashberg, J.; Huber, J.; Saini, R.
E-commerce has proved to be fertile ground for new business models, which may be patented (for up to 20 years) and have potentially far-reaching impact on the e-commerce landscape. One such electronic market is the reverse-auction model popularized by Priceline com. There is still uncertainty surrounding the survival of such new electronic markets currently available on the Internet. Understanding user behavior is necessary for better assessment of these sites' survival. This paper adds to economic analysis a formal representation of the emotions evoked by the auction process, specifically, the excitement of winning if a bid is accepted, and the frustration of losing if it is not. We generate and empirically test a number of insights related to (1) the impact of expected excitement at winning, and frustration at losing, on bids across consumers and biddings scenarios; and (2) the dynamic nature of the bidding behavior-that is, how winning and losing in previous bids influence their future bidding behavior.</description><author>Ding, M.; Eliashberg, J.; Huber, J.; Saini, R.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>CABOB: A fast, optimal algorithm for winner determination in combinatorial auctions</title><link>http://www.example.com/articles/1</link><description>Sandholm, T.; Suri, S.; Gilpin, A.; Levine, D.
Combinatorial auctions where bidders can bid on bundles of items' can lead to more economically efficient allocations, but determining the winners is NP-complete and inapproximable. We present CABOB, a sophisticated optimal search algorithm for the problem. It uses decomposition techniques, upper and lower, bounding (also across components), elaborate and dynamically chosen bid-ordering heuristics, and a host of structural observations. CABOB attempts to capture structure in any instance. without making assumptions about the instance distribution. Experiments against the fastest prior algorithm, CPLEX 8.0, show that CABOB is often faster, seldom drastically slower, and in many cases drastically faster-especially in cases with structure. CABOB's search runs in linear space and has significantly better anytime performance than CPLEX We also uncover interesting aspects of the problem itself. First, problems with short bids, which were hard for the first generation of specialized algorithms, are easy. Second, almost all of the CATS distributions are easy, and the run time is virtually unaffected by the number of goods. Third, we test several random restart strategies, showing that they do not help on this problem-the run-time distribution does not have a heavy tail.</description><author>Sandholm, T.; Suri, S.; Gilpin, A.; Levine, D.</author><pubDate>Tue, 01 Mar 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>An ARIMA supply chain model</title><link>http://www.example.com/articles/1</link><description>Gilbert, K.
This paper presents a multistage supply chain model that is based on Autoregressive Integrated Moving Average (ARIMA) time-series models. Given an ARIMA model of consumer demand and the lead times at each stage, it is shown that the orders and inventories at each stage are also ARIMA, and closed-form expressions for these models are given. The paper also discusses the causes of the bullwhip effect, a phenomenon in which variation in demand produces larger variations in upstream orders and inventory. This discussion reveals how different modeling can lead to different insights because they make different assumptions about the cause of the bullwhip effect. These observations are used to develop managerial insights about reducing the bullwhip effect.</description><author>Gilbert, K.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Productivity change, technical progress, and relative efficiency change in the public accounting industry</title><link>http://www.example.com/articles/1</link><description>Banker, R. D.; Chang, H. S.; Natarajan, R.
We present evidence on components of productivity change in the public accounting industry toward the end of the 20th century. Using revenue and human resource data from 64 of the 100 largest public accounting firms in the United States for the 1995-1999 period, we analyze productivity change, technical progress, and relative efficiency change over time. The average public accounting firm experienced a productivity growth of 9.5% between 1995 and 1999. We find support for the hypothesis that technical progress rather than an improvement in relative efficiency was the reason for this productivity growth. Firms that were early movers into management advisory services (MAS) and those that emphasized growth in MAS over growth in the traditional audit and tax services enjoyed significantly higher productivity growth than their peers. These firms also contributed significantly more to the industry's technical progress.</description><author>Banker, R. D.; Chang, H. S.; Natarajan, R.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Portfolio investment with the exact tax basis via nonlinear programming</title><link>http://www.example.com/articles/1</link><description>DeMiguel, V.; Uppal, R.
Computing the optimal portfolio policy of an investor facing capital gains tax is a challenging problem: because the tax to be paid depends on the price at which the security was purchased (the tax basis), the optimal policy is path dependent and the size of the problem grows exponentially with the number of time periods. Dammon et al. (2001, 2002, 2004), Garlappi et al. (2001), and Gallmeyer et al. (2001) address this problem by approximating the exact tax basis by the weighted average purchase price. Our contribution is threefold. First, we show that the structure of the problem has several attractive features that can be exploited to determine the optimal portfolio policy using the exact tax basis via nonlinear programming. Second, we characterize the optimal portfolio policy in the presence of capital gains tax when using the exact tax basis. Third, we show that the certainty equivalent loss from using the average tax basis instead of the exact basis is very small: it is typically less than 1% for problems with up to 10 periods, and this result is robust to the choice of parameter values and to the presence of transaction costs, dividends, intermediate consumption, labor income, tax reset provision at death, and wash-sale constraints.</description><author>DeMiguel, V.; Uppal, R.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Customer targeting: A neural network approach guided by genetic algorithms</title><link>http://www.example.com/articles/1</link><description>Kim, Y.; Street, W. N.; Russell, G. J.; Menczer, F.
One of the key problems in database marketing is the identification and profiling of households that are most likely to be interested in a particular product or service. Principal component analysis (PCA) of customer background information followed by logistic regression analysis of response behavior is commonly used by database marketers. In this paper, we propose a new approach that uses artificial neural networks (ANNs) guided by genetic algorithms (GAs) to target households. We show that the resulting selection rule is more accurate and more parsimonious than the PCA/logit rule when the manager has a clear decision criterion. Under vague decision criteria, the new procedure loses its advantage in interpretability, but is still more accurate than PCA/logit in targeting households.</description><author>Kim, Y.; Street, W. N.; Russell, G. J.; Menczer, F.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Moral property rights in bargaining with infeasible claims</title><link>http://www.example.com/articles/1</link><description>Gachter, S.; Riedl, A.
In many business transactions, labor-management relations, international conflicts, and welfare-state reforms, bargainers hold strong entitlements that are often generated by claims that are not feasible anymore. These entitlements seem to shape negotiation behavior considerably. By using the novel setup of a "bargaining with claims" experiment, we provide new systematic evidence tracking the influence of entitlements and obligations through the whole bargaining process. We find strong entitlement effects that shape opening offers, bargaining duration, concessions, and (dis)agreements. We argue that entitlements constitute a "moral property right" that is influential independent of negotiators' legal property rights.</description><author>Gachter, S.; Riedl, A.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Priority auctions and queue disciplines that depend on processing time</title><link>http://www.example.com/articles/1</link><description>Kittsteiner, T.; Moldovanu, B.
We analyze the allocation of priority in queues via simple bidding mechanisms. In our model, the stochastically arriving customers are privately informed about their own processing time. They make bids upon arrival at a queue whose length is unobservable. We consider two bidding schemes that differ in the definition of bids (these may reflect either total payments or payments per unit of time) and in the timing of payments (before or after service). In both schemes, a customer obtains priority over all customers, waiting in the queue or arriving while he is waiting, who make lower bids. Our main results show how the convexity/concavity of the function expressing the costs of delay determines the queue discipline (i.e., shortest-processing-time-first (SPT), longest-processing-time-first (LPT)) arising in a bidding equilibrium.</description><author>Kittsteiner, T.; Moldovanu, B.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item><item><title>Engineering solution of a basic call-center model</title><link>http://www.example.com/articles/1</link><description>Whitt, W.
An algorithm is developed to rapidly compute approximations for all the standard steady-state performance measures in the basic call-center queueing model M/GI/s/r + GI, which has a Poisson arrival process, independent and identically distributed (IID) service times with a general distribution, s servers, r extra waiting spaces and IID customer abandonment times with a general distribution. Empirical studies of call centers indicate that the service-time and abandon-time distributions often are not nearly exponential, so that it is important to go beyond the Markovian M/M/s/r + M special case, but the general service-time and abandon-time distributions make the realistic model very difficult to analyze directly. The proposed algorithm is based on an approximation by an appropriate Markovian M/M/s/r + M(n) queueing model, where M(n) denotes state-dependent abandonment rates. After making an additional approximation, steady-state waiting-time distributions are characterized via their Laplace transforms. Then the approximate distributions are computed by numerically inverting the transforms. Simulation experiments show that the approximation is quite accurate. The overall algorithm can be applied to determine desired staffing levels, e.g., the minimum number of servers needed to guarantee that, first, the abandonment rate is below any specified target value and, second, that the conditional probability that an arriving customer will be served within a specified deadline, given that the customer eventually will be served, is at least a specified target value.</description><author>Whitt, W.</author><pubDate>Tue, 01 Feb 2005 00:00:00 GMT</pubDate><guid isPermaLink="true">http://www.example.com/articles/1</guid></item></channel></rss>